#!/usr/bin/env python3
"""
Advanced Incremental Index Update System

–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏,
–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π, –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–µ–º–æ–Ω—Ç–æ–º.

Features:
- üöÄ Incremental updates (—Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã)
- ‚ö° Parallel processing (multiprocessing –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞–∑)
- üîç Change detection (MD5 fingerprinting + mtime tracking)
- üõ†Ô∏è Index validation & repair (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫)
- üíæ Multiple backends (JSON cache, SQLite database)
- üå≥ Dependency tracking (smart reindexing –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
- üìä Comprehensive statistics (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –ø–æ–∫—Ä—ã—Ç–∏–µ, health)
- üéØ Selective updates (–ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, —Ç–µ–≥–∞–º)

Usage:
    python3 update_indexes.py                    # Full update
    python3 update_indexes.py --incremental      # Only changed files
    python3 update_indexes.py --category cooking # Specific category
    python3 update_indexes.py --validate         # Validate indexes
    python3 update_indexes.py --parallel 4       # Use 4 worker processes
"""

import os
import re
import sys
import json
import hashlib
import sqlite3
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from multiprocessing import Pool, cpu_count
from typing import Dict, List, Set, Tuple, Optional
import yaml


class ChangeTracker:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ MD5 + mtime"""

    def __init__(self, cache_file=".index_cache.json"):
        self.cache_file = Path(cache_file)
        self.cache = self._load_cache()

    def _load_cache(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError):
                return {'files': {}, 'last_full_update': None}
        return {'files': {}, 'last_full_update': None}

    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, indent=2)

    def calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–∏—Ç—å MD5 —Ö—ç—à —Ñ–∞–π–ª–∞"""
        md5 = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(8192), b''):
                    md5.update(chunk)
            return md5.hexdigest()
        except IOError:
            return ''

    def has_changed(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª"""
        file_str = str(file_path)
        current_mtime = file_path.stat().st_mtime
        current_hash = self.calculate_file_hash(file_path)

        cached = self.cache['files'].get(file_str, {})

        # –°—Ä–∞–≤–Ω–∏—Ç—å mtime –∏ hash
        if cached.get('mtime') != current_mtime or cached.get('hash') != current_hash:
            # –û–±–Ω–æ–≤–∏—Ç—å –∫—ç—à
            self.cache['files'][file_str] = {
                'mtime': current_mtime,
                'hash': current_hash,
                'last_indexed': datetime.now().isoformat()
            }
            return True

        return False

    def mark_full_update(self):
        """–û—Ç–º–µ—Ç–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ"""
        self.cache['last_full_update'] = datetime.now().isoformat()
        self._save_cache()

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è"""
        self._save_cache()


class IndexValidator:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ä–µ–º–æ–Ω—Ç –∏–Ω–¥–µ–∫—Å–æ–≤"""

    def __init__(self, root_dir: Path):
        self.root_dir = root_dir
        self.errors = []
        self.warnings = []

    def validate_frontmatter(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å frontmatter"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
            if not match:
                self.warnings.append(f"No frontmatter: {file_path}")
                return False

            frontmatter = yaml.safe_load(match.group(1))

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            required_fields = ['title', 'type']
            for field in required_fields:
                if field not in frontmatter:
                    self.errors.append(f"Missing '{field}' in {file_path}")
                    return False

            return True

        except Exception as e:
            self.errors.append(f"Error reading {file_path}: {e}")
            return False

    def validate_links(self, file_path: Path) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª–µ"""
        broken_links = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # –ù–∞–π—Ç–∏ –≤—Å–µ markdown-—Å—Å—ã–ª–∫–∏
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)

            for text, link in links:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                if link.startswith(('http://', 'https://', '#')):
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                target = (file_path.parent / link).resolve()
                if not target.exists():
                    broken_links.append(f"{file_path}: broken link to {link}")

        except Exception as e:
            self.errors.append(f"Error validating links in {file_path}: {e}")

        return broken_links

    def repair_index(self, index_file: Path) -> bool:
        """–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        try:
            # –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            backup = index_file.with_suffix('.md.bak')
            if index_file.exists():
                import shutil
                shutil.copy(index_file, backup)

            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ä–µ–º–æ–Ω—Ç–∞
            # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ frontmatter
            if not re.match(r'^---\s*\n', content):
                self.errors.append(f"Index {index_file} has no frontmatter")
                return False

            return True

        except Exception as e:
            self.errors.append(f"Cannot repair {index_file}: {e}")
            return False

    def get_report(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç—á—ë—Ç –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        report = "# Index Validation Report\n\n"

        if not self.errors and not self.warnings:
            report += "‚úÖ **All indexes are valid!**\n"
        else:
            if self.errors:
                report += f"## ‚ùå Errors ({len(self.errors)})\n\n"
                for error in self.errors:
                    report += f"- {error}\n"
                report += "\n"

            if self.warnings:
                report += f"## ‚ö†Ô∏è Warnings ({len(self.warnings)})\n\n"
                for warning in self.warnings:
                    report += f"- {warning}\n"

        return report


class DependencyGraph:
    """–ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è smart reindexing"""

    def __init__(self):
        self.graph = defaultdict(set)  # file -> dependencies
        self.reverse_graph = defaultdict(set)  # file -> dependents

    def add_dependency(self, file_path: str, depends_on: str):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å"""
        self.graph[file_path].add(depends_on)
        self.reverse_graph[depends_on].add(file_path)

    def get_affected_files(self, changed_file: str) -> Set[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã, –∑–∞–≤–∏—Å—è—â–∏–µ –æ—Ç –∏–∑–º–µ–Ω—ë–Ω–Ω–æ–≥–æ"""
        affected = {changed_file}
        queue = [changed_file]

        while queue:
            current = queue.pop(0)
            for dependent in self.reverse_graph.get(current, []):
                if dependent not in affected:
                    affected.add(dependent)
                    queue.append(dependent)

        return affected

    def build_from_links(self, articles: List[Dict]):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∏–∑ —Å—Å—ã–ª–æ–∫ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏"""
        for article in articles:
            file_path = str(article['file'])

            try:
                with open(article['file'], 'r', encoding='utf-8') as f:
                    content = f.read()

                # –ù–∞–π—Ç–∏ –≤—Å–µ internal links
                links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
                for _, link in links:
                    if not link.startswith(('http://', 'https://', '#')):
                        target = (article['file'].parent / link).resolve()
                        if target.exists():
                            self.add_dependency(file_path, str(target))

            except Exception:
                pass


class ParallelIndexer:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤"""

    @staticmethod
    def process_article(args: Tuple[Path, Path]) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é (–¥–ª—è multiprocessing)"""
        md_file, category_path = args

        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # –ò–∑–≤–ª–µ—á—å frontmatter
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
            if not match:
                return None

            frontmatter = yaml.safe_load(match.group(1))
            relative_path = md_file.relative_to(category_path)

            return {
                'path': str(relative_path),
                'file': md_file,
                'title': frontmatter.get('title', md_file.stem),
                'tags': frontmatter.get('tags', []),
                'subcategory': frontmatter.get('subcategory', 'other'),
                'date': frontmatter.get('date', 'unknown'),
                'status': frontmatter.get('status', 'unknown'),
                'size': md_file.stat().st_size
            }

        except Exception:
            return None

    @staticmethod
    def scan_parallel(articles_dir: Path, category_path: Path, workers: int = None) -> List[Dict]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π"""
        if workers is None:
            workers = max(1, cpu_count() - 1)

        md_files = list(articles_dir.rglob("*.md"))
        args_list = [(md_file, category_path) for md_file in md_files]

        if not args_list:
            return []

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å multiprocessing —Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤
        if len(args_list) < 10:
            # Sequential –¥–ª—è –º–∞–ª—ã—Ö –æ–±—ä—ë–º–æ–≤
            results = [ParallelIndexer.process_article(args) for args in args_list]
        else:
            # Parallel –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤
            with Pool(processes=workers) as pool:
                results = pool.map(ParallelIndexer.process_article, args_list)

        return [r for r in results if r is not None]


class AdvancedIndexUpdater:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤"""

    def __init__(self, root_dir=".", incremental=False, workers=None):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.stats = defaultdict(lambda: defaultdict(int))
        self.incremental = incremental
        self.workers = workers or max(1, cpu_count() - 1)

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.change_tracker = ChangeTracker(self.root_dir / ".index_cache.json")
        self.validator = IndexValidator(self.root_dir)
        self.dependency_graph = DependencyGraph()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance = {
            'files_scanned': 0,
            'files_updated': 0,
            'files_skipped': 0,
            'errors': 0,
            'start_time': None,
            'end_time': None
        }

    def extract_frontmatter(self, file_path):
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ frontmatter"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
        if not match:
            return None

        try:
            frontmatter = yaml.safe_load(match.group(1))
            return frontmatter
        except yaml.YAMLError:
            return None

    def scan_articles(self, category_path, force=False):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å–∏ (—Å incremental support)"""
        articles_dir = category_path / "articles"

        if not articles_dir.exists():
            return []

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        all_articles = ParallelIndexer.scan_parallel(articles_dir, category_path, self.workers)

        self.performance['files_scanned'] += len(all_articles)

        # Incremental filtering
        if self.incremental and not force:
            changed_articles = []
            for article in all_articles:
                if self.change_tracker.has_changed(article['file']):
                    changed_articles.append(article)
                    self.performance['files_updated'] += 1
                else:
                    self.performance['files_skipped'] += 1

            return changed_articles

        # Full update
        for article in all_articles:
            self.change_tracker.has_changed(article['file'])  # Update cache
            self.performance['files_updated'] += 1

        return all_articles

    def group_by_subcategory(self, articles):
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å–∏ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        grouped = defaultdict(list)
        for article in articles:
            grouped[article['subcategory']].append(article)
        return grouped

    def update_category_index(self, category_name, force=False):
        """–û–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_path = self.knowledge_dir / category_name
        index_file = category_path / "index" / "INDEX.md"

        if not index_file.exists():
            print(f"‚ö†Ô∏è  –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_file}")
            self.performance['errors'] += 1
            return

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self.validator.validate_frontmatter(index_file):
            print(f"‚ö†Ô∏è  Invalid frontmatter in {index_file}")
            if not self.validator.repair_index(index_file):
                return

        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å–∏
        articles = self.scan_articles(category_path, force=force)

        # –ï—Å–ª–∏ incremental –∏ –Ω–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
        if self.incremental and not force and not articles:
            print(f"‚è≠Ô∏è  {category_name}: no changes detected")
            return

        # –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è - –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        if not self.incremental or force:
            articles = self.scan_articles(category_path, force=True)

        grouped = self.group_by_subcategory(articles)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_articles = len(articles)
        today = datetime.now().strftime("%Y-%m-%d")

        print(f"üìä {category_name}: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {total_articles} —Å—Ç–∞—Ç–µ–π")

        # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∏–Ω–¥–µ–∫—Å–µ
        with open(index_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # –û–±–Ω–æ–≤–∏—Ç—å –¥–∞—Ç—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        content = re.sub(
            r'date_updated: \d{4}-\d{2}-\d{2}',
            f'date_updated: {today}',
            content
        )

        # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Å–µ–∫—Ü–∏–∏ "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
        stats_pattern = r'(##\s+–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.*?\n)(.*?)(\n##|\Z)'

        def update_stats(match):
            header = match.group(1)
            old_stats = match.group(2)

            # –ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É —Å–æ–∑–¥–∞–Ω–∏—è
            date_created = today
            date_match = re.search(r'–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:\s*(\d{4}-\d{2}-\d{2})', old_stats)
            if date_match:
                date_created = date_match.group(1)

            stats_text = (
                f"- –í—Å–µ–≥–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–≤: {len(grouped)}\n"
                f"- –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total_articles}\n"
                f"- –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {date_created}\n"
                f"- –î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {today}\n"
            )
            return header + stats_text + match.group(3)

        content = re.sub(stats_pattern, update_stats, content, flags=re.DOTALL)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª–µ–Ω: {index_file}")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats[category_name]['total'] = total_articles
        self.stats[category_name]['subcategories'] = len(grouped)

    def update_main_index(self):
        """–û–±–Ω–æ–≤–∏—Ç—å –≥–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        index_file = self.root_dir / "INDEX.md"

        if not index_file.exists():
            self.create_main_index()
            return

        total_articles = sum(cat['total'] for cat in self.stats.values())
        total_categories = len(self.stats)
        today = datetime.now().strftime("%Y-%m-%d")

        print(f"\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {total_categories}")
        print(f"   –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total_articles}")

        # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        with open(index_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # –û–±–Ω–æ–≤–∏—Ç—å –¥–∞—Ç—É
        content = re.sub(
            r'date_updated: \d{4}-\d{2}-\d{2}',
            f'date_updated: {today}',
            content
        )

        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"‚úÖ –ì–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª–µ–Ω")

    def create_main_index(self):
        """–°–æ–∑–¥–∞—Ç—å –≥–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        index_file = self.root_dir / "INDEX.md"
        today = datetime.now().strftime("%Y-%m-%d")

        content = f"""---
title: "–ì–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"
type: main-index
date_created: {today}
date_updated: {today}
---

# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: –ì–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å

## –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–Ω–∞–Ω–∏–π

### üíª [–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏](knowledge/computers/index/INDEX.md)
–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, hardware, AI, —Å–µ—Ç–∏, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –¥—Ä—É–≥–∏–µ IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

### üè† [–ë—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –∏ –¥–æ–º–∞—à–Ω–µ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ](knowledge/household/index/INDEX.md)
–í—ã–±–æ—Ä –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –±—ã—Ç–æ–≤–æ–π —Ç–µ—Ö–Ω–∏–∫–∏, —Ä–µ–º–æ–Ω—Ç, —É–±–æ—Ä–∫–∞, —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

### üç≥ [–ö—É–ª–∏–Ω–∞—Ä–∏—è –∏ —Ä–µ—Ü–µ–ø—Ç—ã](knowledge/cooking/index/INDEX.md)
–†–µ—Ü–µ–ø—Ç—ã, —Ç–µ—Ö–Ω–∏–∫–∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è, –∫—É–ª–∏–Ω–∞—Ä–Ω—ã–µ —Å–æ–≤–µ—Ç—ã

## –°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã

- üì• [–í—Ö–æ–¥—è—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã](inbox/) - –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- üìö [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](docs/) - –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
- üõ†Ô∏è [–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã](tools/) - –°–∫—Ä–∏–ø—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
- üì¶ [–ê—Ä—Ö–∏–≤](archive/) - –£—Å—Ç–∞—Ä–µ–≤—à–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

1. –ù–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–æ–±–∞–≤–ª—è–π—Ç–µ –≤ `inbox/raw/`
2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç—ã –≤ `tools/`
3. –ß–∏—Ç–∞–π—Ç–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –≤ `docs/METHODOLOGY.md`
4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —à–∞–±–ª–æ–Ω—ã –∏–∑ `docs/TEMPLATES.md`

## –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π

**–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω–¥–µ–∫—Å–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
**–ü–æ —Ç–µ–≥–∞–º:** –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ grep –∏–ª–∏ —Å–∫—Ä–∏–ø—Ç—ã
**–ü–æ —Å—Å—ã–ª–∫–∞–º:** –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –ø–æ —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å—Ç–∞—Ç—å—è–º

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

–û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ `tools/update_indexes.py`

–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {today}
"""

        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω –≥–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {index_file}")

    def run(self, categories=None, validate_only=False):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        self.performance['start_time'] = datetime.now()

        mode = "Incremental" if self.incremental else "Full"
        print(f"üîÑ {mode} index update (workers: {self.workers})...\n")

        # –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if validate_only:
            self.validate_indexes()
            return

        # –ù–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if categories:
            category_list = categories
        else:
            category_list = [d.name for d in self.knowledge_dir.iterdir()
                           if d.is_dir() and not d.name.startswith('.')]

        # –û–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for category in category_list:
            try:
                self.update_category_index(category)
            except Exception as e:
                print(f"‚ùå Error updating {category}: {e}")
                self.performance['errors'] += 1

        # –û–±–Ω–æ–≤–∏—Ç—å –≥–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        self.update_main_index()

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –∏–∑–º–µ–Ω–µ–Ω–∏–π
        if not self.incremental:
            self.change_tracker.mark_full_update()
        self.change_tracker.save()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance['end_time'] = datetime.now()
        self.print_performance_report()

        print("\n‚úÖ –í—Å–µ –∏–Ω–¥–µ–∫—Å—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")

    def validate_indexes(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤"""
        print("üîç Validating indexes...\n")

        # –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –≥–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        main_index = self.root_dir / "INDEX.md"
        if main_index.exists():
            self.validator.validate_frontmatter(main_index)

        # –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for category_dir in self.knowledge_dir.iterdir():
            if not category_dir.is_dir():
                continue

            index_file = category_dir / "index" / "INDEX.md"
            if index_file.exists():
                self.validator.validate_frontmatter(index_file)
                broken_links = self.validator.validate_links(index_file)
                if broken_links:
                    self.validator.warnings.extend(broken_links)

        # –í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á—ë—Ç
        report = self.validator.get_report()
        print(report)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç
        report_file = self.root_dir / "INDEX_VALIDATION_REPORT.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\nüìÑ Report saved to {report_file}")

    def print_performance_report(self):
        """–í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        elapsed = (self.performance['end_time'] - self.performance['start_time']).total_seconds()

        print("\n" + "="*60)
        print("üìä PERFORMANCE REPORT")
        print("="*60)
        print(f"‚è±Ô∏è  Total time: {elapsed:.2f}s")
        print(f"üìÅ Files scanned: {self.performance['files_scanned']}")
        print(f"‚úèÔ∏è  Files updated: {self.performance['files_updated']}")
        print(f"‚è≠Ô∏è  Files skipped: {self.performance['files_skipped']}")
        print(f"‚ùå Errors: {self.performance['errors']}")

        if self.performance['files_scanned'] > 0:
            throughput = self.performance['files_scanned'] / elapsed
            print(f"üöÄ Throughput: {throughput:.1f} files/sec")

        print("="*60)


def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(
        description="Advanced Incremental Index Update System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                          # Full update (all categories)
  %(prog)s --incremental            # Incremental update (only changed files)
  %(prog)s --category cooking       # Update specific category
  %(prog)s --validate               # Validate indexes only
  %(prog)s --parallel 8             # Use 8 worker processes
  %(prog)s -i -p 4                  # Incremental with 4 workers
        """
    )

    parser.add_argument(
        '-i', '--incremental',
        action='store_true',
        help='Incremental update (only changed files)'
    )

    parser.add_argument(
        '-c', '--category',
        type=str,
        action='append',
        help='Update specific category (can be repeated)'
    )

    parser.add_argument(
        '-p', '--parallel',
        type=int,
        metavar='N',
        help=f'Number of parallel workers (default: {max(1, cpu_count()-1)})'
    )

    parser.add_argument(
        '-v', '--validate',
        action='store_true',
        help='Validate indexes only (no updates)'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='Force full update (ignore cache)'
    )

    args = parser.parse_args()

    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    # –°–æ–∑–¥–∞—Ç—å updater
    updater = AdvancedIndexUpdater(
        root_dir,
        incremental=args.incremental and not args.force,
        workers=args.parallel
    )

    # –ó–∞–ø—É—Å—Ç–∏—Ç—å
    updater.run(
        categories=args.category,
        validate_only=args.validate
    )


if __name__ == "__main__":
    main()
