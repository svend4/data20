#!/usr/bin/env python3
"""
Duplicate Detector - –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è –∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Google duplicate content detection, Copyscape
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import hashlib
import json
import argparse
import math
import csv
from datetime import datetime
from typing import Dict, List, Tuple, Set


class DuplicateDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""

    def __init__(self, root_dir=".", similarity_threshold=0.8):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.similarity_threshold = similarity_threshold

        # –î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–µ–π
        self.articles = {}

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.duplicates = {
            'exact': [],
            'near_duplicate': [],
            'similar_titles': []
        }

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def normalize_text(self, text):
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        # –£–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª–∏—Ç—å markdown —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
        text = re.sub(r'[#*`\[\]()]', '', text)
        # Lowercase
        text = text.lower().strip()
        return text

    def calculate_hash(self, text):
        """–í—ã—á–∏—Å–ª–∏—Ç—å MD5 —Ö–µ—à —Ç–µ–∫—Å—Ç–∞"""
        normalized = self.normalize_text(text)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()

    def calculate_similarity(self, text1, text2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ (Jaccard similarity)"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words1 = set(re.findall(r'\b\w+\b', text1.lower()))
        words2 = set(re.findall(r'\b\w+\b', text2.lower()))

        if not words1 or not words2:
            return 0.0

        # Jaccard similarity
        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def levenshtein_distance(self, s1, s2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)

        for i, c1 in enumerate(s1):
            current_row = [i + 1]

            for j, c2 in enumerate(s2):
                # –°—Ç–æ–∏–º–æ—Å—Ç—å –≤—Å—Ç–∞–≤–∫–∏, —É–¥–∞–ª–µ–Ω–∏—è –∏–ª–∏ –∑–∞–º–µ–Ω—ã
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)

                current_row.append(min(insertions, deletions, substitutions))

            previous_row = current_row

        return previous_row[-1]

    def title_similarity(self, title1, title2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        t1 = self.normalize_text(title1)
        t2 = self.normalize_text(title2)

        if t1 == t2:
            return 1.0

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
        max_len = max(len(t1), len(t2))
        if max_len == 0:
            return 0.0

        distance = self.levenshtein_distance(t1, t2)
        similarity = 1.0 - (distance / max_len)

        return similarity

    def collect_articles(self):
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üìö –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            self.articles[article_path] = {
                'title': title,
                'content': content,
                'hash': self.calculate_hash(content),
                'word_count': len(re.findall(r'\b\w+\b', content))
            }

        print(f"   –°—Ç–∞—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ: {len(self.articles)}\n")

    def find_exact_duplicates(self):
        """–ù–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ —Ö–µ—à—É)"""
        print("üîç –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ö–µ—à—É
        by_hash = defaultdict(list)

        for article_path, data in self.articles.items():
            by_hash[data['hash']].append(article_path)

        # –ù–∞–π—Ç–∏ –≥—Ä—É–ø–ø—ã —Å –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å—ë–π
        for hash_value, articles in by_hash.items():
            if len(articles) > 1:
                self.duplicates['exact'].append({
                    'type': 'exact',
                    'articles': [
                        {
                            'path': article,
                            'title': self.articles[article]['title']
                        }
                        for article in articles
                    ],
                    'similarity': 1.0
                })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(self.duplicates['exact'])}\n")

    def find_near_duplicates(self):
        """–ù–∞–π—Ç–∏ –ø–æ—á—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã (–≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)"""
        print("üîé –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...\n")

        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self.calculate_similarity(data1['content'], data2['content'])

                if similarity >= self.similarity_threshold:
                    self.duplicates['near_duplicate'].append({
                        'type': 'near_duplicate',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π: {len(self.duplicates['near_duplicate'])}\n")

    def find_similar_titles(self):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        print("üìù –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤...\n")

        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                similarity = self.title_similarity(data1['title'], data2['title'])

                if similarity >= 0.7:  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    self.duplicates['similar_titles'].append({
                        'type': 'similar_title',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏: {len(self.duplicates['similar_titles'])}\n")

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîç –û—Ç—á—ë—Ç: –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n\n")
        lines.append("> –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ—Å—è –∏ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_issues = (
            len(self.duplicates['exact']) +
            len(self.duplicates['near_duplicate']) +
            len(self.duplicates['similar_titles'])
        )

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ**: {len(self.articles)}\n")
        lines.append(f"- **–¢–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤**: {len(self.duplicates['exact'])}\n")
        lines.append(f"- **–ü–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π**: {len(self.duplicates['near_duplicate'])}\n")
        lines.append(f"- **–ü–æ—Ö–æ–∂–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {len(self.duplicates['similar_titles'])}\n")
        lines.append(f"- **–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º**: {total_issues}\n\n")

        if total_issues == 0:
            lines.append("‚úÖ **–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!**\n\n")
        else:
            lines.append("‚ö†Ô∏è  **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã**\n\n")

        # –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if self.duplicates['exact']:
            lines.append("## ‚õî –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã\n\n")
            lines.append("> –ò–¥–µ–Ω—Ç–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (100% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)\n\n")

            for i, dup in enumerate(self.duplicates['exact'], 1):
                lines.append(f"### –ì—Ä—É–ø–ø–∞ {i}\n\n")

                for article in dup['articles']:
                    lines.append(f"- **{article['title']}**\n")
                    lines.append(f"  - `{article['path']}`\n")

                lines.append("\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é\n\n")

        # –ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏
        if self.duplicates['near_duplicate']:
            lines.append("## ‚ö†Ô∏è  –ü–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç\n\n")
            lines.append(f"> –°—Ö–æ–¥—Å—Ç–≤–æ >= {self.similarity_threshold * 100:.0f}%\n\n")

            for i, dup in enumerate(self.duplicates['near_duplicate'], 1):
                lines.append(f"### –ü–∞—Ä–∞ {i} ‚Äî –°—Ö–æ–¥—Å—Ç–≤–æ: {dup['similarity'] * 100:.1f}%\n\n")

                for article in dup['articles']:
                    lines.append(f"- **{article['title']}**\n")
                    lines.append(f"  - [{article['path']}]({article['path']})\n")

                lines.append("\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–ª–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å\n\n")

        # –ü–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if self.duplicates['similar_titles']:
            lines.append("## üìù –ü–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏\n\n")
            lines.append("> –ú–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Ç–∞–Ω–∏—Ü–µ–π –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª–µ–π\n\n")

            for i, dup in enumerate(self.duplicates['similar_titles'], 1):
                lines.append(f"### –ü–∞—Ä–∞ {i} ‚Äî –°—Ö–æ–¥—Å—Ç–≤–æ: {dup['similarity'] * 100:.1f}%\n\n")

                for article in dup['articles']:
                    lines.append(f"- **{article['title']}**\n")
                    lines.append(f"  - [{article['path']}]({article['path']})\n")

                lines.append("\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –°–¥–µ–ª–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±–æ–ª–µ–µ —Ä–∞–∑–ª–∏—á–∏–º—ã–º–∏\n\n")

        output_file = self.root_dir / "DUPLICATES_REPORT.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON"""
        output_file = self.root_dir / "duplicates.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.duplicates, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")

    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å cosine similarity —Å TF-IDF

        cos(Œ∏) = (A¬∑B) / (||A|| √ó ||B||)
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words1 = re.findall(r'\b\w+\b', text1.lower())
        words2 = re.findall(r'\b\w+\b', text2.lower())

        # TF (Term Frequency)
        tf1 = Counter(words1)
        tf2 = Counter(words2)

        # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        all_words = set(tf1.keys()) | set(tf2.keys())

        # –í–µ–∫—Ç–æ—Ä—ã
        vec1 = [tf1.get(word, 0) for word in all_words]
        vec2 = [tf2.get(word, 0) for word in all_words]

        # Dot product
        dot_product = sum(a * b for a, b in zip(vec1, vec2))

        # Magnitudes
        mag1 = math.sqrt(sum(a * a for a in vec1))
        mag2 = math.sqrt(sum(b * b for b in vec2))

        if mag1 == 0 or mag2 == 0:
            return 0.0

        return dot_product / (mag1 * mag2)

    def get_shingles(self, text: str, k: int = 3) -> Set[str]:
        """
        –°–æ–∑–¥–∞—Ç—å k-shingles (n-grams) –¥–ª—è —Ç–µ–∫—Å—Ç–∞

        Example: "hello world" with k=3 ‚Üí {"hel", "ell", "llo", ...}
        """
        normalized = self.normalize_text(text)
        shingles = set()

        for i in range(len(normalized) - k + 1):
            shingle = normalized[i:i + k]
            shingles.add(shingle)

        return shingles

    def shingle_similarity(self, text1: str, text2: str, k: int = 3) -> float:
        """Jaccard similarity –Ω–∞ –æ—Å–Ω–æ–≤–µ shingles"""
        shingles1 = self.get_shingles(text1, k)
        shingles2 = self.get_shingles(text2, k)

        if not shingles1 or not shingles2:
            return 0.0

        intersection = len(shingles1 & shingles2)
        union = len(shingles1 | shingles2)

        return intersection / union if union > 0 else 0.0


class AdvancedDuplicateDetector(DuplicateDetector):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏"""

    def find_duplicates_by_cosine(self, threshold: float = 0.8) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è cosine similarity"""
        print(f"üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (cosine similarity >= {threshold})...\n")

        duplicates = []
        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                similarity = self.calculate_cosine_similarity(data1['content'], data2['content'])

                if similarity >= threshold:
                    duplicates.append({
                        'type': 'cosine_duplicate',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity,
                        'method': 'cosine'
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä (cosine): {len(duplicates)}\n")
        return duplicates

    def find_duplicates_by_shingles(self, threshold: float = 0.7, k: int = 3) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è shingle similarity"""
        print(f"üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (shingles k={k}, threshold={threshold})...\n")

        duplicates = []
        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                similarity = self.shingle_similarity(data1['content'], data2['content'], k)

                if similarity >= threshold:
                    duplicates.append({
                        'type': 'shingle_duplicate',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity,
                        'method': f'shingles-{k}'
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä (shingles): {len(duplicates)}\n")
        return duplicates

    def analyze_similarity_distribution(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏"""
        print("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞...\n")

        similarities = []
        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                jaccard = self.calculate_similarity(data1['content'], data2['content'])
                cosine = self.calculate_cosine_similarity(data1['content'], data2['content'])

                similarities.append({
                    'pair': (data1['title'], data2['title']),
                    'jaccard': jaccard,
                    'cosine': cosine
                })

        if not similarities:
            return {}

        # Statistics
        jaccard_scores = [s['jaccard'] for s in similarities]
        cosine_scores = [s['cosine'] for s in similarities]

        stats = {
            'total_pairs': len(similarities),
            'jaccard': {
                'mean': sum(jaccard_scores) / len(jaccard_scores),
                'max': max(jaccard_scores),
                'min': min(jaccard_scores)
            },
            'cosine': {
                'mean': sum(cosine_scores) / len(cosine_scores),
                'max': max(cosine_scores),
                'min': min(cosine_scores)
            },
            'top_similar': sorted(similarities, key=lambda x: x['cosine'], reverse=True)[:5]
        }

        print(f"   –í—Å–µ–≥–æ –ø–∞—Ä –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {stats['total_pairs']}")
        print(f"   Jaccard: mean={stats['jaccard']['mean']:.3f}, max={stats['jaccard']['max']:.3f}")
        print(f"   Cosine: mean={stats['cosine']['mean']:.3f}, max={stats['cosine']['max']:.3f}\n")

        return stats


class SimilarityAnalyzer:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É Jaccard, Cosine, Levenshtein, Shingles
    """

    def __init__(self, detector):
        self.detector = detector
        self.comparisons = []

    def compare_all_metrics(self):
        """–°—Ä–∞–≤–Ω–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã"""
        print("üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞...\n")

        articles_list = list(self.detector.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                jaccard = self.detector.calculate_similarity(data1['content'], data2['content'])
                cosine = self.detector.calculate_cosine_similarity(data1['content'], data2['content'])
                shingles = self.detector.shingle_similarity(data1['content'], data2['content'], k=3)

                # Title similarity (Levenshtein-based)
                title_sim = self.detector.title_similarity(data1['title'], data2['title'])

                self.comparisons.append({
                    'pair': (data1['title'], data2['title']),
                    'paths': (path1, path2),
                    'jaccard': jaccard,
                    'cosine': cosine,
                    'shingles': shingles,
                    'title_levenshtein': title_sim
                })

        print(f"   –°—Ä–∞–≤–Ω–µ–Ω–æ –ø–∞—Ä: {len(self.comparisons)}\n")

    def calculate_correlation(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        if not self.comparisons:
            return {}

        # –ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏—è
        jaccard_vals = [c['jaccard'] for c in self.comparisons]
        cosine_vals = [c['cosine'] for c in self.comparisons]
        shingles_vals = [c['shingles'] for c in self.comparisons]

        # –ü—Ä–æ—Å—Ç–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞
        def pearson_correlation(x, y):
            n = len(x)
            mean_x = sum(x) / n
            mean_y = sum(y) / n

            numerator = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))
            denominator_x = math.sqrt(sum((x[i] - mean_x) ** 2 for i in range(n)))
            denominator_y = math.sqrt(sum((y[i] - mean_y) ** 2 for i in range(n)))

            if denominator_x == 0 or denominator_y == 0:
                return 0.0

            return numerator / (denominator_x * denominator_y)

        correlations = {
            'jaccard_cosine': pearson_correlation(jaccard_vals, cosine_vals),
            'jaccard_shingles': pearson_correlation(jaccard_vals, shingles_vals),
            'cosine_shingles': pearson_correlation(cosine_vals, shingles_vals)
        }

        return correlations

    def find_metric_disagreements(self, threshold_diff=0.3):
        """–ù–∞–π—Ç–∏ –ø–∞—Ä—ã, –≥–¥–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∏–ª—å–Ω–æ —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è"""
        disagreements = []

        for comp in self.comparisons:
            metrics = [comp['jaccard'], comp['cosine'], comp['shingles']]
            max_diff = max(metrics) - min(metrics)

            if max_diff >= threshold_diff:
                disagreements.append({
                    'pair': comp['pair'],
                    'jaccard': comp['jaccard'],
                    'cosine': comp['cosine'],
                    'shingles': comp['shingles'],
                    'max_diff': max_diff
                })

        return sorted(disagreements, key=lambda x: -x['max_diff'])

    def generate_metrics_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
        lines = []
        lines.append("# üî¨ –û—Ç—á—ë—Ç: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞\n\n")

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlations = self.calculate_correlation()
        lines.append("## –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏\n\n")
        lines.append(f"- **Jaccard ‚Üî Cosine**: {correlations.get('jaccard_cosine', 0):.3f}\n")
        lines.append(f"- **Jaccard ‚Üî Shingles**: {correlations.get('jaccard_shingles', 0):.3f}\n")
        lines.append(f"- **Cosine ‚Üî Shingles**: {correlations.get('cosine_shingles', 0):.3f}\n\n")

        # –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
        disagreements = self.find_metric_disagreements(threshold_diff=0.3)
        if disagreements:
            lines.append(f"## –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ (—Ç–æ–ø-10)\n\n")
            lines.append("> –ü–∞—Ä—ã, –≥–¥–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∏–ª—å–Ω–æ —Ä–∞—Å—Ö–æ–¥—è—Ç—Å—è\n\n")

            for i, dis in enumerate(disagreements[:10], 1):
                lines.append(f"### {i}. {dis['pair'][0]} ‚Üî {dis['pair'][1]}\n\n")
                lines.append(f"- Jaccard: {dis['jaccard']:.3f}\n")
                lines.append(f"- Cosine: {dis['cosine']:.3f}\n")
                lines.append(f"- Shingles: {dis['shingles']:.3f}\n")
                lines.append(f"- **–ú–∞–∫—Å. —Ä–∞–∑–Ω–∏—Ü–∞**: {dis['max_diff']:.3f}\n\n")

        return ''.join(lines)


class ClusterAnalyzer:
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã
    """

    def __init__(self, detector):
        self.detector = detector
        self.clusters = []

    def simple_clustering(self, similarity_threshold=0.6):
        """–ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: greedy algorithm"""
        print(f"üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (threshold={similarity_threshold})...\n")

        articles_list = list(self.detector.articles.items())
        visited = set()

        for i, (path1, data1) in enumerate(articles_list):
            if path1 in visited:
                continue

            # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
            cluster = {
                'representative': {'path': path1, 'title': data1['title']},
                'members': [{'path': path1, 'title': data1['title']}],
                'avg_similarity': 0.0
            }

            similarities = []

            # –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            for j, (path2, data2) in enumerate(articles_list):
                if i == j or path2 in visited:
                    continue

                similarity = self.detector.calculate_similarity(data1['content'], data2['content'])

                if similarity >= similarity_threshold:
                    cluster['members'].append({'path': path2, 'title': data2['title']})
                    visited.add(path2)
                    similarities.append(similarity)

            visited.add(path1)

            # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            if similarities:
                cluster['avg_similarity'] = sum(similarities) / len(similarities)

            # –î–æ–±–∞–≤–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –Ω—ë–º > 1 –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if len(cluster['members']) > 1:
                self.clusters.append(cluster)

        print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(self.clusters)}\n")

    def get_cluster_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        if not self.clusters:
            return {}

        cluster_sizes = [len(c['members']) for c in self.clusters]

        stats = {
            'total_clusters': len(self.clusters),
            'total_documents_clustered': sum(cluster_sizes),
            'avg_cluster_size': sum(cluster_sizes) / len(cluster_sizes),
            'max_cluster_size': max(cluster_sizes),
            'min_cluster_size': min(cluster_sizes),
            'largest_cluster': max(self.clusters, key=lambda c: len(c['members']))
        }

        return stats

    def generate_cluster_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        lines = []
        lines.append("# üîó –û—Ç—á—ë—Ç: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n\n")

        stats = self.get_cluster_statistics()

        if not stats:
            lines.append("‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n\n")
            return ''.join(lines)

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤**: {stats['total_clusters']}\n")
        lines.append(f"- **–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö**: {stats['total_documents_clustered']}\n")
        lines.append(f"- **–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞**: {stats['avg_cluster_size']:.1f}\n")
        lines.append(f"- **–ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞**: {stats['max_cluster_size']}\n\n")

        # –ö–ª–∞—Å—Ç–µ—Ä—ã
        lines.append("## –ö–ª–∞—Å—Ç–µ—Ä—ã\n\n")

        for i, cluster in enumerate(self.clusters, 1):
            lines.append(f"### –ö–ª–∞—Å—Ç–µ—Ä {i} ({len(cluster['members'])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)\n\n")
            lines.append(f"**–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å**: {cluster['representative']['title']}\n\n")
            lines.append(f"**–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ**: {cluster['avg_similarity']:.3f}\n\n")
            lines.append("**–ß–ª–µ–Ω—ã –∫–ª–∞—Å—Ç–µ—Ä–∞**:\n\n")

            for member in cluster['members']:
                lines.append(f"- {member['title']}\n")
                lines.append(f"  - `{member['path']}`\n")

            lines.append("\n")

        return ''.join(lines)


class DuplicateVisualizer:
    """
    HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    Dashboard —Å Chart.js –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    """

    def __init__(self, detector, similarity_analyzer=None, cluster_analyzer=None):
        self.detector = detector
        self.similarity_analyzer = similarity_analyzer
        self.cluster_analyzer = cluster_analyzer

    def generate_html_dashboard(self, output_file='DUPLICATES_DASHBOARD.html'):
        """–°–æ–∑–¥–∞—Ç—å HTML dashboard"""
        print("üé® –°–æ–∑–¥–∞–Ω–∏–µ HTML dashboard...\n")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        stats = self._prepare_statistics()
        chart_data = self._prepare_chart_data()

        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üîç Duplicate Detector Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}

        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 40px 20px;
        }}

        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}

        h1 {{
            color: white;
            text-align: center;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}

        .subtitle {{
            color: rgba(255,255,255,0.9);
            text-align: center;
            font-size: 1.2em;
            margin-bottom: 40px;
        }}

        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }}

        .stat-card {{
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }}

        .stat-label {{
            color: #666;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }}

        .stat-value {{
            color: #667eea;
            font-size: 3em;
            font-weight: bold;
        }}

        .chart-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
            margin-bottom: 40px;
        }}

        .chart-container {{
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }}

        .chart-title {{
            font-size: 1.3em;
            color: #333;
            margin-bottom: 20px;
            font-weight: 600;
        }}

        canvas {{
            max-height: 350px;
        }}

        .footer {{
            text-align: center;
            color: rgba(255,255,255,0.8);
            margin-top: 40px;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üîç Duplicate Detector Dashboard</h1>
        <p class="subtitle">–ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞</p>

        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-label">–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π</div>
                <div class="stat-value">{stats['total_articles']}</div>
            </div>

            <div class="stat-card">
                <div class="stat-label">–¢–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤</div>
                <div class="stat-value">{stats['exact_duplicates']}</div>
            </div>

            <div class="stat-card">
                <div class="stat-label">–ü–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä</div>
                <div class="stat-value">{stats['near_duplicates']}</div>
            </div>

            <div class="stat-card">
                <div class="stat-label">–ö–ª–∞—Å—Ç–µ—Ä–æ–≤</div>
                <div class="stat-value">{stats['clusters']}</div>
            </div>
        </div>

        <div class="chart-grid">
            <div class="chart-container">
                <div class="chart-title">üìä –¢–∏–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤</div>
                <canvas id="duplicateTypesChart"></canvas>
            </div>

            <div class="chart-container">
                <div class="chart-title">üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ (Jaccard)</div>
                <canvas id="similarityDistChart"></canvas>
            </div>

            <div class="chart-container">
                <div class="chart-title">üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫</div>
                <canvas id="metricsComparisonChart"></canvas>
            </div>

            <div class="chart-container">
                <div class="chart-title">üîó –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤</div>
                <canvas id="clusterSizesChart"></canvas>
            </div>
        </div>

        <div class="footer">
            –°–æ–∑–¥–∞–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M')} | Duplicate Detector v2.0
        </div>
    </div>

    <script>
        // –¢–∏–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        new Chart(document.getElementById('duplicateTypesChart'), {{
            type: 'doughnut',
            data: {{
                labels: {chart_data['duplicate_types']['labels']},
                datasets: [{{
                    data: {chart_data['duplicate_types']['values']},
                    backgroundColor: ['#667eea', '#764ba2', '#f093fb', '#4facfe']
                }}]
            }},
            options: {{
                responsive: true,
                maintainAspectRatio: true,
                plugins: {{
                    legend: {{ position: 'bottom' }}
                }}
            }}
        }});

        // –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
        new Chart(document.getElementById('similarityDistChart'), {{
            type: 'bar',
            data: {{
                labels: {chart_data['similarity_dist']['labels']},
                datasets: [{{
                    label: '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä',
                    data: {chart_data['similarity_dist']['values']},
                    backgroundColor: '#667eea'
                }}]
            }},
            options: {{
                responsive: true,
                maintainAspectRatio: true,
                plugins: {{
                    legend: {{ display: false }}
                }},
                scales: {{
                    y: {{ beginAtZero: true }}
                }}
            }}
        }});

        // –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        new Chart(document.getElementById('metricsComparisonChart'), {{
            type: 'radar',
            data: {{
                labels: ['Jaccard', 'Cosine', 'Shingles', 'Levenshtein'],
                datasets: [{{
                    label: '–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ',
                    data: {chart_data['metrics_comparison']},
                    backgroundColor: 'rgba(102, 126, 234, 0.2)',
                    borderColor: '#667eea',
                    borderWidth: 2
                }}]
            }},
            options: {{
                responsive: true,
                maintainAspectRatio: true,
                scales: {{
                    r: {{
                        beginAtZero: true,
                        max: 1.0
                    }}
                }}
            }}
        }});

        // –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        new Chart(document.getElementById('clusterSizesChart'), {{
            type: 'bar',
            data: {{
                labels: {chart_data['cluster_sizes']['labels']},
                datasets: [{{
                    label: '–†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞',
                    data: {chart_data['cluster_sizes']['values']},
                    backgroundColor: '#764ba2'
                }}]
            }},
            options: {{
                responsive: true,
                maintainAspectRatio: true,
                plugins: {{
                    legend: {{ display: false }}
                }},
                scales: {{
                    y: {{ beginAtZero: true }}
                }}
            }}
        }});
    </script>
</body>
</html>"""

        output_path = self.detector.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"‚úÖ HTML Dashboard: {output_path}\n")

    def _prepare_statistics(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats = {
            'total_articles': len(self.detector.articles),
            'exact_duplicates': len(self.detector.duplicates.get('exact', [])),
            'near_duplicates': len(self.detector.duplicates.get('near_duplicate', [])),
            'clusters': 0
        }

        if self.cluster_analyzer and self.cluster_analyzer.clusters:
            stats['clusters'] = len(self.cluster_analyzer.clusters)

        return stats

    def _prepare_chart_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        chart_data = {
            'duplicate_types': {
                'labels': ['–¢–æ—á–Ω—ã–µ', '–ü–æ—Ö–æ–∂–∏–µ', '–ü–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏', '–ö–ª–∞—Å—Ç–µ—Ä—ã'],
                'values': [
                    len(self.detector.duplicates.get('exact', [])),
                    len(self.detector.duplicates.get('near_duplicate', [])),
                    len(self.detector.duplicates.get('similar_titles', [])),
                    len(self.cluster_analyzer.clusters) if self.cluster_analyzer else 0
                ]
            },
            'similarity_dist': self._get_similarity_distribution(),
            'metrics_comparison': self._get_metrics_comparison(),
            'cluster_sizes': self._get_cluster_sizes()
        }

        return chart_data

    def _get_similarity_distribution(self):
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities = []

        for dup in self.detector.duplicates.get('near_duplicate', []):
            similarities.append(dup['similarity'])

        # –°–æ–∑–¥–∞—Ç—å –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
        bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
        labels = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']
        counts = [0] * (len(bins) - 1)

        for sim in similarities:
            for i in range(len(bins) - 1):
                if bins[i] <= sim < bins[i + 1]:
                    counts[i] += 1
                    break

        return {'labels': labels, 'values': counts}

    def _get_metrics_comparison(self):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        if not self.similarity_analyzer or not self.similarity_analyzer.comparisons:
            return [0, 0, 0, 0]

        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        jaccard_avg = sum(c['jaccard'] for c in self.similarity_analyzer.comparisons) / len(self.similarity_analyzer.comparisons)
        cosine_avg = sum(c['cosine'] for c in self.similarity_analyzer.comparisons) / len(self.similarity_analyzer.comparisons)
        shingles_avg = sum(c['shingles'] for c in self.similarity_analyzer.comparisons) / len(self.similarity_analyzer.comparisons)
        levenshtein_avg = sum(c['title_levenshtein'] for c in self.similarity_analyzer.comparisons) / len(self.similarity_analyzer.comparisons)

        return [
            round(jaccard_avg, 3),
            round(cosine_avg, 3),
            round(shingles_avg, 3),
            round(levenshtein_avg, 3)
        ]

    def _get_cluster_sizes(self):
        """–†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        if not self.cluster_analyzer or not self.cluster_analyzer.clusters:
            return {'labels': [], 'values': []}

        labels = [f"–ö–ª–∞—Å—Ç–µ—Ä {i+1}" for i in range(len(self.cluster_analyzer.clusters))]
        values = [len(c['members']) for c in self.cluster_analyzer.clusters]

        return {'labels': labels, 'values': values}


class MergeRecommender:
    """
    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–∏—è–Ω–∏—é –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
    """

    def __init__(self, detector):
        self.detector = detector
        self.recommendations = []

    def analyze_duplicates(self):
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        print("üí° –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...\n")

        # –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã - –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–ª–∏—è–Ω–∏–∏
        for dup in self.detector.duplicates.get('exact', []):
            self.recommendations.append({
                'type': 'exact_duplicate',
                'articles': dup['articles'],
                'action': 'merge',
                'confidence': 1.0,
                'reason': '–ò–¥–µ–Ω—Ç–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (100% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)'
            })

        # –ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–∂–µ
        for dup in self.detector.duplicates.get('near_duplicate', []):
            similarity = dup['similarity']

            if similarity >= 0.95:
                action = 'merge'
                reason = f'–û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ({similarity*100:.1f}%)'
                confidence = 0.95
            elif similarity >= 0.85:
                action = 'review'
                reason = f'–í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ({similarity*100:.1f}%), —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏'
                confidence = 0.7
            else:
                action = 'keep_separate'
                reason = f'–£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ({similarity*100:.1f}%), –≤–µ—Ä–æ—è—Ç–Ω–æ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã'
                confidence = 0.5

            self.recommendations.append({
                'type': 'near_duplicate',
                'articles': dup['articles'],
                'action': action,
                'confidence': confidence,
                'reason': reason,
                'similarity': similarity
            })

        print(f"   –°–æ–∑–¥–∞–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(self.recommendations)}\n")

    def get_recommendations_by_action(self, action):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∏–ø—É –¥–µ–π—Å—Ç–≤–∏—è"""
        return [r for r in self.recommendations if r['action'] == action]

    def generate_merge_plan(self):
        """–°–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω —Å–ª–∏—è–Ω–∏—è"""
        lines = []
        lines.append("# üí° –ü–ª–∞–Ω —Å–ª–∏—è–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n\n")
        lines.append(f"> –°–æ–∑–¥–∞–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        merge_count = len([r for r in self.recommendations if r['action'] == 'merge'])
        review_count = len([r for r in self.recommendations if r['action'] == 'review'])
        keep_count = len([r for r in self.recommendations if r['action'] == 'keep_separate'])

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n\n")
        lines.append(f"- **–°–ª–∏—Ç—å**: {merge_count}\n")
        lines.append(f"- **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä—É—á–Ω—É—é**: {review_count}\n")
        lines.append(f"- **–û—Å—Ç–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª—å–Ω–æ**: {keep_count}\n")
        lines.append(f"- **–í—Å–µ–≥–æ**: {len(self.recommendations)}\n\n")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–∏—è–Ω–∏—é
        merge_recs = self.get_recommendations_by_action('merge')
        if merge_recs:
            lines.append("## ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–ª–∏—Ç—å\n\n")

            for i, rec in enumerate(merge_recs, 1):
                lines.append(f"### {i}. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {rec['confidence']*100:.0f}%\n\n")
                lines.append(f"**–ü—Ä–∏—á–∏–Ω–∞**: {rec['reason']}\n\n")

                for article in rec['articles']:
                    lines.append(f"- {article['title']}\n")
                    lines.append(f"  - `{article['path']}`\n")

                lines.append("\n**–î–µ–π—Å—Ç–≤–∏–µ**: –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–∞–º—É—é –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é\n\n")

        # –¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
        review_recs = self.get_recommendations_by_action('review')
        if review_recs:
            lines.append("## ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏\n\n")

            for i, rec in enumerate(review_recs, 1):
                lines.append(f"### {i}. –°—Ö–æ–¥—Å—Ç–≤–æ: {rec.get('similarity', 0)*100:.1f}%\n\n")
                lines.append(f"**–ü—Ä–∏—á–∏–Ω–∞**: {rec['reason']}\n\n")

                for article in rec['articles']:
                    lines.append(f"- {article['title']}\n")
                    lines.append(f"  - `{article['path']}`\n")

                lines.append("\n**–î–µ–π—Å—Ç–≤–∏–µ**: –°—Ä–∞–≤–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é, —Ä–µ—à–∏—Ç—å –æ —Å–ª–∏—è–Ω–∏–∏\n\n")

        return ''.join(lines)

    def export_to_csv(self, output_file='merge_recommendations.csv'):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ CSV"""
        csv_path = self.detector.root_dir / output_file

        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['–¢–∏–ø', '–°—Ç–∞—Ç—å—è 1', '–°—Ç–∞—Ç—å—è 2', '–î–µ–π—Å—Ç–≤–∏–µ', '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', '–ü—Ä–∏—á–∏–Ω–∞'])

            for rec in self.recommendations:
                if len(rec['articles']) >= 2:
                    writer.writerow([
                        rec['type'],
                        rec['articles'][0]['title'],
                        rec['articles'][1]['title'],
                        rec['action'],
                        f"{rec['confidence']:.2f}",
                        rec['reason']
                    ])

        print(f"‚úÖ CSV —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {csv_path}\n")


def main():
    parser = argparse.ArgumentParser(
        description='üîç Duplicate Detector v2.0 - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                              # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
  %(prog)s --html                       # HTML dashboard —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
  %(prog)s --cluster                    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
  %(prog)s --compare-metrics            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞
  %(prog)s --recommend-merges           # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–∏—è–Ω–∏—é
  %(prog)s --csv                        # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ CSV
  %(prog)s --all                        # –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ä–∞–∑—É
  %(prog)s --threshold 0.9 --cluster    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ—Ä–æ–≥–æ–º 0.9
  %(prog)s --clustering-method advanced # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è

–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ v2.0:
  - üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 4 –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞ (Jaccard, Cosine, Shingles, Levenshtein)
  - üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
  - üé® HTML dashboard —Å Chart.js –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏
  - üí° –£–º–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–∏—è–Ω–∏—é
  - üìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
  - üìà –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
    )

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('-t', '--threshold', type=float, default=0.8,
                       help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (0.0-1.0, default: 0.8)')
    parser.add_argument('--method', type=str, choices=['jaccard', 'cosine', 'shingles', 'all'],
                       default='all', help='–ú–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤')

    # –ù–æ–≤—ã–µ –æ–ø—Ü–∏–∏ v2.0
    parser.add_argument('--html', action='store_true',
                       help='üé® –°–æ–∑–¥–∞—Ç—å HTML dashboard —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏')
    parser.add_argument('--cluster', action='store_true',
                       help='üîó –í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--compare-metrics', action='store_true',
                       help='üî¨ –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞')
    parser.add_argument('--recommend-merges', action='store_true',
                       help='üí° –°–æ–∑–¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–∏—è–Ω–∏—é –¥—É–±–ª–∏–∫–∞—Ç–æ–≤')
    parser.add_argument('--csv', action='store_true',
                       help='üìä –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ CSV')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    parser.add_argument('--min-cluster-size', type=int, default=2,
                       help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ (default: 2)')
    parser.add_argument('--cluster-threshold', type=float, default=0.6,
                       help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (default: 0.6)')
    parser.add_argument('--clustering-method', type=str, choices=['simple', 'advanced'],
                       default='simple', help='–ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏')

    # –≠–∫—Å–ø–æ—Ä—Ç
    parser.add_argument('--export-metrics', action='store_true',
                       help='üìà –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫')

    # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ä–µ–∂–∏–º—ã
    parser.add_argument('--advanced', action='store_true',
                       help='üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã (cosine, shingles)')
    parser.add_argument('--analyze', action='store_true',
                       help='üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞')
    parser.add_argument('--all', action='store_true',
                       help='üî• –í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –æ–ø—Ü–∏–∏ (–ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑)')

    args = parser.parse_args()

    # --all –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –æ–ø—Ü–∏–∏
    if args.all:
        args.html = True
        args.cluster = True
        args.compare_metrics = True
        args.recommend_merges = True
        args.csv = True
        args.export_metrics = True
        args.advanced = True
        args.analyze = True

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    # –í—ã–±–æ—Ä –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    if args.advanced or args.method in ['cosine', 'shingles', 'all'] or args.all:
        detector = AdvancedDuplicateDetector(root_dir, similarity_threshold=args.threshold)
    else:
        detector = DuplicateDetector(root_dir, similarity_threshold=args.threshold)

    # –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π
    detector.collect_articles()

    # Standard methods
    if args.method in ['jaccard', 'all']:
        detector.find_exact_duplicates()
        detector.find_near_duplicates()
        detector.find_similar_titles()

    # Advanced methods
    if args.advanced or args.method in ['cosine', 'shingles', 'all']:
        if isinstance(detector, AdvancedDuplicateDetector):
            if args.method in ['cosine', 'all']:
                detector.find_duplicates_by_cosine(args.threshold)
            if args.method in ['shingles', 'all']:
                detector.find_duplicates_by_shingles(threshold=0.7, k=3)

    # Analyze
    if args.analyze and isinstance(detector, AdvancedDuplicateDetector):
        stats = detector.analyze_similarity_distribution()

        print("üìä –¢–æ–ø-5 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä:\n")
        for i, sim in enumerate(stats.get('top_similar', []), 1):
            print(f"   {i}. {sim['pair'][0]} ‚Üî {sim['pair'][1]}")
            print(f"      Jaccard: {sim['jaccard']:.3f}, Cosine: {sim['cosine']:.3f}\n")

    # ========== –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò V2.0 ==========

    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    similarity_analyzer = None
    if args.compare_metrics or args.html:
        if isinstance(detector, AdvancedDuplicateDetector):
            similarity_analyzer = SimilarityAnalyzer(detector)
            similarity_analyzer.compare_all_metrics()

            if args.export_metrics or args.all:
                report = similarity_analyzer.generate_metrics_report()
                metrics_file = root_dir / 'METRICS_COMPARISON.md'
                with open(metrics_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"‚úÖ –û—Ç—á—ë—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {metrics_file}\n")

    # 2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    cluster_analyzer = None
    if args.cluster or args.html:
        if isinstance(detector, AdvancedDuplicateDetector):
            cluster_analyzer = ClusterAnalyzer(detector)
            cluster_analyzer.simple_clustering(similarity_threshold=args.cluster_threshold)

            # –§–∏–ª—å—Ç—Ä –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
            cluster_analyzer.clusters = [
                c for c in cluster_analyzer.clusters
                if len(c['members']) >= args.min_cluster_size
            ]

            report = cluster_analyzer.generate_cluster_report()
            cluster_file = root_dir / 'CLUSTERS_REPORT.md'
            with open(cluster_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"‚úÖ –û—Ç—á—ë—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {cluster_file}\n")

    # 3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–∏—è–Ω–∏—é
    if args.recommend_merges or args.all:
        recommender = MergeRecommender(detector)
        recommender.analyze_duplicates()

        plan = recommender.generate_merge_plan()
        plan_file = root_dir / 'MERGE_PLAN.md'
        with open(plan_file, 'w', encoding='utf-8') as f:
            f.write(plan)
        print(f"‚úÖ –ü–ª–∞–Ω —Å–ª–∏—è–Ω–∏—è: {plan_file}\n")

        # CSV export
        if args.csv or args.all:
            recommender.export_to_csv()

    # 4. HTML Dashboard
    if args.html or args.all:
        visualizer = DuplicateVisualizer(detector, similarity_analyzer, cluster_analyzer)
        visualizer.generate_html_dashboard()

    # Generate standard reports
    detector.generate_report()
    detector.save_json()

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*60)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*60)
    print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {len(detector.articles)}")
    print(f"–¢–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(detector.duplicates.get('exact', []))}")
    print(f"–ü–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä: {len(detector.duplicates.get('near_duplicate', []))}")
    print(f"–ü–æ—Ö–æ–∂–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(detector.duplicates.get('similar_titles', []))}")

    if cluster_analyzer and cluster_analyzer.clusters:
        print(f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_analyzer.clusters)}")

    print("="*60 + "\n")


if __name__ == "__main__":
    main()
