#!/usr/bin/env python3
"""
Backlinks Generator - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é "–ö—Ç–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç—É —Å—Ç–∞—Ç—å—é"

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Wikipedia backlinks, Roam Research bidirectional links
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import json
import argparse
from typing import Dict, List, Tuple, Set
import math


class BacklinkAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–µ—Ç—Ä–∏–∫ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""

    def __init__(self, backlinks: Dict, articles: Dict):
        self.backlinks = backlinks
        self.articles = articles

    def calculate_citation_strength(self, article_path: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å–∏–ª—É —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –∫–∞—á–µ—Å—Ç–≤–æ)"""
        if article_path not in self.backlinks:
            return 0.0

        backlinks = self.backlinks[article_path]
        if not backlinks:
            return 0.0

        # –ë–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        count_score = len(backlinks)

        # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏)
        sources = set()
        for bl in backlinks:
            source_dir = Path(bl['source']).parent
            sources.add(str(source_dir))

        diversity_bonus = len(sources) / len(backlinks) if backlinks else 0

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        return count_score * (1 + diversity_bonus)

    def get_backlink_distribution(self) -> Dict[int, int]:
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: —Å–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ç–µ–π –∏–º–µ—é—Ç N –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        distribution = Counter()

        # –í—Å–µ —Å—Ç–∞—Ç—å–∏, –≤–∫–ª—é—á–∞—è —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö 0 backlinks
        for article_path in self.articles:
            count = len(self.backlinks.get(article_path, []))
            distribution[count] += 1

        return dict(sorted(distribution.items()))

    def find_citation_hubs(self, min_backlinks: int = 5) -> List[Tuple[str, int]]:
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏-—Ö–∞–±—ã (–Ω–∞ –Ω–∏—Ö –º–Ω–æ–≥–æ —Å—Å—ã–ª–∞—é—Ç—Å—è)"""
        hubs = []

        for article_path, backlinks in self.backlinks.items():
            if len(backlinks) >= min_backlinks:
                hubs.append((article_path, len(backlinks)))

        return sorted(hubs, key=lambda x: -x[1])

    def calculate_citation_network_density(self) -> float:
        """–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: actual_links / max_possible_links"""
        n = len(self.articles)
        if n <= 1:
            return 0.0

        max_possible = n * (n - 1)  # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ
        actual_links = sum(len(links) for links in self.backlinks.values())

        return actual_links / max_possible if max_possible > 0 else 0.0

    def get_mutual_citations(self) -> List[Tuple[str, str]]:
        """–ù–∞–π—Ç–∏ –≤–∑–∞–∏–º–Ω—ã–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (A‚ÜíB –∏ B‚ÜíA)"""
        mutual = []

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å forward links –∏–∑ backlinks
        forward_links = defaultdict(set)
        for target, backlinks in self.backlinks.items():
            for bl in backlinks:
                forward_links[bl['source']].add(target)

        # –ù–∞–π—Ç–∏ –≤–∑–∞–∏–º–Ω—ã–µ
        checked = set()
        for article_a in forward_links:
            for article_b in forward_links[article_a]:
                if article_a in forward_links.get(article_b, set()):
                    pair = tuple(sorted([article_a, article_b]))
                    if pair not in checked:
                        mutual.append(pair)
                        checked.add(pair)

        return mutual


class BacklinkScorer:
    """–û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""

    def __init__(self, backlinks: Dict, articles: Dict):
        self.backlinks = backlinks
        self.articles = articles

    def score_backlink(self, backlink: Dict) -> float:
        """–û—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å—Å—ã–ª–∫–∏"""
        score = 1.0

        # –§–∞–∫—Ç–æ—Ä 1: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ (–¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞)
        context = backlink.get('context', '')
        if context:
            context_len = len(context)
            if context_len > 50:
                score *= 1.5  # –î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç = –≤–∞–∂–Ω–∞—è —Å—Å—ã–ª–∫–∞
            elif context_len > 20:
                score *= 1.2
            elif context_len < 10:
                score *= 0.8  # –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç = –º–µ–Ω–µ–µ –≤–∞–∂–Ω–∞—è

        # –§–∞–∫—Ç–æ—Ä 2: –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫?
        # (–µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –≤—Å—ë –ø–æ–¥—Ä—è–¥, –∫–∞–∂–¥–∞—è —Å—Å—ã–ª–∫–∞ –º–µ–Ω–µ–µ —Ü–µ–Ω–Ω–∞)
        source_path = backlink['source']
        source_outgoing = 0

        for _, backlinks_list in self.backlinks.items():
            source_outgoing += sum(1 for bl in backlinks_list if bl['source'] == source_path)

        if source_outgoing > 0:
            # Penalty –∑–∞ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
            penalty = 1 / math.sqrt(source_outgoing)
            score *= penalty

        return score

    def get_weighted_backlinks(self, article_path: str) -> List[Tuple[Dict, float]]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ —Å –≤–µ—Å–∞–º–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏)"""
        if article_path not in self.backlinks:
            return []

        weighted = []
        for backlink in self.backlinks[article_path]:
            score = self.score_backlink(backlink)
            weighted.append((backlink, score))

        return sorted(weighted, key=lambda x: -x[1])

    def calculate_authority_score(self, article_path: str) -> float:
        """Authority score: —Å—É–º–º–∞ –≤–µ—Å–æ–≤ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫"""
        weighted = self.get_weighted_backlinks(article_path)
        return sum(score for _, score in weighted)


class BrokenBacklinksDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —Å–ª–æ–º–∞–Ω–Ω—ã—Ö/–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""

    def __init__(self, root_dir: Path):
        self.root_dir = root_dir
        self.knowledge_dir = root_dir / "knowledge"

    def check_backlinks(self, backlinks: Dict, articles: Dict) -> Dict[str, List[Dict]]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –æ–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å"""
        issues = defaultdict(list)

        for target_path, backlinks_list in backlinks.items():
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –¶–µ–ª–µ–≤–æ–π —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç?
            if target_path not in articles:
                issues['missing_targets'].append({
                    'target': target_path,
                    'backlinks_count': len(backlinks_list)
                })
                continue

            target_file = articles[target_path]['file']
            if not target_file.exists():
                issues['missing_files'].append({
                    'target': target_path,
                    'file': str(target_file)
                })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç?
            for backlink in backlinks_list:
                source_path = backlink['source']
                if source_path not in articles:
                    issues['missing_sources'].append({
                        'source': source_path,
                        'target': target_path
                    })
                    continue

                source_file = articles[source_path]['file']
                if not source_file.exists():
                    issues['missing_source_files'].append({
                        'source': source_path,
                        'target': target_path,
                        'file': str(source_file)
                    })

        return dict(issues)

    def find_orphaned_articles(self, backlinks: Dict, articles: Dict) -> List[str]:
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏-—Å–∏—Ä–æ—Ç—ã (–Ω–µ—Ç –≤—Ö–æ–¥—è—â–∏—Ö –ò –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫)"""
        orphaned = []

        # –°—Ç–∞—Ç—å–∏ —Å –∏—Å—Ö–æ–¥—è—â–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
        has_outgoing = set()
        for _, backlinks_list in backlinks.items():
            for bl in backlinks_list:
                has_outgoing.add(bl['source'])

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é
        for article_path in articles:
            has_incoming = article_path in backlinks and len(backlinks[article_path]) > 0
            has_out = article_path in has_outgoing

            if not has_incoming and not has_out:
                orphaned.append(article_path)

        return sorted(orphaned)


class BacklinksGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ì—Ä–∞—Ñ —Å—Å—ã–ª–æ–∫
        self.backlinks = defaultdict(list)
        self.articles = {}

        # –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.analyzer = None
        self.scorer = None
        self.broken_detector = None

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return match.group(1), match.group(2)
        except:
            pass
        return None, None

    def build_backlinks_graph(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        print("üîó –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...\n")

        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter_str, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))

            # –ü–∞—Ä—Å–∏–Ω–≥ frontmatter –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if frontmatter_str:
                try:
                    frontmatter = yaml.safe_load(frontmatter_str)
                    title = frontmatter.get('title', md_file.stem)
                except:
                    title = md_file.stem
            else:
                title = md_file.stem

            self.articles[article_path] = {
                'title': title,
                'file': md_file
            }

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å—Å—ã–ª–æ–∫
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            _, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            source_path = str(md_file.relative_to(self.root_dir))
            source_title = self.articles[source_path]['title']

            # –ò–∑–≤–ª–µ—á—å –≤—Å–µ markdown —Å—Å—ã–ª–∫–∏
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)

            for link_text, link_url in links:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                if link_url.startswith('http'):
                    continue

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —è–∫–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                if link_url.startswith('#'):
                    continue

                try:
                    # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                    target = (md_file.parent / link_url.split('#')[0]).resolve()

                    if target.exists() and target.is_relative_to(self.root_dir):
                        target_path = str(target.relative_to(self.root_dir))

                        # –î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å—Å—ã–ª–∫—É
                        if target_path in self.articles:
                            self.backlinks[target_path].append({
                                'source': source_path,
                                'title': source_title,
                                'context': link_text
                            })
                except:
                    pass

        print(f"   –°—Ç–∞—Ç–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.articles)}")
        print(f"   –û–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {sum(len(links) for links in self.backlinks.values())}\n")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.analyzer = BacklinkAnalyzer(self.backlinks, self.articles)
        self.scorer = BacklinkScorer(self.backlinks, self.articles)
        self.broken_detector = BrokenBacklinksDetector(self.root_dir)

    def generate_backlinks_section(self, article_path):
        """–°–æ–∑–¥–∞—Ç—å —Å–µ–∫—Ü–∏—é –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        if article_path not in self.backlinks:
            return ""

        backlinks = self.backlinks[article_path]

        if not backlinks:
            return ""

        lines = []
        lines.append("\n---\n\n")
        lines.append("## üîó –û–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n\n")
        lines.append(f"> –≠—Ç—É —Å—Ç–∞—Ç—å—é —Ü–∏—Ç–∏—Ä—É—é—Ç {len(backlinks)} —Å—Ç–∞—Ç–µ–π:\n\n")

        for backlink in backlinks:
            # –í—ã—á–∏—Å–ª–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            article_file = self.articles[article_path]['file']
            source_file = self.articles[backlink['source']]['file']

            try:
                rel_path = source_file.relative_to(article_file.parent)
            except:
                rel_path = backlink['source']

            lines.append(f"- [{backlink['title']}]({rel_path})")

            if backlink['context']:
                lines.append(f" ‚Äî *\"{backlink['context']}\"*")

            lines.append("\n")

        return ''.join(lines)

    def update_article(self, article_path):
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—å—é —Å –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏"""
        article_file = self.articles[article_path]['file']

        frontmatter_str, content = self.extract_frontmatter_and_content(article_file)

        if not content:
            return False

        # –£–¥–∞–ª–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ–∫—Ü–∏—é –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        # –ò—â–µ–º "## üîó –û–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏" –¥–æ –∫–æ–Ω—Ü–∞ —Ñ–∞–π–ª–∞
        content = re.sub(
            r'\n---\s*\n+##\s*üîó\s*–û–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏.*',
            '',
            content,
            flags=re.DOTALL
        )

        # –¢–∞–∫–∂–µ —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        content = re.sub(
            r'\n---\s*\n+##\s*–û–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏.*',
            '',
            content,
            flags=re.DOTALL
        )

        # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
        backlinks_section = self.generate_backlinks_section(article_path)

        if backlinks_section:
            content += backlinks_section

        # –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª
        full_content = f"---\n{frontmatter_str}\n---\n\n{content}"

        # –ó–∞–ø–∏—Å–∞—Ç—å
        with open(article_file, 'w', encoding='utf-8') as f:
            f.write(full_content)

        return bool(backlinks_section)

    def update_all(self, dry_run=False):
        """–û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("‚úçÔ∏è  –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π...\n")

        updated = 0
        skipped = 0

        for article_path in self.articles:
            if article_path in self.backlinks and self.backlinks[article_path]:
                if not dry_run:
                    if self.update_article(article_path):
                        updated += 1
                        print(f"   ‚úÖ {article_path} ({len(self.backlinks[article_path])} –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫)")
                else:
                    print(f"   [DRY RUN] {article_path} ‚Äî –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(self.backlinks[article_path])} –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
                    updated += 1
            else:
                skipped += 1

        print(f"\n‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {updated}")
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫): {skipped}")

    def run_analysis(self):
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
        if not self.analyzer:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ (build_backlinks_graph)")
            return

        print("\nüìä –ê–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫\n")

        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        distribution = self.analyzer.get_backlink_distribution()
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫:")
        for count, articles in sorted(distribution.items())[:10]:
            print(f"   {count} —Å—Å—ã–ª–æ–∫: {articles} —Å—Ç–∞—Ç–µ–π")
        if len(distribution) > 10:
            print(f"   ... (–≤—Å–µ–≥–æ {len(distribution)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)")

        # 2. –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏
        density = self.analyzer.calculate_citation_network_density()
        print(f"\n–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {density:.4f}")
        print(f"   ({density*100:.2f}% –æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–≤—è–∑–µ–π)")

        # 3. –•–∞–±—ã
        hubs = self.analyzer.find_citation_hubs(min_backlinks=3)
        if hubs:
            print(f"\nüéØ –¢–æ–ø-5 —Å—Ç–∞—Ç–µ–π-—Ö–∞–±–æ–≤:")
            for article_path, count in hubs[:5]:
                title = self.articles[article_path]['title']
                strength = self.analyzer.calculate_citation_strength(article_path)
                print(f"   ‚Ä¢ {title}")
                print(f"     –°—Å—ã–ª–æ–∫: {count}, –°–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {strength:.2f}")

        # 4. –í–∑–∞–∏–º–Ω—ã–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        mutual = self.analyzer.get_mutual_citations()
        if mutual:
            print(f"\n‚ÜîÔ∏è  –í–∑–∞–∏–º–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {len(mutual)}")
            for a, b in mutual[:3]:
                title_a = self.articles[a]['title']
                title_b = self.articles[b]['title']
                print(f"   ‚Ä¢ {title_a} ‚Üî {title_b}")
            if len(mutual) > 3:
                print(f"   ... –∏ –µ—â—ë {len(mutual) - 3}")

    def check_broken_links(self):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
        if not self.broken_detector:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ (build_backlinks_graph)")
            return

        print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Å—Å—ã–ª–æ–∫\n")

        issues = self.broken_detector.check_backlinks(self.backlinks, self.articles)

        if not issues:
            print("‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
        else:
            for issue_type, items in issues.items():
                print(f"\n‚ö†Ô∏è  {issue_type}: {len(items)}")
                for item in items[:5]:
                    print(f"   ‚Ä¢ {item}")
                if len(items) > 5:
                    print(f"   ... –∏ –µ—â—ë {len(items) - 5}")

        # –°—Ç–∞—Ç—å–∏-—Å–∏—Ä–æ—Ç—ã
        orphaned = self.broken_detector.find_orphaned_articles(self.backlinks, self.articles)
        if orphaned:
            print(f"\nüèùÔ∏è  –°—Ç–∞—Ç—å–∏-—Å–∏—Ä–æ—Ç—ã (–Ω–µ—Ç —Å–≤—è–∑–µ–π): {len(orphaned)}")
            for article_path in orphaned[:10]:
                title = self.articles[article_path]['title']
                print(f"   ‚Ä¢ {title}")
            if len(orphaned) > 10:
                print(f"   ... –∏ –µ—â—ë {len(orphaned) - 10}")

    def export_json(self, output_file: str = "backlinks.json"):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ JSON"""
        data = {
            'metadata': {
                'total_articles': len(self.articles),
                'total_backlinks': sum(len(links) for links in self.backlinks.values()),
                'articles_with_backlinks': len([a for a in self.backlinks.values() if a])
            },
            'articles': {},
            'backlinks': {}
        }

        # –°—Ç–∞—Ç—å–∏
        for article_path, info in self.articles.items():
            data['articles'][article_path] = {
                'title': info['title'],
                'file': str(info['file'])
            }

        # –û–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        for article_path, backlinks in self.backlinks.items():
            data['backlinks'][article_path] = [
                {
                    'source': bl['source'],
                    'title': bl['title'],
                    'context': bl['context']
                }
                for bl in backlinks
            ]

        # –ú–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.analyzer:
            data['metrics'] = {
                'distribution': self.analyzer.get_backlink_distribution(),
                'network_density': self.analyzer.calculate_citation_network_density(),
                'hubs': [
                    {'article': path, 'backlinks': count}
                    for path, count in self.analyzer.find_citation_hubs(min_backlinks=1)[:20]
                ],
                'mutual_citations': [
                    {'article_a': a, 'article_b': b}
                    for a, b in self.analyzer.get_mutual_citations()
                ]
            }

        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç: {output_path}")

    def export_html(self, output_file: str = "backlinks.html"):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HTML"""
        html = []
        html.append("<!DOCTYPE html>")
        html.append("<html lang='ru'>")
        html.append("<head>")
        html.append("  <meta charset='UTF-8'>")
        html.append("  <meta name='viewport' content='width=device-width, initial-scale=1.0'>")
        html.append("  <title>–ì—Ä–∞—Ñ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫</title>")
        html.append("  <style>")
        html.append("    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 40px; background: #f5f5f5; }")
        html.append("    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }")
        html.append("    h1 { color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }")
        html.append("    h2 { color: #555; margin-top: 30px; }")
        html.append("    .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }")
        html.append("    .stat-card { background: #f9f9f9; padding: 20px; border-radius: 6px; border-left: 4px solid #4CAF50; }")
        html.append("    .stat-value { font-size: 32px; font-weight: bold; color: #4CAF50; }")
        html.append("    .stat-label { color: #777; font-size: 14px; }")
        html.append("    .article { margin: 15px 0; padding: 15px; background: #fafafa; border-radius: 6px; }")
        html.append("    .article-title { font-weight: bold; color: #333; margin-bottom: 5px; }")
        html.append("    .backlinks { margin-left: 20px; }")
        html.append("    .backlink-item { margin: 5px 0; color: #666; }")
        html.append("    .backlink-context { font-style: italic; color: #999; font-size: 14px; }")
        html.append("    .badge { display: inline-block; background: #4CAF50; color: white; padding: 2px 8px; border-radius: 12px; font-size: 12px; margin-left: 5px; }")
        html.append("  </style>")
        html.append("</head>")
        html.append("<body>")
        html.append("  <div class='container'>")
        html.append("    <h1>üîó –ì—Ä–∞—Ñ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫</h1>")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_backlinks = sum(len(links) for links in self.backlinks.values())
        articles_with_backlinks = len([a for a in self.backlinks.values() if a])

        html.append("    <div class='stats'>")
        html.append(f"      <div class='stat-card'><div class='stat-value'>{len(self.articles)}</div><div class='stat-label'>–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π</div></div>")
        html.append(f"      <div class='stat-card'><div class='stat-value'>{total_backlinks}</div><div class='stat-label'>–û–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫</div></div>")
        html.append(f"      <div class='stat-card'><div class='stat-value'>{articles_with_backlinks}</div><div class='stat-label'>–°—Ç–∞—Ç–µ–π —Å —Å—Å—ã–ª–∫–∞–º–∏</div></div>")

        if self.analyzer:
            density = self.analyzer.calculate_citation_network_density()
            html.append(f"      <div class='stat-card'><div class='stat-value'>{density*100:.1f}%</div><div class='stat-label'>–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏</div></div>")

        html.append("    </div>")

        # –°—Ç–∞—Ç—å–∏ —Å –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
        html.append("    <h2>–°—Ç–∞—Ç—å–∏ —Å –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏</h2>")

        sorted_articles = sorted(
            [(path, links) for path, links in self.backlinks.items() if links],
            key=lambda x: -len(x[1])
        )

        for article_path, backlinks in sorted_articles:
            title = self.articles[article_path]['title']
            count = len(backlinks)

            html.append(f"    <div class='article'>")
            html.append(f"      <div class='article-title'>{title} <span class='badge'>{count}</span></div>")
            html.append(f"      <div class='backlinks'>")

            for bl in backlinks[:10]:
                html.append(f"        <div class='backlink-item'>‚Üê {bl['title']}")
                if bl['context']:
                    html.append(f" <span class='backlink-context'>\"{bl['context']}\"</span>")
                html.append("</div>")

            if len(backlinks) > 10:
                html.append(f"        <div class='backlink-item'>... –∏ –µ—â—ë {len(backlinks) - 10}</div>")

            html.append("      </div>")
            html.append("    </div>")

        html.append("  </div>")
        html.append("</body>")
        html.append("</html>")

        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(html))

        print(f"\n‚úÖ HTML —ç–∫—Å–ø–æ—Ä—Ç: {output_path}")

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîó –û—Ç—á—ë—Ç: –û–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n\n")
        lines.append("> –ö–∞—Ä—Ç–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_backlinks = sum(len(links) for links in self.backlinks.values())
        articles_with_backlinks = len([a for a in self.backlinks.values() if a])

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π**: {len(self.articles)}\n")
        lines.append(f"- **–°—Ç–∞—Ç–µ–π —Å –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏**: {articles_with_backlinks}\n")
        lines.append(f"- **–í—Å–µ–≥–æ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫**: {total_backlinks}\n")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if self.analyzer:
            density = self.analyzer.calculate_citation_network_density()
            lines.append(f"- **–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**: {density:.4f} ({density*100:.2f}%)\n")

            mutual = self.analyzer.get_mutual_citations()
            if mutual:
                lines.append(f"- **–í–∑–∞–∏–º–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π**: {len(mutual)}\n")

            orphaned = self.broken_detector.find_orphaned_articles(self.backlinks, self.articles)
            if orphaned:
                lines.append(f"- **–°—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç** (–Ω–µ—Ç —Å–≤—è–∑–µ–π): {len(orphaned)}\n")

        lines.append("\n")

        # –¢–æ–ø —Ü–∏—Ç–∏—Ä—É–µ–º—ã—Ö
        lines.append("## –¢–æ–ø-10 —Å–∞–º—ã—Ö —Ü–∏—Ç–∏—Ä—É–µ–º—ã—Ö\n\n")

        sorted_articles = sorted(
            self.backlinks.items(),
            key=lambda x: -len(x[1])
        )

        for i, (article_path, backlinks) in enumerate(sorted_articles[:10], 1):
            if not backlinks:
                break

            title = self.articles[article_path]['title']

            lines.append(f"### {i}. {title}\n\n")
            lines.append(f"- **–§–∞–π–ª**: [{article_path}]({article_path})\n")
            lines.append(f"- **–û–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫**: {len(backlinks)}\n\n")

            lines.append("**–¶–∏—Ç–∏—Ä—É—é—Ç:**\n")
            for backlink in backlinks[:5]:
                lines.append(f"- [{backlink['title']}]({backlink['source']})")
                if backlink['context']:
                    lines.append(f" ‚Äî *\"{backlink['context']}\"*")
                lines.append("\n")

            if len(backlinks) > 5:
                lines.append(f"\n...–∏ –µ—â—ë {len(backlinks) - 5}\n")

            lines.append("\n")

        output_file = self.root_dir / "BACKLINKS_REPORT.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"\n‚úÖ –û—Ç—á—ë—Ç: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='üîó Backlinks Generator - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                          # –û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç
  %(prog)s --dry-run                # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
  %(prog)s --analyze                # –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
  %(prog)s --check-broken           # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
  %(prog)s --export-json            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ JSON
  %(prog)s --export-html            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HTML
  %(prog)s --all --json out.json    # –í—Å—ë: –æ–±–Ω–æ–≤–∏—Ç—å, –∞–Ω–∞–ª–∏–∑, —ç–∫—Å–ø–æ—Ä—Ç—ã
        """
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='–ù–µ –∏–∑–º–µ–Ω—è—Ç—å —Ñ–∞–π–ª—ã, —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ'
    )

    parser.add_argument(
        '--analyze',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫'
    )

    parser.add_argument(
        '--check-broken',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ª–æ–º–∞–Ω–Ω—ã–µ –∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏'
    )

    parser.add_argument(
        '--export-json',
        dest='json_file',
        metavar='FILE',
        nargs='?',
        const='backlinks.json',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ JSON (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: backlinks.json)'
    )

    parser.add_argument(
        '--export-html',
        dest='html_file',
        metavar='FILE',
        nargs='?',
        const='backlinks.html',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HTML (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: backlinks.html)'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å—ë: –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—å–∏, –∞–Ω–∞–ª–∏–∑, —ç–∫—Å–ø–æ—Ä—Ç—ã'
    )

    parser.add_argument(
        '--no-update',
        action='store_true',
        help='–ù–µ –æ–±–Ω–æ–≤–ª—è—Ç—å —Ñ–∞–π–ª—ã —Å—Ç–∞—Ç–µ–π'
    )

    parser.add_argument(
        '--no-report',
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å markdown –æ—Ç—á—ë—Ç'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    generator = BacklinksGenerator(root_dir)

    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ
    generator.build_backlinks_graph()

    # –†–µ–∂–∏–º --all
    if args.all:
        if not args.no_update:
            generator.update_all(dry_run=args.dry_run)
        generator.run_analysis()
        generator.check_broken_links()
        if not args.no_report:
            generator.generate_report()
        generator.export_json(args.json_file or 'backlinks.json')
        generator.export_html(args.html_file or 'backlinks.html')
        return

    # –ê–Ω–∞–ª–∏–∑
    if args.analyze:
        generator.run_analysis()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    if args.check_broken:
        generator.check_broken_links()

    # –≠–∫—Å–ø–æ—Ä—Ç—ã
    if args.json_file:
        generator.export_json(args.json_file)

    if args.html_file:
        generator.export_html(args.html_file)

    # –î–µ–π—Å—Ç–≤–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã —Ñ–ª–∞–≥–∏)
    if not any([args.analyze, args.check_broken, args.json_file, args.html_file]):
        if not args.no_update:
            generator.update_all(dry_run=args.dry_run)
        if not args.no_report:
            generator.generate_report()


if __name__ == "__main__":
    main()
