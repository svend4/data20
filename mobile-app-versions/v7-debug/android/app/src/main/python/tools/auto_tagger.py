#!/usr/bin/env python3
"""
Advanced Auto-Tagger - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤
–§—É–Ω–∫—Ü–∏–∏:
- TF-IDF scoring (–≤–∞–∂–Ω–æ—Å—Ç—å, –Ω–µ –ø—Ä–æ—Å—Ç–æ —á–∞—Å—Ç–æ—Ç–∞)
- N-–≥—Ä–∞–º–º—ã (–±–∏–≥—Ä–∞–º–º—ã, —Ç—Ä–∏–≥—Ä–∞–º–º—ã –∫–∞–∫ —Ç–µ–≥–∏-—Ñ—Ä–∞–∑—ã)
- Weighted analysis (–∑–∞–≥–æ–ª–æ–≤–∫–∏, –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–µ—Å—è—Ç –±–æ–ª—å—à–µ)
- Tag recommendations (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π)
- Confidence scores –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- Tag hierarchies –∏ clustering
- –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–≥–æ–≤ –≤ –±–∞–∑–µ (–ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å)
- Auto-apply mode (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ)
- Tag co-occurrence analysis
- Synonym detection
- Multi-language support

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: WordPress auto-tagging, ML text classification, Tag clustering
"""

from pathlib import Path
import yaml
import re
from collections import Counter, defaultdict
import json
import math


class AdvancedAutoTagger:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        self.stop_words = set([
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '–æ', '–æ—Ç', '–∏–∑', '—É', '–∑–∞', '—ç—Ç–æ', '–∫–∞–∫',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does'
        ])

        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–≥–æ–≤
        self.tag_stats = defaultdict(int)
        self.tag_cooccurrence = defaultdict(lambda: defaultdict(int))
        self.document_frequency = defaultdict(int)  # –î–ª—è IDF
        self.total_documents = 0

        # –ö—ç—à –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π
        self.article_vectors = {}

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return match.group(1), yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None, None

    def build_corpus_statistics(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º—É –∫–æ—Ä–ø—É—Å—É"""
        print("üìä –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è TF-IDF...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            _, frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            self.total_documents += 1

            # –°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–≥–æ–≤
            if frontmatter and 'tags' in frontmatter:
                tags = frontmatter['tags']
                for tag in tags:
                    self.tag_stats[tag.lower()] += 1

                # Co-occurrence
                for i, tag1 in enumerate(tags):
                    for tag2 in tags[i+1:]:
                        self.tag_cooccurrence[tag1.lower()][tag2.lower()] += 1
                        self.tag_cooccurrence[tag2.lower()][tag1.lower()] += 1

            # Document frequency –¥–ª—è IDF
            words = self.tokenize(content)
            unique_words = set(words)
            for word in unique_words:
                self.document_frequency[word] += 1

        print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.total_documents}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤: {len(self.tag_stats)}")
        print(f"   –°–ª–æ–≤–∞—Ä—å: {len(self.document_frequency)} —Å–ª–æ–≤\n")

    def tokenize(self, text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–∏—Ç—å markdown
        text = re.sub(r'[#*`\[\]()]', ' ', text)

        # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ)
        words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())

        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = [w for w in words if w not in self.stop_words]

        return words

    def extract_ngrams(self, text, n=2):
        """–ò–∑–≤–ª–µ—á—å n-–≥—Ä–∞–º–º—ã (–±–∏–≥—Ä–∞–º–º—ã, —Ç—Ä–∏–≥—Ä–∞–º–º—ã)"""
        words = self.tokenize(text)
        ngrams = []

        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams.append(ngram)

        return ngrams

    def calculate_tfidf(self, text):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è —Å–ª–æ–≤"""
        words = self.tokenize(text)
        word_count = len(words)

        # Term Frequency
        tf = Counter(words)

        # TF-IDF scores
        tfidf_scores = {}

        for word, count in tf.items():
            # TF (normalized)
            term_freq = count / word_count if word_count > 0 else 0

            # IDF
            doc_freq = self.document_frequency.get(word, 1)
            idf = math.log(self.total_documents / doc_freq) if doc_freq > 0 else 0

            # TF-IDF
            tfidf_scores[word] = term_freq * idf

        return tfidf_scores

    def extract_weighted_keywords(self, content):
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –≤–µ—Å–∞–º–∏"""
        # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ —á–∞—Å—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        parts = {
            'headers': [],
            'bold': [],
            'body': content
        }

        # –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ (# –∑–∞–≥–æ–ª–æ–≤–æ–∫)
        headers = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
        parts['headers'] = ' '.join(headers)

        # –ò–∑–≤–ª–µ—á—å –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (**bold**)
        bold_text = re.findall(r'\*\*([^*]+)\*\*', content)
        parts['bold'] = ' '.join(bold_text)

        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
        scores = defaultdict(float)

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ - –≤–µ—Å 3.0
        if parts['headers']:
            header_scores = self.calculate_tfidf(parts['headers'])
            for word, score in header_scores.items():
                scores[word] += score * 3.0

        # –í—ã–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç - –≤–µ—Å 2.0
        if parts['bold']:
            bold_scores = self.calculate_tfidf(parts['bold'])
            for word, score in bold_scores.items():
                scores[word] += score * 2.0

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç - –≤–µ—Å 1.0
        body_scores = self.calculate_tfidf(content)
        for word, score in body_scores.items():
            scores[word] += score * 1.0

        return scores

    def extract_ngram_candidates(self, content):
        """–ò–∑–≤–ª–µ—á—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤-–±–∏–≥—Ä–∞–º–º –∏ —Ç—Ä–∏–≥—Ä–∞–º–º"""
        bigrams = self.extract_ngrams(content, n=2)
        trigrams = self.extract_ngrams(content, n=3)

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É
        bigram_freq = Counter(bigrams)
        trigram_freq = Counter(trigrams)

        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å (–º–∏–Ω–∏–º—É–º 2 —É–ø–æ–º–∏–Ω–∞–Ω–∏—è)
        good_bigrams = {bg: count for bg, count in bigram_freq.items() if count >= 2}
        good_trigrams = {tg: count for tg, count in trigram_freq.items() if count >= 2}

        return good_bigrams, good_trigrams

    def calculate_confidence(self, word, score, word_freq, existing_tags):
        """–í—ã—á–∏—Å–ª–∏—Ç—å confidence score –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        confidence = 0.0

        # –ë–∞–∑–æ–≤—ã–π score –æ—Ç TF-IDF (0-1)
        confidence += min(score / 10, 1.0) * 0.4

        # –ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ —Ç–µ–≥ –≤ –∫–æ—Ä–ø—É—Å–µ
        if word in self.tag_stats:
            tag_popularity = min(self.tag_stats[word] / 10, 1.0)
            confidence += tag_popularity * 0.3

        # Co-occurrence —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ç–µ–≥–∞–º–∏
        if existing_tags:
            max_cooccurrence = 0
            for existing_tag in existing_tags:
                cooc = self.tag_cooccurrence[word].get(existing_tag.lower(), 0)
                max_cooccurrence = max(max_cooccurrence, cooc)

            cooc_score = min(max_cooccurrence / 5, 1.0)
            confidence += cooc_score * 0.3

        return min(confidence, 1.0)

    def find_similar_articles(self, article_path, top_k=3):
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        # –ü—Ä–æ—Å—Ç–∞—è similarity –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–ª–æ–≤
        target_file = self.root_dir / article_path
        _, _, target_content = self.extract_frontmatter_and_content(target_file)

        if not target_content:
            return []

        target_words = set(self.tokenize(target_content))

        similarities = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            other_path = str(md_file.relative_to(self.root_dir))
            if other_path == article_path:
                continue

            _, frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content or not frontmatter or 'tags' not in frontmatter:
                continue

            other_words = set(self.tokenize(content))

            # Jaccard similarity
            intersection = len(target_words & other_words)
            union = len(target_words | other_words)

            if union > 0:
                similarity = intersection / union

                if similarity > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    similarities.append({
                        'path': other_path,
                        'similarity': similarity,
                        'tags': frontmatter['tags']
                    })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ similarity
        similarities.sort(key=lambda x: -x['similarity'])

        return similarities[:top_k]

    def get_recommendations_from_similar(self, similar_articles, existing_tags):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π"""
        recommended = Counter()

        for article in similar_articles:
            for tag in article['tags']:
                tag_lower = tag.lower()

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
                if tag_lower not in [t.lower() for t in existing_tags]:
                    # –í–µ—Å = similarity
                    recommended[tag_lower] += article['similarity']

        return recommended

    def suggest_tags(self, file_path, num_suggestions=5):
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ç–µ–≥–∏ –¥–ª—è —Å—Ç–∞—Ç—å–∏ (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–µ—Ä—Å–∏—è)"""
        frontmatter_str, frontmatter, content = self.extract_frontmatter_and_content(file_path)

        if not content:
            return None

        article_path = str(file_path.relative_to(self.root_dir))
        title = frontmatter.get('title', file_path.stem) if frontmatter else file_path.stem
        existing_tags = frontmatter.get('tags', []) if frontmatter else []

        # 1. TF-IDF —Å –≤–µ—Å–∞–º–∏
        weighted_scores = self.extract_weighted_keywords(content)

        # 2. N-–≥—Ä–∞–º–º—ã
        bigrams, trigrams = self.extract_ngram_candidates(content)

        # 3. –ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏
        similar_articles = self.find_similar_articles(article_path)
        similar_recommendations = self.get_recommendations_from_similar(similar_articles, existing_tags)

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        candidates = []

        # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞ (TF-IDF)
        for word, score in weighted_scores.items():
            if word not in [t.lower() for t in existing_tags]:
                word_freq = Counter(self.tokenize(content))[word]
                confidence = self.calculate_confidence(word, score, word_freq, existing_tags)

                candidates.append({
                    'tag': word,
                    'score': score,
                    'confidence': confidence,
                    'type': 'word',
                    'source': 'tfidf'
                })

        # –ë–∏–≥—Ä–∞–º–º—ã
        for bigram, freq in bigrams.items():
            if bigram not in [t.lower() for t in existing_tags]:
                candidates.append({
                    'tag': bigram,
                    'score': freq * 2,  # –ë–∏–≥—Ä–∞–º–º—ã —Ü–µ–Ω–Ω–µ–µ
                    'confidence': min(freq / 10, 1.0),
                    'type': 'bigram',
                    'source': 'ngram'
                })

        # –¢—Ä–∏–≥—Ä–∞–º–º—ã
        for trigram, freq in trigrams.items():
            if trigram not in [t.lower() for t in existing_tags]:
                candidates.append({
                    'tag': trigram,
                    'score': freq * 3,  # –¢—Ä–∏–≥—Ä–∞–º–º—ã –µ—â–µ —Ü–µ–Ω–Ω–µ–µ
                    'confidence': min(freq / 10, 1.0),
                    'type': 'trigram',
                    'source': 'ngram'
                })

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –ø–æ—Ö–æ–∂–∏—Ö
        for tag, score in similar_recommendations.items():
            candidates.append({
                'tag': tag,
                'score': score * 10,
                'confidence': min(score, 1.0),
                'type': 'word',
                'source': 'similar'
            })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ score * confidence
        candidates.sort(key=lambda x: -(x['score'] * x['confidence']))

        # –¢–æ–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        top_suggestions = candidates[:num_suggestions]

        return {
            'path': article_path,
            'title': title,
            'existing_tags': existing_tags,
            'suggestions': top_suggestions,
            'similar_articles': similar_articles
        }

    def analyze_all(self):
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üè∑Ô∏è  –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤...\n")

        # –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.build_corpus_statistics()

        suggestions = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            result = self.suggest_tags(md_file)
            if result and result['suggestions']:
                suggestions.append(result)

        print(f"   –°—Ç–∞—Ç–µ–π —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏: {len(suggestions)}\n")

        return suggestions

    def generate_report(self, suggestions):
        """–°–æ–∑–¥–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üè∑Ô∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–µ–≥–æ–≤\n\n")
        lines.append("> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF, n-–≥—Ä–∞–º–º –∏ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π\n\n")

        for item in suggestions:
            lines.append(f"## {item['title']}\n\n")
            lines.append(f"`{item['path']}`\n\n")

            # –¢–µ–∫—É—â–∏–µ —Ç–µ–≥–∏
            if item['existing_tags']:
                lines.append(f"**–¢–µ–∫—É—â–∏–µ —Ç–µ–≥–∏**: {', '.join(item['existing_tags'])}\n\n")
            else:
                lines.append(f"**–¢–µ–∫—É—â–∏–µ —Ç–µ–≥–∏**: –Ω–µ—Ç\n\n")

            # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏
            lines.append("**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏**:\n\n")

            for i, sugg in enumerate(item['suggestions'], 1):
                confidence_pct = sugg['confidence'] * 100

                # –ò–∫–æ–Ω–∫–∞ —Ç–∏–ø–∞
                type_icon = {
                    'word': 'üî§',
                    'bigram': 'üîó',
                    'trigram': 'üîóüîó'
                }.get(sugg['type'], '‚Ä¢')

                # –ò—Å—Ç–æ—á–Ω–∏–∫
                source_label = {
                    'tfidf': 'TF-IDF',
                    'ngram': 'N-gram',
                    'similar': '–ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏'
                }.get(sugg['source'], 'Unknown')

                lines.append(f"{i}. {type_icon} **{sugg['tag']}** ")
                lines.append(f"(confidence: {confidence_pct:.0f}%, source: {source_label})\n")

            # –ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏
            if item.get('similar_articles'):
                lines.append("\n**–ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏** (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞):\n\n")

                for similar in item['similar_articles'][:3]:
                    sim_pct = similar['similarity'] * 100
                    lines.append(f"- {Path(similar['path']).stem} ({sim_pct:.0f}% –ø–æ—Ö–æ–∂–µ—Å—Ç–∏)\n")
                    lines.append(f"  –¢–µ–≥–∏: {', '.join(similar['tags'])}\n")

            lines.append("\n---\n\n")

        output_file = self.root_dir / "ADVANCED_TAGGING_SUGGESTIONS.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def export_json(self, suggestions):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON"""
        output_file = self.root_dir / "tagging_suggestions.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(suggestions, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON: {output_file}")

    def auto_apply_tags(self, article_path, tags_to_add):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥–∏ –≤ —Å—Ç–∞—Ç—å—é"""
        file_path = self.root_dir / article_path

        frontmatter_str, frontmatter, content = self.extract_frontmatter_and_content(file_path)

        if not frontmatter:
            frontmatter = {}

        # –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥–∏
        existing_tags = frontmatter.get('tags', [])
        new_tags = existing_tags + tags_to_add

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏
        new_tags = list(dict.fromkeys(new_tags))

        frontmatter['tags'] = new_tags

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
        new_frontmatter = yaml.dump(frontmatter, allow_unicode=True, default_flow_style=False)
        full_content = f"---\n{new_frontmatter}---\n\n{content}"

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(full_content)

        return True


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Auto-Tagger')
    parser.add_argument('-n', '--num-suggestions', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    parser.add_argument('--json', action='store_true',
                       help='–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    tagger = AdvancedAutoTagger(root_dir)
    suggestions = tagger.analyze_all()
    tagger.generate_report(suggestions)

    if args.json:
        tagger.export_json(suggestions)


if __name__ == "__main__":
    main()
