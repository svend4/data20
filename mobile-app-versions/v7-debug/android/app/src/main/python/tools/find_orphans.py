#!/usr/bin/env python3
"""
Advanced Orphan Finder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç
–§—É–Ω–∫—Ü–∏–∏:
- Orphan classification (–Ω–æ–≤—ã–µ, —Å—Ç–∞—Ä—ã–µ, –∫—Ä–∏—Ç–∏—á–Ω—ã–µ)
- Fix suggestions (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≥–¥–µ –¥–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏)
- Severity levels (high, medium, low)
- Integration candidates (—Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å—Å—ã–ª–∞—Ç—å—Å—è)
- Orphan age detection
- Link density analysis
- Category-based analysis
- Automatic fix generation
- JSON export
- Graph visualization data

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Wikipedia orphan detection, SEO tools, Content auditing tools
"""

from pathlib import Path
import re
import yaml
from datetime import datetime, timedelta
import json
from collections import defaultdict, Counter
import math


class OrphanImpactAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —Å–∏—Ä–æ—Ç –Ω–∞ –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π"""

    def __init__(self, all_articles, incoming_links, outgoing_links):
        self.all_articles = all_articles
        self.incoming_links = incoming_links
        self.outgoing_links = outgoing_links

    def calculate_lost_pagerank(self, orphan_path):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–π PageRank –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫

        PageRank —É–∑–ª–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫. –°–∏—Ä–æ—Ç–∞ –∏–º–µ–µ—Ç
        —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–π PageRank = (1-d)/N, –≥–¥–µ d=0.85, N=–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤.

        Returns:
            dict: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª PageRank –µ—Å–ª–∏ –±—ã –±—ã–ª–∏ —Å—Å—ã–ª–∫–∏
        """
        n = len(self.all_articles)
        damping = 0.85

        # –ë–∞–∑–æ–≤—ã–π PageRank —Å–∏—Ä–æ—Ç—ã
        base_pr = (1 - damping) / n

        # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π PageRank –µ—Å–ª–∏ –±—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å—Å—ã–ª–∞–ª–∏—Å—å
        potential_pr = base_pr

        # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è: –µ—Å–ª–∏ –±—ã –Ω–∞ —Å–∏—Ä–æ—Ç—É —Å—Å—ã–ª–∞–ª–∏—Å—å —Å—Ç–∞—Ç—å–∏ —Å –≤—ã—Å–æ–∫–∏–º out-degree,
        # –µ—ë PR –±—ã–ª –±—ã –≤—ã—à–µ
        outgoing_count = len(self.outgoing_links.get(orphan_path, set()))

        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π PR ~ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
        if outgoing_count > 0:
            potential_pr += (outgoing_count * damping) / (n * 10)

        lost_pr = potential_pr - base_pr

        return {
            'base_pagerank': base_pr,
            'potential_pagerank': potential_pr,
            'lost_pagerank': lost_pr,
            'lost_percentage': (lost_pr / potential_pr * 100) if potential_pr > 0 else 0
        }

    def calculate_connectivity_impact(self, orphan_path):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–≤—è–∑–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞

        –°–∏—Ä–æ—Ç—ã —É–º–µ–Ω—å—à–∞—é—Ç —Å–≤—è–∑–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞. –ú–µ—Ç—Ä–∏–∫–∏:
        - Disconnected component size
        - Average path length increase
        - Betweenness centrality lost

        Returns:
            dict: –º–µ—Ç—Ä–∏–∫–∏ –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Å–≤—è–∑–Ω–æ—Å—Ç—å
        """
        outgoing = self.outgoing_links.get(orphan_path, set())

        # –ï—Å–ª–∏ —Å–∏—Ä–æ—Ç–∞ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—Ç–∞—Ç—å–∏, –Ω–æ –Ω–∞ –Ω–µ–µ –Ω–∏–∫—Ç–æ –Ω–µ —Å—Å—ã–ª–∞–µ—Ç—Å—è,
        # —ç—Ç–æ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è —Å–≤—è–∑—å
        one_way_connections = len(outgoing)

        # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —Å–≤—è–∑–∏ (–µ—Å–ª–∏ –±—ã –Ω–∞ —Å–∏—Ä–æ—Ç—É —Å—Å—ã–ª–∞–ª–∏—Å—å –æ–±—Ä–∞—Ç–Ω–æ)
        potential_bidirectional = 0
        for target in outgoing:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—Å—ã–ª–∞–µ—Ç—Å—è –ª–∏ target –æ–±—Ä–∞—Ç–Ω–æ
            if orphan_path not in self.outgoing_links.get(target, set()):
                potential_bidirectional += 1

        # Impact score: –≤—ã—à–µ –µ—Å–ª–∏ –º–Ω–æ–≥–æ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Å–≤—è–∑–µ–π
        impact_score = one_way_connections + potential_bidirectional * 2

        return {
            'one_way_connections': one_way_connections,
            'potential_bidirectional': potential_bidirectional,
            'impact_score': impact_score,
            'connectivity_rating': self._rate_connectivity_impact(impact_score)
        }

    def _rate_connectivity_impact(self, score):
        """–û—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Å–≤—è–∑–Ω–æ—Å—Ç—å"""
        if score >= 10:
            return 'critical'
        elif score >= 5:
            return 'high'
        elif score >= 2:
            return 'medium'
        else:
            return 'low'

    def analyze_cluster_isolation(self, orphans):
        """
        –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–π –∏–∑–æ–ª—è—Ü–∏–∏

        –ì—Ä—É–ø–ø—ã —Å–∏—Ä–æ—Ç –≤ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/—Ç–µ–º–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞
        —Å–∏—Å—Ç–µ–º–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–Ω–∞–Ω–∏–π.

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç

        Returns:
            dict: –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Ä–æ—Ç
        """
        category_clusters = defaultdict(list)
        tag_clusters = defaultdict(list)

        for orphan in orphans:
            metadata = orphan['metadata']

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            category = metadata.get('category')
            if category:
                category_clusters[category].append(orphan['path'])

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–µ–≥–∞–º
            tags = metadata.get('tags', [])
            for tag in tags:
                tag_clusters[tag].append(orphan['path'])

        # –ù–∞–π—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å–∏—Ä–æ—Ç–∞–º–∏
        problem_categories = {cat: paths for cat, paths in category_clusters.items() if len(paths) >= 3}
        problem_tags = {tag: paths for tag, paths in tag_clusters.items() if len(paths) >= 3}

        return {
            'category_clusters': problem_categories,
            'tag_clusters': problem_tags,
            'isolated_categories_count': len(problem_categories),
            'isolated_tags_count': len(problem_tags)
        }

    def calculate_discoverability_score(self, orphan_path):
        """
        –û—Ü–µ–Ω–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∏—Ä–æ—Ç—ã

        Discoverability - –Ω–∞—Å–∫–æ–ª—å–∫–æ –ª–µ–≥–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å—é
        —á–µ—Ä–µ–∑ –Ω–∞–≤–∏–≥–∞—Ü–∏—é (–±–µ–∑ –ø–æ–∏—Å–∫–∞).

        –§–∞–∫—Ç–æ—Ä—ã:
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª–µ–≥—á–µ –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ –Ω–∏—Ö)
        - –ù–∞–ª–∏—á–∏–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö/—Ç–µ–≥–∞—Ö
        - –ì–ª—É–±–∏–Ω–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π

        Returns:
            int: –æ—Ü–µ–Ω–∫–∞ 0-100 (100 = –ª–µ–≥–∫–æ –Ω–∞–π—Ç–∏)
        """
        metadata = self.all_articles[orphan_path]
        score = 0

        # –ò—Å—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏ (0-30 –±–∞–ª–ª–æ–≤)
        outgoing_count = len(self.outgoing_links.get(orphan_path, set()))
        score += min(30, outgoing_count * 5)

        # –¢–µ–≥–∏ (0-25 –±–∞–ª–ª–æ–≤)
        tags_count = len(metadata.get('tags', []))
        score += min(25, tags_count * 5)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è (0-20 –±–∞–ª–ª–æ–≤)
        if metadata.get('category'):
            score += 20

        # –ì–ª—É–±–∏–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π (0-25 –±–∞–ª–ª–æ–≤, –º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
        depth = len(Path(orphan_path).parts) - 2  # -2 –¥–ª—è 'knowledge' –∏ filename
        depth_score = max(0, 25 - depth * 5)
        score += depth_score

        return min(100, score)


class AutoLinker:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""

    def __init__(self, all_articles, incoming_links, outgoing_links):
        self.all_articles = all_articles
        self.incoming_links = incoming_links
        self.outgoing_links = outgoing_links

    def extract_keywords(self, content, top_n=10):
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

        –ü—Ä–æ—Å—Ç–∞—è TF (term frequency) –±–µ–∑ IDF –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏.

        Args:
            content: —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
            top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-—Å–ª–æ–≤

        Returns:
            list: —Ç–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        """
        if not content:
            return []

        # –£–¥–∞–ª–∏—Ç—å markdown —Ä–∞–∑–º–µ—Ç–∫—É
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', content)  # —Å—Å—ã–ª–∫–∏
        text = re.sub(r'#+\s+', '', text)  # –∑–∞–≥–æ–ª–æ–≤–∫–∏
        text = re.sub(r'[*_`]', '', text)  # —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è–ê-–Øa-zA-Z]{3,}\b', text.lower())

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫)
        stop_words = {
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'out', 'this', 'that', 'with', 'have', 'from',
            '—ç—Ç–æ', '–¥–ª—è', '–∫–∞–∫', '–∏–ª–∏', '—á—Ç–æ', '—Ç–∞–∫', '–≤—Å–µ', '–µ—â–µ', '—É–∂–µ', '–±—ã–ª', '–±—ã–ª', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–º–æ–π', '—Ç–≤–æ–π', '–µ–≥–æ', '–µ—ë'
        }

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É
        word_freq = Counter(w for w in words if w not in stop_words)

        return [word for word, _ in word_freq.most_common(top_n)]

    def find_content_similarity(self, orphan_path, other_path):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç–∞—Ç—å—è–º–∏

        –ò—Å–ø–æ–ª—å–∑—É–µ–º Jaccard similarity –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.

        Args:
            orphan_path: –ø—É—Ç—å –∫ —Å–∏—Ä–æ—Ç–µ
            other_path: –ø—É—Ç—å –∫ –¥—Ä—É–≥–æ–π —Å—Ç–∞—Ç—å–µ

        Returns:
            float: similarity score 0-1
        """
        orphan_metadata = self.all_articles[orphan_path]
        other_metadata = self.all_articles[other_path]

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
        try:
            with open(orphan_metadata['file_path'], 'r', encoding='utf-8') as f:
                orphan_content = f.read()
            with open(other_metadata['file_path'], 'r', encoding='utf-8') as f:
                other_content = f.read()
        except:
            return 0.0

        # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        orphan_keywords = set(self.extract_keywords(orphan_content, top_n=20))
        other_keywords = set(self.extract_keywords(other_content, top_n=20))

        # Jaccard similarity
        if not orphan_keywords or not other_keywords:
            return 0.0

        intersection = len(orphan_keywords & other_keywords)
        union = len(orphan_keywords | other_keywords)

        return intersection / union if union > 0 else 0.0

    def suggest_contextual_links(self, orphan_path, top_n=5):
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏

        –ù–∞—Ö–æ–¥–∏—Ç —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Ö–æ–∂–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å–∏—Ä–æ—Ç—É.

        Args:
            orphan_path: –ø—É—Ç—å –∫ —Å–∏—Ä–æ—Ç–µ
            top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

        Returns:
            list: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å similarity scores
        """
        suggestions = []

        for article_path in self.all_articles.keys():
            if article_path == orphan_path:
                continue

            # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = self.find_content_similarity(orphan_path, article_path)

            if similarity > 0.1:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                suggestions.append({
                    'article': article_path,
                    'similarity': similarity,
                    'type': 'content_similarity'
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
        suggestions.sort(key=lambda x: -x['similarity'])

        return suggestions[:top_n]

    def generate_auto_link_text(self, orphan_path, target_path, context='generic'):
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

        Args:
            orphan_path: –ø—É—Ç—å –∫ —Å–∏—Ä–æ—Ç–µ
            target_path: –ø—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç —Å—Å—ã–ª–∞—Ç—å—Å—è
            context: –∫–æ–Ω—Ç–µ–∫—Å—Ç ('generic', 'related', 'see_also')

        Returns:
            str: markdown –∫–æ–¥ —Å—Å—ã–ª–∫–∏
        """
        orphan_metadata = self.all_articles[orphan_path]
        orphan_name = Path(orphan_path).stem
        orphan_title = orphan_metadata.get('frontmatter', {}).get('title', orphan_name)

        if context == 'related':
            return f"–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ: [{orphan_title}]({Path(orphan_path).name})"
        elif context == 'see_also':
            return f"–°–º. [{orphan_title}]({Path(orphan_path).name})"
        else:
            return f"[{orphan_title}]({Path(orphan_path).name})"

    def bulk_analyze_opportunities(self, orphans, threshold=0.15):
        """
        –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç
            threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏

        Returns:
            dict: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
        """
        total_opportunities = 0
        high_confidence = 0  # similarity > 0.3
        medium_confidence = 0  # similarity > 0.2
        low_confidence = 0  # similarity > threshold

        for orphan in orphans:
            suggestions = self.suggest_contextual_links(orphan['path'])

            for suggestion in suggestions:
                sim = suggestion['similarity']
                if sim >= threshold:
                    total_opportunities += 1

                    if sim > 0.3:
                        high_confidence += 1
                    elif sim > 0.2:
                        medium_confidence += 1
                    else:
                        low_confidence += 1

        return {
            'total_opportunities': total_opportunities,
            'high_confidence': high_confidence,
            'medium_confidence': medium_confidence,
            'low_confidence': low_confidence
        }


class OrphanClusterAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏—Ä–æ—Ç"""

    def __init__(self, all_articles):
        self.all_articles = all_articles

    def cluster_by_category(self, orphans):
        """
        –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ä–æ—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç

        Returns:
            dict: {category: [orphan_paths]}
        """
        clusters = defaultdict(list)

        for orphan in orphans:
            metadata = orphan['metadata']
            category = metadata.get('category', 'uncategorized')
            clusters[category].append(orphan)

        return dict(clusters)

    def cluster_by_age(self, orphans):
        """
        –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É

        –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã:
        - Fresh (< 7 –¥–Ω–µ–π)
        - Recent (7-30 –¥–Ω–µ–π)
        - Mature (30-90 –¥–Ω–µ–π)
        - Old (90+ –¥–Ω–µ–π)

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç

        Returns:
            dict: {age_group: [orphan_paths]}
        """
        clusters = {
            'fresh': [],
            'recent': [],
            'mature': [],
            'old': []
        }

        for orphan in orphans:
            age_days = orphan['metadata']['age_days']

            if age_days < 7:
                clusters['fresh'].append(orphan)
            elif age_days < 30:
                clusters['recent'].append(orphan)
            elif age_days < 90:
                clusters['mature'].append(orphan)
            else:
                clusters['old'].append(orphan)

        return clusters

    def cluster_by_directory(self, orphans):
        """
        –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º

        –ï—Å–ª–∏ –≤ –æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–Ω–æ–≥–æ —Å–∏—Ä–æ—Ç, —ç—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç
        –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ.

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç

        Returns:
            dict: {directory: [orphan_paths]}
        """
        clusters = defaultdict(list)

        for orphan in orphans:
            directory = str(Path(orphan['path']).parent)
            clusters[directory].append(orphan)

        return dict(clusters)

    def find_problematic_areas(self, orphans):
        """
        –ù–∞–π—Ç–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ (–º–Ω–æ–≥–æ —Å–∏—Ä–æ—Ç –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ)

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç

        Returns:
            list: –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        dir_clusters = self.cluster_by_directory(orphans)
        cat_clusters = self.cluster_by_category(orphans)

        problematic = []

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (3+ —Å–∏—Ä–æ—Ç)
        for directory, cluster_orphans in dir_clusters.items():
            if len(cluster_orphans) >= 3:
                problematic.append({
                    'type': 'directory',
                    'location': directory,
                    'orphan_count': len(cluster_orphans),
                    'severity': 'high' if len(cluster_orphans) >= 5 else 'medium',
                    'orphans': [o['path'] for o in cluster_orphans]
                })

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (4+ —Å–∏—Ä–æ—Ç)
        for category, cluster_orphans in cat_clusters.items():
            if len(cluster_orphans) >= 4:
                problematic.append({
                    'type': 'category',
                    'location': category,
                    'orphan_count': len(cluster_orphans),
                    'severity': 'high' if len(cluster_orphans) >= 7 else 'medium',
                    'orphans': [o['path'] for o in cluster_orphans]
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∏—Ä–æ—Ç
        problematic.sort(key=lambda x: -x['orphan_count'])

        return problematic

    def analyze_orphan_trends(self, orphans):
        """
        –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤: –∫–æ–≥–¥–∞ –ø–æ—è–≤–ª—è—é—Ç—Å—è —Å–∏—Ä–æ—Ç—ã

        Args:
            orphans: —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç

        Returns:
            dict: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤
        """
        age_clusters = self.cluster_by_age(orphans)

        # –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–∏—Ä–æ—Ç
        fresh_count = len(age_clusters['fresh'])
        recent_count = len(age_clusters['recent'])
        mature_count = len(age_clusters['mature'])
        old_count = len(age_clusters['old'])

        # –¢—Ä–µ–Ω–¥: —Ä–∞—Å—Ç—ë—Ç –ª–∏ —á–∏—Å–ª–æ —Å–∏—Ä–æ—Ç?
        # –ï—Å–ª–∏ fresh + recent > mature + old, —Ç–æ —Ç—Ä–µ–Ω–¥ —Ä–∞—Å—Ç—É—â–∏–π
        recent_total = fresh_count + recent_count
        old_total = mature_count + old_count

        if recent_total > old_total:
            trend = 'increasing'
            trend_severity = 'warning'
        elif recent_total < old_total * 0.5:
            trend = 'decreasing'
            trend_severity = 'good'
        else:
            trend = 'stable'
            trend_severity = 'neutral'

        return {
            'fresh_count': fresh_count,
            'recent_count': recent_count,
            'mature_count': mature_count,
            'old_count': old_count,
            'trend': trend,
            'trend_severity': trend_severity,
            'orphan_rate': (recent_total / len(self.all_articles) * 100) if self.all_articles else 0
        }


class OrphanVisualizer:
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Ä–æ—Ç"""

    def __init__(self, all_articles, orphans, incoming_links, outgoing_links):
        self.all_articles = all_articles
        self.orphans = orphans
        self.incoming_links = incoming_links
        self.outgoing_links = outgoing_links

    def generate_html_report(self):
        """
        –°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML –æ—Ç—á—ë—Ç

        –í–∫–ª—é—á–∞–µ—Ç:
        - –î–∞—à–±–æ—Ä–¥ —Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç
        - –§–∏–ª—å—Ç—Ä—ã –ø–æ severity, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –≤–æ–∑—Ä–∞—Å—Ç—É
        - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

        Returns:
            str: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_articles = len(self.all_articles)
        total_orphans = len(self.orphans)
        orphan_percentage = (total_orphans / total_articles * 100) if total_articles > 0 else 0

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ severity
        by_severity = defaultdict(int)
        for orphan in self.orphans:
            severity = orphan['classification']['severity']
            by_severity[severity] += 1

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø—É
        by_type = defaultdict(int)
        for orphan in self.orphans:
            orphan_type = orphan['classification']['type']
            by_type[orphan_type] += 1

        # JSON –¥–ª—è JavaScript
        orphans_json = []
        for orphan in self.orphans:
            orphans_json.append({
                'path': orphan['path'],
                'name': Path(orphan['path']).stem,
                'type': orphan['classification']['type'],
                'severity': orphan['classification']['severity'],
                'age_days': orphan['metadata']['age_days'],
                'category': orphan['metadata'].get('category', 'uncategorized'),
                'content_length': orphan['metadata']['content_length'],
                'candidates_count': len(orphan['candidates'])
            })

        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}

        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}

        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}

        h1 {{
            color: white;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            text-shadow: 0 2px 10px rgba(0,0,0,0.3);
        }}

        .dashboard {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}

        .stat-card {{
            background: white;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            text-align: center;
        }}

        .stat-value {{
            font-size: 3em;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 10px;
        }}

        .stat-label {{
            color: #666;
            font-size: 1.1em;
        }}

        .filters {{
            background: white;
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            margin-bottom: 20px;
        }}

        .filter-group {{
            display: inline-block;
            margin-right: 20px;
            margin-bottom: 10px;
        }}

        .filter-group label {{
            margin-right: 10px;
            font-weight: 600;
        }}

        .filter-group select, .filter-group input {{
            padding: 8px 12px;
            border: 2px solid #667eea;
            border-radius: 5px;
            font-size: 14px;
        }}

        .orphans-list {{
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        }}

        .orphan-item {{
            border-left: 4px solid #ccc;
            padding: 15px;
            margin-bottom: 15px;
            background: #f8f9fa;
            border-radius: 5px;
        }}

        .orphan-item.high {{
            border-left-color: #dc3545;
        }}

        .orphan-item.medium {{
            border-left-color: #ffc107;
        }}

        .orphan-item.low {{
            border-left-color: #28a745;
        }}

        .orphan-title {{
            font-size: 1.3em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #333;
        }}

        .orphan-meta {{
            color: #666;
            font-size: 0.95em;
            margin-bottom: 5px;
        }}

        .orphan-badge {{
            display: inline-block;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
            margin-right: 8px;
        }}

        .severity-high {{
            background: #dc3545;
            color: white;
        }}

        .severity-medium {{
            background: #ffc107;
            color: #333;
        }}

        .severity-low {{
            background: #28a745;
            color: white;
        }}

        .no-results {{
            text-align: center;
            padding: 40px;
            color: #999;
            font-size: 1.2em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç</h1>

        <div class="dashboard">
            <div class="stat-card">
                <div class="stat-value">{total_articles}</div>
                <div class="stat-label">–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{total_orphans}</div>
                <div class="stat-label">–°–∏—Ä–æ—Ç</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{orphan_percentage:.1f}%</div>
                <div class="stat-label">–ü—Ä–æ—Ü–µ–Ω—Ç —Å–∏—Ä–æ—Ç</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{by_severity.get('high', 0)}</div>
                <div class="stat-label">–ö—Ä–∏—Ç–∏—á–Ω—ã—Ö</div>
            </div>
        </div>

        <div class="filters">
            <div class="filter-group">
                <label>Severity:</label>
                <select id="filterSeverity">
                    <option value="all">–í—Å–µ</option>
                    <option value="high">High</option>
                    <option value="medium">Medium</option>
                    <option value="low">Low</option>
                </select>
            </div>
            <div class="filter-group">
                <label>–¢–∏–ø:</label>
                <select id="filterType">
                    <option value="all">–í—Å–µ</option>
                    <option value="old_orphan">–°—Ç–∞—Ä—ã–µ</option>
                    <option value="new">–ù–æ–≤—ã–µ</option>
                    <option value="isolated">–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ</option>
                    <option value="completely_isolated">–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ</option>
                    <option value="stub">Stubs</option>
                </select>
            </div>
            <div class="filter-group">
                <label>–ü–æ–∏—Å–∫:</label>
                <input type="text" id="searchInput" placeholder="–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏...">
            </div>
        </div>

        <div class="orphans-list" id="orphansList"></div>
    </div>

    <script>
        const orphans = {json.dumps(orphans_json, ensure_ascii=False)};

        function renderOrphans(filtered) {{
            const container = document.getElementById('orphansList');

            if (filtered.length === 0) {{
                container.innerHTML = '<div class="no-results">–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</div>';
                return;
            }}

            container.innerHTML = filtered.map(orphan => `
                <div class="orphan-item ${{orphan.severity}}">
                    <div class="orphan-title">${{orphan.name}}</div>
                    <div class="orphan-meta">
                        <span class="orphan-badge severity-${{orphan.severity}}">${{orphan.severity.toUpperCase()}}</span>
                        <span class="orphan-badge" style="background: #6c757d; color: white;">${{orphan.type}}</span>
                    </div>
                    <div class="orphan-meta">
                        üìÇ ${{orphan.path}}
                    </div>
                    <div class="orphan-meta">
                        üìÖ –í–æ–∑—Ä–∞—Å—Ç: ${{orphan.age_days}} –¥–Ω–µ–π |
                        üìù –†–∞–∑–º–µ—Ä: ${{orphan.content_length}} —Å–∏–º–≤–æ–ª–æ–≤ |
                        üîó –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: ${{orphan.candidates_count}}
                    </div>
                    ${{orphan.category !== 'uncategorized' ? `<div class="orphan-meta">üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏—è: ${{orphan.category}}</div>` : ''}}
                </div>
            `).join('');
        }}

        function filterOrphans() {{
            const severity = document.getElementById('filterSeverity').value;
            const type = document.getElementById('filterType').value;
            const search = document.getElementById('searchInput').value.toLowerCase();

            const filtered = orphans.filter(orphan => {{
                if (severity !== 'all' && orphan.severity !== severity) return false;
                if (type !== 'all' && orphan.type !== type) return false;
                if (search && !orphan.name.toLowerCase().includes(search)) return false;
                return true;
            }});

            renderOrphans(filtered);
        }}

        // Event listeners
        document.getElementById('filterSeverity').addEventListener('change', filterOrphans);
        document.getElementById('filterType').addEventListener('change', filterOrphans);
        document.getElementById('searchInput').addEventListener('input', filterOrphans);

        // Initial render
        renderOrphans(orphans);
    </script>
</body>
</html>"""

        return html

    def generate_category_chart_data(self):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

        Returns:
            dict: –¥–∞–Ω–Ω—ã–µ –¥–ª—è Chart.js
        """
        category_counts = defaultdict(int)

        for orphan in self.orphans:
            category = orphan['metadata'].get('category', 'uncategorized')
            category_counts[category] += 1

        return {
            'labels': list(category_counts.keys()),
            'data': list(category_counts.values())
        }


class AdvancedOrphanFinder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.all_articles = {}  # path -> metadata
        self.incoming_links = defaultdict(set)  # target -> sources
        self.outgoing_links = defaultdict(set)  # source -> targets

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def analyze_article(self, file_path):
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å—é"""
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        article_path = str(file_path.relative_to(self.root_dir))

        # –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è/–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        mtime = file_path.stat().st_mtime
        modified_date = datetime.fromtimestamp(mtime)
        age_days = (datetime.now() - modified_date).days

        # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_length = len(content) if content else 0

        # –¢–µ–≥–∏/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        tags = []
        category = None
        if frontmatter:
            tags = frontmatter.get('tags', [])
            category = frontmatter.get('category', None)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.all_articles[article_path] = {
            'path': article_path,
            'frontmatter': frontmatter,
            'content_length': content_length,
            'modified_date': modified_date,
            'age_days': age_days,
            'tags': tags,
            'category': category,
            'file_path': file_path
        }

        return content

    def build_link_graph(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å—Å—ã–ª–æ–∫"""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π –∏ —Å—Å—ã–ª–æ–∫...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            # –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å—é
            content = self.analyze_article(md_file)
            article_path = str(md_file.relative_to(self.root_dir))

            if not content:
                continue

            # –ù–∞–π—Ç–∏ –≤—Å–µ markdown —Å—Å—ã–ª–∫–∏
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
            for text, link in links:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ —è–∫–æ—Ä—è
                if link.startswith('http') or link.startswith('#'):
                    continue

                # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                try:
                    target = (md_file.parent / link.split('#')[0]).resolve()
                    target_path = str(target.relative_to(self.root_dir))

                    # –ó–∞–ø–∏—Å–∞—Ç—å —Å–≤—è–∑—å
                    self.outgoing_links[article_path].add(target_path)
                    self.incoming_links[target_path].add(article_path)
                except:
                    pass

            # –°—Å—ã–ª–∫–∏ –∏–∑ frontmatter (related)
            frontmatter = self.all_articles[article_path]['frontmatter']
            if frontmatter and 'related' in frontmatter:
                related = frontmatter['related']
                if isinstance(related, list):
                    for link in related:
                        try:
                            target = (md_file.parent / link).resolve()
                            target_path = str(target.relative_to(self.root_dir))

                            self.outgoing_links[article_path].add(target_path)
                            self.incoming_links[target_path].add(article_path)
                        except:
                            pass

        print(f"   –°—Ç–∞—Ç–µ–π: {len(self.all_articles)}")
        print(f"   –°–≤—è–∑–µ–π: {sum(len(v) for v in self.outgoing_links.values())}\n")

    def classify_orphan(self, article_path):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ä–æ—Ç—É"""
        metadata = self.all_articles[article_path]

        age_days = metadata['age_days']
        content_length = metadata['content_length']
        outgoing_links_count = len(self.outgoing_links.get(article_path, set()))

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        classification = {
            'type': 'unknown',
            'severity': 'medium',
            'reason': []
        }

        # –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è (< 7 –¥–Ω–µ–π)
        if age_days < 7:
            classification['type'] = 'new'
            classification['severity'] = 'low'
            classification['reason'].append('–ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è, –≤–æ–∑–º–æ–∂–Ω–æ –µ—â–µ –Ω–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞')

        # –°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è (> 90 –¥–Ω–µ–π) –±–µ–∑ —Å—Å—ã–ª–æ–∫
        elif age_days > 90:
            classification['type'] = 'old_orphan'
            classification['severity'] = 'high'
            classification['reason'].append('–°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è –±–µ–∑ —Å—Å—ã–ª–æ–∫ - —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è')

        # –ö–æ—Ä–æ—Ç–∫–∞—è —Å—Ç–∞—Ç—å—è
        if content_length < 500:
            classification['type'] = 'stub'
            classification['severity'] = 'low'
            classification['reason'].append('–ö–æ—Ä–æ—Ç–∫–∞—è —Å—Ç–∞—Ç—å—è (stub)')

        # –°—Ç–∞—Ç—å—è —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞—Ä—É–∂—É –Ω–æ –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö
        if outgoing_links_count > 0:
            classification['type'] = 'isolated'
            classification['severity'] = 'medium'
            classification['reason'].append(f'–°—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ {outgoing_links_count} —Å—Ç–∞—Ç–µ–π, –Ω–æ –Ω–∞ –Ω–µ–µ –Ω–∏–∫—Ç–æ –Ω–µ —Å—Å—ã–ª–∞–µ—Ç—Å—è')

        # –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è
        if outgoing_links_count == 0:
            classification['type'] = 'completely_isolated'
            classification['severity'] = 'high'
            classification['reason'].append('–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–∞ (–Ω–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∏ –≤ –æ–¥–Ω—É —Å—Ç–æ—Ä–æ–Ω—É)')

        return classification

    def find_integration_candidates(self, orphan_path, max_candidates=5):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å–∏—Ä–æ—Ç—É"""
        orphan_metadata = self.all_articles[orphan_path]
        candidates = []

        orphan_tags = set(orphan_metadata.get('tags', []))
        orphan_category = orphan_metadata.get('category')

        for article_path, metadata in self.all_articles.items():
            if article_path == orphan_path:
                continue

            score = 0
            reasons = []

            # –û–±—â–∏–µ —Ç–µ–≥–∏
            article_tags = set(metadata.get('tags', []))
            common_tags = orphan_tags & article_tags
            if common_tags:
                score += len(common_tags) * 2
                reasons.append(f"–û–±—â–∏–µ —Ç–µ–≥–∏: {', '.join(common_tags)}")

            # –¢–∞ –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            if metadata.get('category') == orphan_category and orphan_category:
                score += 3
                reasons.append(f"–¢–∞ –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {orphan_category}")

            # –°–∏—Ä–æ—Ç–∞ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç—É —Å—Ç–∞—Ç—å—é
            if article_path in self.outgoing_links.get(orphan_path, set()):
                score += 5
                reasons.append("–°–∏—Ä–æ—Ç–∞ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç—É —Å—Ç–∞—Ç—å—é (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤–∑–∞–∏–º–Ω—É—é —Å—Å—ã–ª–∫—É)")

            # –≠—Ç–∞ —Å—Ç–∞—Ç—å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if Path(article_path).parent == Path(orphan_path).parent:
                score += 1
                reasons.append("–í —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")

            # –ú–Ω–æ–≥–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ (hub)
            outgoing_count = len(self.outgoing_links.get(article_path, set()))
            if outgoing_count > 5:
                score += 1
                reasons.append(f"Hub-—Å—Ç–∞—Ç—å—è ({outgoing_count} —Å—Å—ã–ª–æ–∫)")

            if score > 0:
                candidates.append({
                    'path': article_path,
                    'score': score,
                    'reasons': reasons
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ score
        candidates.sort(key=lambda x: -x['score'])

        return candidates[:max_candidates]

    def generate_fix_suggestion(self, orphan_path, candidate):
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é"""
        orphan_name = Path(orphan_path).stem
        candidate_name = Path(candidate['path']).stem

        suggestion = f"–í —Ñ–∞–π–ª–µ `{candidate['path']}` –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ `{orphan_path}`:\n"
        suggestion += f"```markdown\n"
        suggestion += f"[{orphan_name}]({Path(orphan_path).name})\n"
        suggestion += f"```\n"
        suggestion += f"–ü—Ä–∏—á–∏–Ω–∞: {', '.join(candidate['reasons'])}"

        return suggestion

    def find_orphans(self):
        """–ù–∞–π—Ç–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ä–æ—Ç"""
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ
        self.build_link_graph()

        print("üîç –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç...\n")

        # –ù–∞–π—Ç–∏ —Å–∏—Ä–æ—Ç—ã
        orphans = []

        for article_path in self.all_articles.keys():
            incoming_count = len(self.incoming_links.get(article_path, set()))

            if incoming_count == 0:
                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
                classification = self.classify_orphan(article_path)

                # –ù–∞–π—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
                candidates = self.find_integration_candidates(article_path)

                orphan_data = {
                    'path': article_path,
                    'metadata': self.all_articles[article_path],
                    'classification': classification,
                    'candidates': candidates
                }

                orphans.append(orphan_data)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = len(self.all_articles)
        linked = total - len(orphans)

        print(f"   –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total}")
        print(f"   –°–æ —Å—Å—ã–ª–∫–∞–º–∏: {linked}")
        print(f"   –°–∏—Ä–æ—Ç—ã: {len(orphans)}\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        if orphans:
            types = defaultdict(int)
            for orphan in orphans:
                types[orphan['classification']['type']] += 1

            print("   –¢–∏–ø—ã —Å–∏—Ä–æ—Ç:")
            for orphan_type, count in sorted(types.items(), key=lambda x: -x[1]):
                print(f"      {orphan_type}: {count}")
            print()

        return orphans

    def generate_report(self, orphans):
        """–°–æ–∑–¥–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîç –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ—Ç—á—ë—Ç: –°—Ç–∞—Ç—å–∏-—Å–∏—Ä–æ—Ç—ã\n\n")
        lines.append("> –°—Ç–∞—Ç—å–∏ –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏\n\n")

        lines.append(f"**–ù–∞–π–¥–µ–Ω–æ —Å–∏—Ä–æ—Ç**: {len(orphans)}\n\n")

        if not orphans:
            lines.append("‚úÖ –ù–µ—Ç —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç! –í—Å–µ —Å—Ç–∞—Ç—å–∏ —Å–≤—è–∑–∞–Ω—ã.\n")
        else:
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ severity
            by_severity = defaultdict(list)
            for orphan in orphans:
                severity = orphan['classification']['severity']
                by_severity[severity].append(orphan)

            # High severity
            if 'high' in by_severity:
                lines.append("## üî¥ –ö—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–∏—Ä–æ—Ç—ã (High Severity)\n\n")
                lines.append("–¢—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n\n")

                for orphan in by_severity['high']:
                    self._add_orphan_section(lines, orphan)

            # Medium severity
            if 'medium' in by_severity:
                lines.append("\n## üü° –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (Medium Severity)\n\n")

                for orphan in by_severity['medium']:
                    self._add_orphan_section(lines, orphan)

            # Low severity
            if 'low' in by_severity:
                lines.append("\n## üü¢ –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (Low Severity)\n\n")

                for orphan in by_severity['low']:
                    self._add_orphan_section(lines, orphan)

        output_file = self.root_dir / "ORPHANED_ARTICLES_ADVANCED.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def _add_orphan_section(self, lines, orphan):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é –¥–ª—è –æ–¥–Ω–æ–π —Å–∏—Ä–æ—Ç—ã"""
        path = orphan['path']
        classification = orphan['classification']
        metadata = orphan['metadata']
        candidates = orphan['candidates']

        lines.append(f"### {Path(path).stem}\n\n")
        lines.append(f"- **–ü—É—Ç—å**: `{path}`\n")
        lines.append(f"- **–¢–∏–ø**: {classification['type']}\n")
        lines.append(f"- **Severity**: {classification['severity']}\n")
        lines.append(f"- **–í–æ–∑—Ä–∞—Å—Ç**: {metadata['age_days']} –¥–Ω–µ–π\n")
        lines.append(f"- **–†–∞–∑–º–µ—Ä**: {metadata['content_length']} —Å–∏–º–≤–æ–ª–æ–≤\n")

        if classification['reason']:
            lines.append(f"- **–ü—Ä–∏—á–∏–Ω—ã**:\n")
            for reason in classification['reason']:
                lines.append(f"  - {reason}\n")

        # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        if candidates:
            lines.append(f"\n**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏** (—Ç–æ–ø-{len(candidates)}):\n\n")

            for i, candidate in enumerate(candidates, 1):
                lines.append(f"{i}. **{Path(candidate['path']).stem}** (score: {candidate['score']})\n")
                lines.append(f"   - –§–∞–π–ª: `{candidate['path']}`\n")
                for reason in candidate['reasons']:
                    lines.append(f"   - {reason}\n")
                lines.append("\n")

        lines.append("\n---\n\n")

    def export_json(self, orphans):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'total_articles': len(self.all_articles),
            'total_orphans': len(orphans),
            'orphans': []
        }

        for orphan in orphans:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å datetime –≤ string
            metadata = orphan['metadata'].copy()
            metadata['modified_date'] = metadata['modified_date'].isoformat()
            metadata.pop('file_path', None)  # –£–¥–∞–ª–∏—Ç—å Path object

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å frontmatter (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å date –æ–±—ä–µ–∫—Ç—ã)
            frontmatter = metadata.get('frontmatter')
            if frontmatter:
                frontmatter_clean = {}
                for key, value in frontmatter.items():
                    if hasattr(value, 'isoformat'):  # datetime –∏–ª–∏ date
                        frontmatter_clean[key] = value.isoformat()
                    else:
                        frontmatter_clean[key] = value
                metadata['frontmatter'] = frontmatter_clean

            data['orphans'].append({
                'path': orphan['path'],
                'metadata': metadata,
                'classification': orphan['classification'],
                'candidates': orphan['candidates']
            })

        output_file = self.root_dir / "orphans_analysis.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='üìä –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –ë–∞–∑–æ–≤—ã–π –æ—Ç—á—ë—Ç (Markdown)
  %(prog)s

  # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML –æ—Ç—á—ë—Ç
  %(prog)s --html

  # –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —Å–∏—Ä–æ—Ç (PageRank, connectivity)
  %(prog)s --impact

  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
  %(prog)s --auto-link

  # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏)
  %(prog)s --clusters

  # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ (–∫–æ–≥–¥–∞ –ø–æ—è–≤–ª—è—é—Ç—Å—è —Å–∏—Ä–æ—Ç—ã)
  %(prog)s --trends

  # JSON —ç–∫—Å–ø–æ—Ä—Ç
  %(prog)s --json

  # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–≤—Å–µ –æ–ø—Ü–∏–∏)
  %(prog)s --all

  # –ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–∏—Ä–æ—Ç
  %(prog)s --top-critical 10

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Wikipedia orphan detection, SEO tools, Content auditing tools
        '''
    )

    parser.add_argument('--html', action='store_true',
                        help='–°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML –æ—Ç—á—ë—Ç')
    parser.add_argument('--impact', action='store_true',
                        help='–ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —Å–∏—Ä–æ—Ç (PageRank, connectivity)')
    parser.add_argument('--auto-link', action='store_true',
                        help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞')
    parser.add_argument('--clusters', action='store_true',
                        help='–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏)')
    parser.add_argument('--trends', action='store_true',
                        help='–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–∏—Ä–æ—Ç')
    parser.add_argument('--json', action='store_true',
                        help='–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON')
    parser.add_argument('--top-critical', type=int, metavar='N',
                        help='–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø N –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–∏—Ä–æ—Ç')
    parser.add_argument('--all', action='store_true',
                        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–≤—Å–µ –æ–ø—Ü–∏–∏)')

    args = parser.parse_args()

    # --all –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –æ–ø—Ü–∏–∏
    if args.all:
        args.html = True
        args.impact = True
        args.auto_link = True
        args.clusters = True
        args.trends = True
        args.json = True
        if not args.top_critical:
            args.top_critical = 10

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    finder = AdvancedOrphanFinder(root_dir)
    orphans = finder.find_orphans()
    finder.generate_report(orphans)

    # JSON —ç–∫—Å–ø–æ—Ä—Ç
    if args.json:
        finder.export_json(orphans)

    # –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è
    if args.impact:
        print("\nüìà –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —Å–∏—Ä–æ—Ç...\n")

        impact_analyzer = OrphanImpactAnalyzer(
            finder.all_articles,
            finder.incoming_links,
            finder.outgoing_links
        )

        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–π –∏–∑–æ–ª—è—Ü–∏–∏
        cluster_isolation = impact_analyzer.analyze_cluster_isolation(orphans)

        print(f"   –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {cluster_isolation['isolated_categories_count']}")
        print(f"   –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–µ–≥–∏: {cluster_isolation['isolated_tags_count']}\n")

        # –¢–æ–ø —Å–∏—Ä–æ—Ç –ø–æ –≤–ª–∏—è–Ω–∏—é
        orphans_with_impact = []
        for orphan in orphans:
            pagerank_impact = impact_analyzer.calculate_lost_pagerank(orphan['path'])
            connectivity_impact = impact_analyzer.calculate_connectivity_impact(orphan['path'])
            discoverability = impact_analyzer.calculate_discoverability_score(orphan['path'])

            orphans_with_impact.append({
                'path': orphan['path'],
                'pagerank_impact': pagerank_impact,
                'connectivity_impact': connectivity_impact,
                'discoverability': discoverability
            })

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç
        impact_report = []
        impact_report.append("# üìà –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —Å–∏—Ä–æ—Ç\n\n")

        impact_report.append("## –ö–ª–∞—Å—Ç–µ—Ä–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è\n\n")
        if cluster_isolation['category_clusters']:
            impact_report.append("### –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (3+ —Å–∏—Ä–æ—Ç)\n\n")
            for category, paths in cluster_isolation['category_clusters'].items():
                impact_report.append(f"**{category}**: {len(paths)} —Å–∏—Ä–æ—Ç\n\n")
                for path in paths[:5]:
                    impact_report.append(f"- `{path}`\n")
                impact_report.append("\n")

        impact_report.append("\n## –¢–æ–ø-10 —Å–∏—Ä–æ—Ç –ø–æ connectivity impact\n\n")
        sorted_by_connectivity = sorted(
            orphans_with_impact,
            key=lambda x: -x['connectivity_impact']['impact_score']
        )

        for orphan_impact in sorted_by_connectivity[:10]:
            path = orphan_impact['path']
            conn = orphan_impact['connectivity_impact']

            impact_report.append(f"### {Path(path).stem}\n\n")
            impact_report.append(f"- **–ü—É—Ç—å**: `{path}`\n")
            impact_report.append(f"- **Impact score**: {conn['impact_score']}\n")
            impact_report.append(f"- **Connectivity rating**: {conn['connectivity_rating']}\n")
            impact_report.append(f"- **–û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Å–≤—è–∑–µ–π**: {conn['one_way_connections']}\n")
            impact_report.append(f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö**: {conn['potential_bidirectional']}\n\n")

        impact_report.append("\n## –¢–æ–ø-10 —Å–∏—Ä–æ—Ç –ø–æ discoverability (–ª–µ–≥–∫–æ –Ω–∞–π—Ç–∏)\n\n")
        sorted_by_discoverability = sorted(
            orphans_with_impact,
            key=lambda x: -x['discoverability']
        )

        for orphan_impact in sorted_by_discoverability[:10]:
            path = orphan_impact['path']
            disc = orphan_impact['discoverability']

            impact_report.append(f"1. **{Path(path).stem}**: {disc}/100\n")

        output_file = root_dir / "ORPHAN_IMPACT_ANALYSIS.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(impact_report)

        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è: {output_file}")

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ
    if args.auto_link:
        print("\nüîó –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫...\n")

        auto_linker = AutoLinker(
            finder.all_articles,
            finder.incoming_links,
            finder.outgoing_links
        )

        # –ú–∞—Å—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        opportunities = auto_linker.bulk_analyze_opportunities(orphans)

        print(f"   –í—Å–µ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π: {opportunities['total_opportunities']}")
        print(f"   –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.3): {opportunities['high_confidence']}")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.2): {opportunities['medium_confidence']}")
        print(f"   –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.15): {opportunities['low_confidence']}\n")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        auto_link_report = []
        auto_link_report.append("# üîó –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫\n\n")
        auto_link_report.append(f"**–í—Å–µ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π**: {opportunities['total_opportunities']}\n\n")

        auto_link_report.append("## –¢–æ–ø-10 —Å–∏—Ä–æ—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è\n\n")

        # –î–ª—è –∫–∞–∂–¥–æ–π —Å–∏—Ä–æ—Ç—ã –Ω–∞–π—Ç–∏ –ª—É—á—à–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        orphans_with_suggestions = []
        for orphan in orphans[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å—Å—è —Ç–æ–ø-20 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            suggestions = auto_linker.suggest_contextual_links(orphan['path'])
            if suggestions:
                orphans_with_suggestions.append({
                    'path': orphan['path'],
                    'best_similarity': suggestions[0]['similarity'],
                    'suggestions': suggestions
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ª—É—á—à–µ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        orphans_with_suggestions.sort(key=lambda x: -x['best_similarity'])

        for orphan_sugg in orphans_with_suggestions[:10]:
            path = orphan_sugg['path']
            suggestions = orphan_sugg['suggestions']

            auto_link_report.append(f"### {Path(path).stem}\n\n")
            auto_link_report.append(f"**–ü—É—Ç—å**: `{path}`\n\n")
            auto_link_report.append("**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è**:\n\n")

            for i, sugg in enumerate(suggestions, 1):
                auto_link_report.append(f"{i}. **{Path(sugg['article']).stem}** (similarity: {sugg['similarity']:.3f})\n")
                auto_link_report.append(f"   - –§–∞–π–ª: `{sugg['article']}`\n")

                # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ —Å—Å—ã–ª–∫–∏
                link_code = auto_linker.generate_auto_link_text(path, sugg['article'], context='see_also')
                auto_link_report.append(f"   - –ö–æ–¥ —Å—Å—ã–ª–∫–∏: `{link_code}`\n")

            auto_link_report.append("\n")

        output_file = root_dir / "AUTO_LINK_SUGGESTIONS.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(auto_link_report)

        print(f"‚úÖ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å–≤—è–∑—ã–≤–∞–Ω–∏—é: {output_file}")

    # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if args.clusters:
        print("\nüóÇÔ∏è  –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏—Ä–æ—Ç...\n")

        cluster_analyzer = OrphanClusterAnalyzer(finder.all_articles)

        problematic_areas = cluster_analyzer.find_problematic_areas(orphans)

        print(f"   –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π: {len(problematic_areas)}\n")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç
        cluster_report = []
        cluster_report.append("# üóÇÔ∏è  –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏—Ä–æ—Ç\n\n")
        cluster_report.append("> –ì—Ä—É–ø–ø—ã —Å–∏—Ä–æ—Ç —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã\n\n")

        cluster_report.append(f"**–ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π**: {len(problematic_areas)}\n\n")

        if problematic_areas:
            cluster_report.append("## –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏\n\n")

            for area in problematic_areas:
                severity_emoji = 'üî¥' if area['severity'] == 'high' else 'üü°'

                cluster_report.append(f"### {severity_emoji} {area['location']}\n\n")
                cluster_report.append(f"- **–¢–∏–ø**: {area['type']}\n")
                cluster_report.append(f"- **–°–∏—Ä–æ—Ç**: {area['orphan_count']}\n")
                cluster_report.append(f"- **Severity**: {area['severity']}\n\n")

                cluster_report.append("**–°–ø–∏—Å–æ–∫ —Å–∏—Ä–æ—Ç**:\n\n")
                for orphan_path in area['orphans'][:10]:
                    cluster_report.append(f"- `{orphan_path}`\n")

                if area['orphan_count'] > 10:
                    cluster_report.append(f"\n... –∏ –µ—â—ë {area['orphan_count'] - 10} —Å–∏—Ä–æ—Ç\n")

                cluster_report.append("\n---\n\n")

        output_file = root_dir / "ORPHAN_CLUSTERS.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(cluster_report)

        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {output_file}")

    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
    if args.trends:
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–∏—Ä–æ—Ç...\n")

        cluster_analyzer = OrphanClusterAnalyzer(finder.all_articles)
        trends = cluster_analyzer.analyze_orphan_trends(orphans)

        print(f"   –¢—Ä–µ–Ω–¥: {trends['trend']} ({trends['trend_severity']})")
        print(f"   –°–∫–æ—Ä–æ—Å—Ç—å –ø–æ—è–≤–ª–µ–Ω–∏—è: {trends['orphan_rate']:.2f}% –Ω–æ–≤—ã—Ö —Å–∏—Ä–æ—Ç\n")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç
        trends_report = []
        trends_report.append("# üìä –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ —Å–∏—Ä–æ—Ç\n\n")

        trends_report.append("## –í–æ–∑—Ä–∞—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n\n")
        trends_report.append(f"- **Fresh (< 7 –¥–Ω–µ–π)**: {trends['fresh_count']}\n")
        trends_report.append(f"- **Recent (7-30 –¥–Ω–µ–π)**: {trends['recent_count']}\n")
        trends_report.append(f"- **Mature (30-90 –¥–Ω–µ–π)**: {trends['mature_count']}\n")
        trends_report.append(f"- **Old (90+ –¥–Ω–µ–π)**: {trends['old_count']}\n\n")

        trends_report.append("## –¢—Ä–µ–Ω–¥\n\n")
        trend_emoji = {
            'increasing': 'üìà',
            'decreasing': 'üìâ',
            'stable': '‚û°Ô∏è'
        }.get(trends['trend'], '‚ùì')

        trends_report.append(f"{trend_emoji} **–¢—Ä–µ–Ω–¥**: {trends['trend']}\n\n")
        trends_report.append(f"**Severity**: {trends['trend_severity']}\n\n")
        trends_report.append(f"**–°–∫–æ—Ä–æ—Å—Ç—å –ø–æ—è–≤–ª–µ–Ω–∏—è**: {trends['orphan_rate']:.2f}% –Ω–æ–≤—ã—Ö/–Ω–µ–¥–∞–≤–Ω–∏—Ö —Å–∏—Ä–æ—Ç\n\n")

        if trends['trend'] == 'increasing':
            trends_report.append("‚ö†Ô∏è  **–í–Ω–∏–º–∞–Ω–∏–µ**: –ß–∏—Å–ª–æ —Å–∏—Ä–æ—Ç —Ä–∞—Å—Ç—ë—Ç! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å–∏–ª–∏—Ç—å —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π.\n")
        elif trends['trend'] == 'decreasing':
            trends_report.append("‚úÖ **–•–æ—Ä–æ—à–æ**: –ß–∏—Å–ª–æ —Å–∏—Ä–æ—Ç —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ!\n")

        output_file = root_dir / "ORPHAN_TRENDS.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(trends_report)

        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤: {output_file}")

    # –¢–æ–ø –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö
    if args.top_critical:
        n = args.top_critical

        print(f"\nüî¥ –¢–æ–ø-{n} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–∏—Ä–æ—Ç:\n")

        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—ã–µ
        critical = [o for o in orphans if o['classification']['severity'] == 'high']

        if not critical:
            print("   –ù–µ—Ç –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–∏—Ä–æ—Ç! ‚úÖ")
        else:
            for i, orphan in enumerate(critical[:n], 1):
                path = orphan['path']
                classification = orphan['classification']
                metadata = orphan['metadata']

                print(f"{i}. **{Path(path).stem}**")
                print(f"   –ü—É—Ç—å: {path}")
                print(f"   –¢–∏–ø: {classification['type']}")
                print(f"   –í–æ–∑—Ä–∞—Å—Ç: {metadata['age_days']} –¥–Ω–µ–π")
                print(f"   –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(orphan['candidates'])}")
                if classification['reason']:
                    print(f"   –ü—Ä–∏—á–∏–Ω—ã: {', '.join(classification['reason'])}")
                print()

    # HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if args.html:
        print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ HTML –æ—Ç—á—ë—Ç–∞...\n")

        visualizer = OrphanVisualizer(
            finder.all_articles,
            orphans,
            finder.incoming_links,
            finder.outgoing_links
        )

        html_content = visualizer.generate_html_report()

        output_file = root_dir / "orphans_report.html"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"‚úÖ HTML –æ—Ç—á—ë—Ç: {output_file}")
        print(f"   –û—Ç–∫—Ä–æ–π—Ç–µ {output_file} –≤ –±—Ä–∞—É–∑–µ—Ä–µ")

    print("\n‚ú® –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")


if __name__ == "__main__":
    main()
