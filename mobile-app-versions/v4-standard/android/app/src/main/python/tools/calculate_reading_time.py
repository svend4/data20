#!/usr/bin/env python3
"""
Reading Time Calculator - –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è
–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è —á—Ç–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏

–ù–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:
- –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è: 200-250 —Å–ª–æ–≤/–º–∏–Ω—É—Ç–∞ (—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫)
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã: 150-200 —Å–ª–æ–≤/–º–∏–Ω—É—Ç–∞
- –ö–æ–¥: —Å—á–∏—Ç–∞–µ—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–µ–µ (–ø—Ä–∏–º–µ—Ä–Ω–æ 50 —Å—Ç—Ä–æ–∫/–º–∏–Ω—É—Ç–∞)
"""

from pathlib import Path
import yaml
import re
import math
import argparse
from collections import Counter, defaultdict
from typing import Dict, List, Tuple
import json


class ReadingSpeedAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∫–æ—Ä–æ—Å—Ç–∏ —á—Ç–µ–Ω–∏—è
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """

    def __init__(self, base_wpm=200):
        self.base_wpm = base_wpm

    def calculate_adjusted_wpm(self, content_type: str, complexity: float = 1.0) -> int:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è

        Args:
            content_type: —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (technical, narrative, reference, etc.)
            complexity: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (0.5 - –ø—Ä–æ—Å—Ç–æ–π, 2.0 - –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–π)
        """
        base_adjustments = {
            'technical': 0.65,      # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã - –º–µ–¥–ª–µ–Ω–Ω–µ–µ
            'programming': 0.60,     # –ö–æ–¥ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ - –µ—â–µ –º–µ–¥–ª–µ–Ω–Ω–µ–µ
            'academic': 0.70,        # –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            'narrative': 1.0,        # –û–±—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            'reference': 0.80,       # –°–ø—Ä–∞–≤–æ—á–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
            'tutorial': 0.75,        # –û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
        }

        adjustment = base_adjustments.get(content_type, 1.0)

        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        final_adjustment = adjustment / complexity

        return int(self.base_wpm * final_adjustment)

    def estimate_comprehension_time(self, words: int, wpm: int, comprehension_level: float = 0.8) -> float:
        """
        –û—Ü–µ–Ω–∏—Ç—å –≤—Ä–µ–º—è —Å —É—á–µ—Ç–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏—è

        Args:
            words: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            wpm: —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É
            comprehension_level: —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è (0.5 - –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–µ, 1.0 - –≥–ª—É–±–æ–∫–æ–µ)
        """
        base_time = words / wpm if wpm > 0 else 0

        # –ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
        comprehension_multiplier = 0.5 + (comprehension_level * 0.5)

        return base_time * comprehension_multiplier

    def calculate_reading_fatigue(self, total_minutes: float) -> Dict[str, any]:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ —É—Å—Ç–∞–ª–æ—Å—Ç–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏

        –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ—Å–ª–µ 20-30 –º–∏–Ω—É—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è
        —Å–∫–æ—Ä–æ—Å—Ç—å —Å–Ω–∏–∂–∞–µ—Ç—Å—è –Ω–∞ 10-15%
        """
        if total_minutes <= 20:
            fatigue_factor = 1.0
            breaks_needed = 0
        elif total_minutes <= 40:
            fatigue_factor = 1.1
            breaks_needed = 1
        elif total_minutes <= 60:
            fatigue_factor = 1.15
            breaks_needed = 2
        else:
            fatigue_factor = 1.2
            breaks_needed = math.ceil(total_minutes / 30)

        adjusted_time = total_minutes * fatigue_factor

        return {
            'original_time': total_minutes,
            'adjusted_time': adjusted_time,
            'fatigue_factor': fatigue_factor,
            'breaks_recommended': breaks_needed,
            'break_duration_minutes': 5 * breaks_needed
        }


class ComplexityScorer:
    """
    –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """

    def __init__(self):
        pass

    def calculate_sentence_complexity(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if not sentences:
            return {'avg_length': 0, 'max_length': 0, 'complexity_score': 0}

        # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        lengths = [len(s.split()) for s in sentences]

        avg_length = sum(lengths) / len(lengths) if lengths else 0
        max_length = max(lengths) if lengths else 0

        # –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: 5-15 —Å–ª–æ–≤
        # –°—Ä–µ–¥–Ω–∏–µ: 15-25 —Å–ª–æ–≤
        # –°–ª–æ–∂–Ω—ã–µ: 25+ —Å–ª–æ–≤
        if avg_length < 15:
            complexity = 1.0
        elif avg_length < 25:
            complexity = 1.5
        else:
            complexity = 2.0

        return {
            'avg_sentence_length': round(avg_length, 1),
            'max_sentence_length': max_length,
            'total_sentences': len(sentences),
            'complexity_score': complexity
        }

    def calculate_vocabulary_richness(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –±–æ–≥–∞—Ç—Å—Ç–≤–∞ —Å–ª–æ–≤–∞—Ä—è (Type-Token Ratio)"""
        # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())

        if not words:
            return {'ttr': 0, 'unique_words': 0, 'total_words': 0}

        unique_words = set(words)

        # Type-Token Ratio
        ttr = len(unique_words) / len(words) if words else 0

        return {
            'total_words': len(words),
            'unique_words': len(unique_words),
            'type_token_ratio': round(ttr, 3),
            'vocabulary_richness': 'high' if ttr > 0.6 else 'medium' if ttr > 0.4 else 'low'
        }

    def detect_technical_terms(self, text: str) -> Dict[str, any]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        patterns = {
            'code_mentions': len(re.findall(r'`[^`]+`', text)),
            'abbreviations': len(re.findall(r'\b[A-Z]{2,}\b', text)),
            'camelCase': len(re.findall(r'\b[a-z]+[A-Z][a-zA-Z]*\b', text)),
            'technical_words': len(re.findall(r'\b(API|HTTP|JSON|XML|SQL|HTML|CSS|JavaScript|Python|function|class|method|algorithm|database)\b', text, re.IGNORECASE)),
        }

        total_technical = sum(patterns.values())

        # –û—Ü–µ–Ω–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if total_technical > 50:
            technical_level = 'very_high'
        elif total_technical > 20:
            technical_level = 'high'
        elif total_technical > 10:
            technical_level = 'medium'
        else:
            technical_level = 'low'

        return {
            **patterns,
            'total_technical_indicators': total_technical,
            'technical_level': technical_level
        }

    def calculate_overall_complexity(self, text: str) -> Dict[str, any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        sentence_metrics = self.calculate_sentence_complexity(text)
        vocab_metrics = self.calculate_vocabulary_richness(text)
        technical_metrics = self.detect_technical_terms(text)

        # –í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (1.0 - –ø—Ä–æ—Å—Ç–æ–π, 3.0 - –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–π)
        complexity_factors = []

        # –§–∞–∫—Ç–æ—Ä –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        complexity_factors.append(sentence_metrics['complexity_score'])

        # –§–∞–∫—Ç–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏
        tech_level_scores = {'low': 1.0, 'medium': 1.5, 'high': 2.0, 'very_high': 2.5}
        complexity_factors.append(tech_level_scores[technical_metrics['technical_level']])

        # –§–∞–∫—Ç–æ—Ä –±–æ–≥–∞—Ç—Å—Ç–≤–∞ —Å–ª–æ–≤–∞—Ä—è (–≤—ã—Å–æ–∫–∏–π TTR = —Å–ª–æ–∂–Ω–µ–µ)
        ttr = vocab_metrics['type_token_ratio']
        if ttr > 0.6:
            complexity_factors.append(1.8)
        elif ttr > 0.4:
            complexity_factors.append(1.3)
        else:
            complexity_factors.append(1.0)

        overall_score = sum(complexity_factors) / len(complexity_factors)

        return {
            'sentence_metrics': sentence_metrics,
            'vocabulary_metrics': vocab_metrics,
            'technical_metrics': technical_metrics,
            'overall_complexity_score': round(overall_score, 2),
            'difficulty_level': self._get_difficulty_label(overall_score)
        }

    def _get_difficulty_label(self, score: float) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∫—É —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        if score >= 2.5:
            return 'very_difficult'
        elif score >= 2.0:
            return 'difficult'
        elif score >= 1.5:
            return 'moderate'
        elif score >= 1.0:
            return 'easy'
        else:
            return 'very_easy'


class ReadabilityMetrics:
    """
    –ú–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    –†–µ–∞–ª–∏–∑—É–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    """

    def __init__(self):
        pass

    def count_syllables_russian(self, word: str) -> int:
        """
        –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤ –≤ —Ä—É—Å—Å–∫–æ–º —Å–ª–æ–≤–µ
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –≥–ª–∞—Å–Ω—ã—Ö
        """
        vowels = '–∞–µ—ë–∏–æ—É—ã—ç—é—èaeiouy'
        word = word.lower()
        syllable_count = sum(1 for char in word if char in vowels)
        return max(1, syllable_count)

    def flesch_reading_ease_adapted(self, text: str) -> Dict[str, any]:
        """
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –§–ª–µ—à–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

        –§–æ—Ä–º—É–ª–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è):
        206.835 - 1.015 √ó (—Å–ª–æ–≤–∞/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) - 84.6 √ó (—Å–ª–æ–≥–∏/—Å–ª–æ–≤–∞)

        –†–µ–∑—É–ª—å—Ç–∞—Ç:
        90-100: –æ—á–µ–Ω—å –ª–µ–≥–∫–æ
        60-70: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç
        0-30: –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ
        """
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())

        if not sentences or not words:
            return {'score': 0, 'level': 'unknown'}

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≥–∏
        total_syllables = sum(self.count_syllables_russian(word) for word in words)

        # –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
        words_per_sentence = len(words) / len(sentences)
        syllables_per_word = total_syllables / len(words)

        # –§–æ—Ä–º—É–ª–∞ –§–ª–µ—à–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
        score = 206.835 - (1.015 * words_per_sentence) - (84.6 * syllables_per_word)

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω
        score = max(0, min(100, score))

        return {
            'flesch_score': round(score, 1),
            'level': self._flesch_level(score),
            'words_per_sentence': round(words_per_sentence, 1),
            'syllables_per_word': round(syllables_per_word, 2),
            'total_sentences': len(sentences),
            'total_words': len(words),
            'total_syllables': total_syllables
        }

    def _flesch_level(self, score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –ø–æ —à–∫–∞–ª–µ –§–ª–µ—à–∞"""
        if score >= 90:
            return 'very_easy'
        elif score >= 80:
            return 'easy'
        elif score >= 70:
            return 'fairly_easy'
        elif score >= 60:
            return 'standard'
        elif score >= 50:
            return 'fairly_difficult'
        elif score >= 30:
            return 'difficult'
        else:
            return 'very_difficult'

    def automated_readability_index(self, text: str) -> Dict[str, any]:
        """
        ARI (Automated Readability Index)

        –§–æ—Ä–º—É–ª–∞:
        4.71 √ó (—Å–∏–º–≤–æ–ª—ã/—Å–ª–æ–≤–∞) + 0.5 √ó (—Å–ª–æ–≤–∞/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) - 21.43

        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω—ã–π –∫–ª–∞—Å—Å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        """
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())
        chars = sum(len(word) for word in words)

        if not sentences or not words:
            return {'ari_score': 0, 'grade_level': 0}

        chars_per_word = chars / len(words)
        words_per_sentence = len(words) / len(sentences)

        ari = (4.71 * chars_per_word) + (0.5 * words_per_sentence) - 21.43

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω
        ari = max(1, min(14, ari))

        return {
            'ari_score': round(ari, 1),
            'grade_level': math.ceil(ari),
            'chars_per_word': round(chars_per_word, 1),
            'words_per_sentence': round(words_per_sentence, 1),
            'interpretation': self._ari_interpretation(ari)
        }

    def _ari_interpretation(self, ari: float) -> str:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ARI"""
        if ari <= 6:
            return 'elementary_school'
        elif ari <= 9:
            return 'middle_school'
        elif ari <= 12:
            return 'high_school'
        else:
            return 'college_level'


class ReadingTimeCalculator:
    """
    –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è
    """

    def __init__(self, root_dir=".", wpm=200):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.wpm = wpm  # words per minute (—Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        self.speed_analyzer = ReadingSpeedAnalyzer(wpm)
        self.complexity_scorer = ComplexityScorer()
        self.readability_metrics = ReadabilityMetrics()

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                fm = yaml.safe_load(match.group(1))
                body = match.group(2)
                return fm, body
        except:
            pass

        return None, None

    def count_words(self, text):
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
        # –£–¥–∞–ª–∏—Ç—å markdown —Ä–∞–∑–º–µ—Ç–∫—É
        # –£–¥–∞–ª–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏
        text = re.sub(r'^#{1,6}\s+.*$', '', text, flags=re.MULTILINE)
        # –£–¥–∞–ª–∏—Ç—å –∫–æ–¥
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
        text = re.sub(r'`[^`]+`', '', text)
        # –£–¥–∞–ª–∏—Ç—å —Å—Å—ã–ª–∫–∏
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
        # –£–¥–∞–ª–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', text)

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + –ª–∞—Ç–∏–Ω–∏—Ü–∞)
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())

        return len(words)

    def count_code_blocks(self, text):
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –±–ª–æ–∫–∏ –∫–æ–¥–∞"""
        code_blocks = re.findall(r'```.*?```', text, re.DOTALL)
        total_lines = 0

        for block in code_blocks:
            lines = block.split('\n')
            # –ú–∏–Ω—É—Å –ø–µ—Ä–≤–∞—è –∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ (```)
            total_lines += max(0, len(lines) - 2)

        return total_lines

    def calculate_reading_time(self, file_path):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –¥–ª—è —Ñ–∞–π–ª–∞

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        {
            'words': –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤,
            'code_lines': —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞,
            'reading_minutes': –º–∏–Ω—É—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞,
            'code_minutes': –º–∏–Ω—É—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è –∫–æ–¥–∞,
            'total_minutes': –æ–±—â–µ–µ –≤—Ä–µ–º—è,
            'formatted': '5 –º–∏–Ω —á—Ç–µ–Ω–∏—è'
        }
        """
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        if not content:
            return None

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–∞
        words = self.count_words(content)

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–¥
        code_lines = self.count_code_blocks(content)

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
        category = frontmatter.get('category', '') if frontmatter else ''

        # –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ - –º–µ–¥–ª–µ–Ω–Ω–µ–µ
        wpm = self.wpm
        if category in ['computers', 'programming']:
            wpm = int(self.wpm * 0.75)  # 25% –º–µ–¥–ª–µ–Ω–Ω–µ–µ

        # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        reading_minutes = words / wpm if wpm > 0 else 0

        # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –∫–æ–¥–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 50 —Å—Ç—Ä–æ–∫/–º–∏–Ω—É—Ç–∞)
        code_minutes = code_lines / 50 if code_lines > 0 else 0

        # –û–±—â–µ–µ –≤—Ä–µ–º—è
        total_minutes = reading_minutes + code_minutes

        # –û–∫—Ä—É–≥–ª–∏—Ç—å –¥–æ –±–ª–∏–∂–∞–π—à–µ–π –º–∏–Ω—É—Ç—ã
        total_minutes_rounded = max(1, math.ceil(total_minutes))

        return {
            'words': words,
            'code_lines': code_lines,
            'reading_minutes': round(reading_minutes, 2),
            'code_minutes': round(code_minutes, 2),
            'total_minutes': round(total_minutes, 2),
            'total_minutes_rounded': total_minutes_rounded,
            'formatted': self.format_time(total_minutes_rounded)
        }

    def format_time(self, minutes):
        """–û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
        if minutes < 1:
            return "< 1 –º–∏–Ω"
        elif minutes == 1:
            return "1 –º–∏–Ω"
        elif minutes < 60:
            return f"{int(minutes)} –º–∏–Ω"
        else:
            hours = int(minutes // 60)
            mins = int(minutes % 60)
            if mins == 0:
                return f"{hours} —á"
            return f"{hours} —á {mins} –º–∏–Ω"

    def add_reading_time_to_articles(self):
        """–î–æ–±–∞–≤–∏—Ç—å –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –∫–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º"""
        print("‚è±Ô∏è  –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è...\n")

        count = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            reading_time = self.calculate_reading_time(md_file)

            if not reading_time:
                continue

            # –û–±–Ω–æ–≤–∏—Ç—å frontmatter
            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not frontmatter:
                continue

            # –î–æ–±–∞–≤–∏—Ç—å –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è
            old_time = frontmatter.get('reading_time')

            frontmatter['reading_time'] = reading_time['formatted']
            frontmatter['reading_time_minutes'] = reading_time['total_minutes_rounded']
            frontmatter['word_count'] = reading_time['words']

            if reading_time['code_lines'] > 0:
                frontmatter['code_lines'] = reading_time['code_lines']

            # –ó–∞–ø–∏—Å–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ
            try:
                new_content = "---\n"
                new_content += yaml.dump(frontmatter, allow_unicode=True, sort_keys=False)
                new_content += "---\n\n"
                new_content += content

                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(new_content)

                if old_time != reading_time['formatted']:
                    count += 1
                    print(f"‚úÖ {md_file.relative_to(self.root_dir)} ‚Äî {reading_time['formatted']}")

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ {md_file}: {e}")

        print(f"\n‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {count}")

    def analyze_complexity(self, file_path):
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
        """
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        if not content:
            return None

        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è
        reading_time = self.calculate_reading_time(file_path)

        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity = self.complexity_scorer.calculate_overall_complexity(content)

        # –ú–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        flesch = self.readability_metrics.flesch_reading_ease_adapted(content)
        ari = self.readability_metrics.automated_readability_index(content)

        # –ê–Ω–∞–ª–∏–∑ —É—Å—Ç–∞–ª–æ—Å—Ç–∏
        fatigue = self.speed_analyzer.calculate_reading_fatigue(reading_time['total_minutes'])

        return {
            'file': str(file_path),
            'reading_time': reading_time,
            'complexity': complexity,
            'flesch_reading_ease': flesch,
            'automated_readability_index': ari,
            'fatigue_analysis': fatigue
        }

    def export_analysis_json(self, analysis_results: List[Dict], output_file: str):
        """–≠–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç: {output_file}")

    def export_analysis_html(self, analysis_results: List[Dict], output_file: str):
        """–≠–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤ HTML —Å –∫—Ä–∞—Å–∏–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º"""
        html = []
        html.append('<!DOCTYPE html>\n<html lang="ru">\n<head>\n')
        html.append('<meta charset="UTF-8">\n')
        html.append('<meta name="viewport" content="width=device-width, initial-scale=1.0">\n')
        html.append('<title>–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞</title>\n')
        html.append('<style>\n')
        html.append('body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; ')
        html.append('max-width: 1200px; margin: 0 auto; padding: 20px; background: #f5f5f5; }\n')
        html.append('h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }\n')
        html.append('h2 { color: #34495e; margin-top: 30px; }\n')
        html.append('.article-card { background: white; border-radius: 8px; padding: 20px; ')
        html.append('margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n')
        html.append('.article-title { font-size: 1.4em; font-weight: bold; color: #2c3e50; margin-bottom: 10px; }\n')
        html.append('.metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); ')
        html.append('gap: 15px; margin-top: 15px; }\n')
        html.append('.metric-box { background: #ecf0f1; padding: 15px; border-radius: 5px; }\n')
        html.append('.metric-label { font-size: 0.85em; color: #7f8c8d; text-transform: uppercase; ')
        html.append('letter-spacing: 0.5px; }\n')
        html.append('.metric-value { font-size: 1.5em; font-weight: bold; color: #2c3e50; margin-top: 5px; }\n')
        html.append('.difficulty-very_easy { background: #d4edda; color: #155724; }\n')
        html.append('.difficulty-easy { background: #d1ecf1; color: #0c5460; }\n')
        html.append('.difficulty-moderate { background: #fff3cd; color: #856404; }\n')
        html.append('.difficulty-difficult { background: #f8d7da; color: #721c24; }\n')
        html.append('.difficulty-very_difficult { background: #f5c6cb; color: #491217; }\n')
        html.append('.complexity-section { margin-top: 20px; padding: 15px; background: #f8f9fa; ')
        html.append('border-left: 4px solid #3498db; border-radius: 4px; }\n')
        html.append('.summary { background: #3498db; color: white; padding: 20px; border-radius: 8px; ')
        html.append('margin-bottom: 30px; }\n')
        html.append('.summary h2 { color: white; margin-top: 0; border: none; }\n')
        html.append('</style>\n</head>\n<body>\n')

        html.append('<h1>‚è±Ô∏è –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞</h1>\n')

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_articles = len(analysis_results)
        total_time = sum(a['reading_time']['total_minutes'] for a in analysis_results)
        total_words = sum(a['reading_time']['words'] for a in analysis_results)
        avg_complexity = sum(a['complexity']['overall_complexity_score'] for a in analysis_results) / total_articles if total_articles > 0 else 0

        html.append('<div class="summary">\n')
        html.append('<h2>–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h2>\n')
        html.append(f'<p><strong>–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π:</strong> {total_articles}</p>\n')
        html.append(f'<p><strong>–û–±—â–µ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è:</strong> {self.format_time(total_time)}</p>\n')
        html.append(f'<p><strong>–í—Å–µ–≥–æ —Å–ª–æ–≤:</strong> {total_words:,}</p>\n')
        html.append(f'<p><strong>–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å:</strong> {avg_complexity:.2f} / 3.0</p>\n')
        html.append('</div>\n')

        # –ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è
        for analysis in sorted(analysis_results, key=lambda x: x['reading_time']['total_minutes'], reverse=True):
            rt = analysis['reading_time']
            comp = analysis['complexity']
            flesch = analysis['flesch_reading_ease']
            ari = analysis['automated_readability_index']
            fatigue = analysis['fatigue_analysis']

            html.append('<div class="article-card">\n')
            html.append(f'<div class="article-title">{Path(analysis["file"]).stem}</div>\n')
            html.append(f'<div style="color: #7f8c8d; font-size: 0.9em;">{analysis["file"]}</div>\n')

            html.append('<div class="metrics">\n')

            # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è
            html.append('<div class="metric-box">\n')
            html.append('<div class="metric-label">–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è</div>\n')
            html.append(f'<div class="metric-value">{rt["formatted"]}</div>\n')
            html.append(f'<div style="font-size: 0.85em; color: #7f8c8d; margin-top: 5px;">{rt["words"]:,} —Å–ª–æ–≤</div>\n')
            html.append('</div>\n')

            # –°–ª–æ–∂–Ω–æ—Å—Ç—å
            difficulty_class = f'difficulty-{comp["difficulty_level"]}'
            html.append(f'<div class="metric-box {difficulty_class}">\n')
            html.append('<div class="metric-label">–°–ª–æ–∂–Ω–æ—Å—Ç—å</div>\n')
            html.append(f'<div class="metric-value">{comp["overall_complexity_score"]:.2f}</div>\n')
            html.append(f'<div style="font-size: 0.85em; margin-top: 5px;">{comp["difficulty_level"].replace("_", " ").title()}</div>\n')
            html.append('</div>\n')

            # Flesch Reading Ease
            html.append('<div class="metric-box">\n')
            html.append('<div class="metric-label">Flesch Score</div>\n')
            html.append(f'<div class="metric-value">{flesch["flesch_score"]:.1f}</div>\n')
            html.append(f'<div style="font-size: 0.85em; margin-top: 5px;">{flesch["level"].replace("_", " ").title()}</div>\n')
            html.append('</div>\n')

            # ARI Grade Level
            html.append('<div class="metric-box">\n')
            html.append('<div class="metric-label">ARI Grade</div>\n')
            html.append(f'<div class="metric-value">{ari["grade_level"]}</div>\n')
            html.append(f'<div style="font-size: 0.85em; margin-top: 5px;">{ari["interpretation"].replace("_", " ").title()}</div>\n')
            html.append('</div>\n')

            html.append('</div>\n')

            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            html.append('<div class="complexity-section">\n')
            html.append('<strong>–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:</strong><br>\n')
            html.append(f'‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {comp["sentence_metrics"]["avg_sentence_length"]} —Å–ª–æ–≤<br>\n')
            html.append(f'‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {comp["vocabulary_metrics"]["unique_words"]} –∏–∑ {comp["vocabulary_metrics"]["total_words"]}<br>\n')
            html.append(f'‚Ä¢ Type-Token Ratio: {comp["vocabulary_metrics"]["type_token_ratio"]:.3f}<br>\n')
            html.append(f'‚Ä¢ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å: {comp["technical_metrics"]["technical_level"].replace("_", " ").title()}<br>\n')

            if fatigue['breaks_recommended'] > 0:
                html.append(f'<br><strong>‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è {fatigue["breaks_recommended"]} –ø–µ—Ä–µ—Ä—ã–≤(–æ–≤) –ø–æ 5 –º–∏–Ω—É—Ç</strong><br>\n')
                html.append(f'–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è —Å –ø–µ—Ä–µ—Ä—ã–≤–∞–º–∏: {self.format_time(fatigue["adjusted_time"])}\n')

            html.append('</div>\n')

            html.append('</div>\n')

        html.append('</body>\n</html>')

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(''.join(html))

        print(f"‚úÖ HTML —ç–∫—Å–ø–æ—Ä—Ç: {output_file}")

    def analyze_all_articles(self) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        results = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            analysis = self.analyze_complexity(md_file)

            if analysis:
                results.append(analysis)

        return results

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è"""
        print("\nüìä –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è...\n")

        articles = []
        total_time = 0
        total_words = 0
        total_code = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            reading_time = self.calculate_reading_time(md_file)

            if not reading_time:
                continue

            frontmatter, _ = self.extract_frontmatter_and_content(md_file)

            articles.append({
                'file': str(md_file.relative_to(self.root_dir)),
                'title': frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem,
                'category': frontmatter.get('category', '') if frontmatter else '',
                **reading_time
            })

            total_time += reading_time['total_minutes']
            total_words += reading_time['words']
            total_code += reading_time['code_lines']

        lines = []
        lines.append("# ‚è±Ô∏è  –û—Ç—á—ë—Ç: –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è\n\n")

        lines.append("## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π**: {len(articles)}\n")
        lines.append(f"- **–û–±—â–µ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è**: {self.format_time(total_time)}\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Å–ª–æ–≤**: {total_words:,}\n")
        lines.append(f"- **–°—Ç—Ä–æ–∫ –∫–æ–¥–∞**: {total_code:,}\n")

        if articles:
            avg_time = total_time / len(articles)
            lines.append(f"- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è**: {self.format_time(avg_time)}\n\n")

        # –¢–æ–ø —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        lines.append("## –¢–æ–ø-10 —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π\n\n")
        sorted_articles = sorted(articles, key=lambda x: x['total_minutes'], reverse=True)

        for i, article in enumerate(sorted_articles[:10], 1):
            lines.append(f"{i}. **{article['title']}** ‚Äî {article['formatted']}\n")
            lines.append(f"   - {article['words']:,} —Å–ª–æ–≤")
            if article['code_lines'] > 0:
                lines.append(f", {article['code_lines']} —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞")
            lines.append(f"\n   - `{article['file']}`\n\n")

        # –¢–æ–ø —Å–∞–º—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö
        lines.append("\n## –¢–æ–ø-10 —Å–∞–º—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç–∞—Ç–µ–π\n\n")
        sorted_articles = sorted(articles, key=lambda x: x['total_minutes'])

        for i, article in enumerate(sorted_articles[:10], 1):
            lines.append(f"{i}. **{article['title']}** ‚Äî {article['formatted']}\n")
            lines.append(f"   - {article['words']:,} —Å–ª–æ–≤\n")
            lines.append(f"   - `{article['file']}`\n\n")

        # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        lines.append("\n## –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n")

        by_category = {}
        for article in articles:
            cat = article['category'] or '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'
            if cat not in by_category:
                by_category[cat] = {'count': 0, 'time': 0, 'words': 0}

            by_category[cat]['count'] += 1
            by_category[cat]['time'] += article['total_minutes']
            by_category[cat]['words'] += article['words']

        for cat, stats in sorted(by_category.items()):
            lines.append(f"### {cat}\n\n")
            lines.append(f"- –°—Ç–∞—Ç–µ–π: {stats['count']}\n")
            lines.append(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è: {self.format_time(stats['time'])}\n")
            lines.append(f"- –í—Å–µ–≥–æ —Å–ª–æ–≤: {stats['words']:,}\n")
            lines.append(f"- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {self.format_time(stats['time'] / stats['count'])}\n\n")

        return ''.join(lines)


def main():
    parser = argparse.ArgumentParser(
        description='‚è±Ô∏è  Reading Time Calculator - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s -f knowledge/article.md              # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –¥–ª—è —Ñ–∞–π–ª–∞
  %(prog)s -f knowledge/article.md --analyze    # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
  %(prog)s -u                                   # –û–±–Ω–æ–≤–∏—Ç—å –≤—Ä–µ–º—è –≤–æ –≤—Å–µ—Ö —Å—Ç–∞—Ç—å—è—Ö
  %(prog)s -r                                   # –°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç
  %(prog)s --complexity                         # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
  %(prog)s --json complexity.json               # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
  %(prog)s --html complexity.html               # –≠–∫—Å–ø–æ—Ä—Ç –≤ HTML
  %(prog)s --all                                # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ + –≤—Å–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
        """
    )

    # –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
    parser.add_argument(
        '-f', '--file',
        help='–í—ã—á–∏—Å–ª–∏—Ç—å –≤—Ä–µ–º—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞'
    )

    parser.add_argument(
        '-u', '--update',
        action='store_true',
        help='–û–±–Ω–æ–≤–∏—Ç—å –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤–æ –≤—Å–µ—Ö —Å—Ç–∞—Ç—å—è—Ö (–¥–æ–±–∞–≤–∏—Ç—å –≤ frontmatter)'
    )

    parser.add_argument(
        '-r', '--report',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è (markdown)'
    )

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    parser.add_argument(
        '--analyze',
        action='store_true',
        help='–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (—Å —Ñ–∞–π–ª–æ–º -f –∏–ª–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π)'
    )

    parser.add_argument(
        '--complexity',
        action='store_true',
        help='–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π'
    )

    parser.add_argument(
        '--readability',
        action='store_true',
        help='–ú–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ (Flesch, ARI)'
    )

    # –≠–∫—Å–ø–æ—Ä—Ç
    parser.add_argument(
        '--json',
        metavar='FILE',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON'
    )

    parser.add_argument(
        '--html',
        metavar='FILE',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ HTML —Å –∫—Ä–∞—Å–∏–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º'
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument(
        '-w', '--wpm',
        type=int,
        default=200,
        help='–°–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 200)'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –≤–∏–¥—ã –∞–Ω–∞–ª–∏–∑–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    calc = ReadingTimeCalculator(root_dir, wpm=args.wpm)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ --all
    if args.all:
        args.complexity = True
        args.readability = True
        if not args.json:
            args.json = str(root_dir / "reading_complexity_analysis.json")
        if not args.html:
            args.html = str(root_dir / "reading_complexity_analysis.html")

    # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if args.file:
        file_path = root_dir / args.file

        if args.analyze or args.complexity or args.readability:
            # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            analysis = calc.analyze_complexity(file_path)

            if analysis:
                print(f"\n‚è±Ô∏è  –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {args.file}\n")
                print("=" * 70)

                # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è
                rt = analysis['reading_time']
                print(f"\nüìñ –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {rt['formatted']}")
                print(f"   ‚Ä¢ –°–ª–æ–≤: {rt['words']:,}")
                print(f"   ‚Ä¢ –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {rt['code_lines']}")
                print(f"   ‚Ä¢ –í—Ä–µ–º—è –Ω–∞ —Ç–µ–∫—Å—Ç: {rt['reading_minutes']:.1f} –º–∏–Ω")
                print(f"   ‚Ä¢ –í—Ä–µ–º—è –Ω–∞ –∫–æ–¥: {rt['code_minutes']:.1f} –º–∏–Ω")

                # –°–ª–æ–∂–Ω–æ—Å—Ç—å
                comp = analysis['complexity']
                print(f"\nüîç –°–ª–æ–∂–Ω–æ—Å—Ç—å: {comp['overall_complexity_score']:.2f} / 3.0")
                print(f"   ‚Ä¢ –£—Ä–æ–≤–µ–Ω—å: {comp['difficulty_level'].replace('_', ' ').title()}")
                print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {comp['sentence_metrics']['avg_sentence_length']} —Å–ª–æ–≤")
                print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {comp['vocabulary_metrics']['unique_words']} –∏–∑ {comp['vocabulary_metrics']['total_words']}")
                print(f"   ‚Ä¢ Type-Token Ratio: {comp['vocabulary_metrics']['type_token_ratio']:.3f}")
                print(f"   ‚Ä¢ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å: {comp['technical_metrics']['technical_level'].replace('_', ' ').title()}")

                # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å
                flesch = analysis['flesch_reading_ease']
                ari = analysis['automated_readability_index']
                print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏:")
                print(f"   ‚Ä¢ Flesch Reading Ease: {flesch['flesch_score']:.1f} ({flesch['level'].replace('_', ' ').title()})")
                print(f"   ‚Ä¢ ARI Grade Level: {ari['grade_level']} ({ari['interpretation'].replace('_', ' ').title()})")
                print(f"   ‚Ä¢ –°–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏: {flesch['words_per_sentence']:.1f}")
                print(f"   ‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤ –≤ —Å–ª–æ–≤–µ: {ari['chars_per_word']:.1f}")

                # –£—Å—Ç–∞–ª–æ—Å—Ç—å
                fatigue = analysis['fatigue_analysis']
                if fatigue['breaks_recommended'] > 0:
                    print(f"\n‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                    print(f"   ‚Ä¢ –ü–µ—Ä–µ—Ä—ã–≤–æ–≤: {fatigue['breaks_recommended']} √ó 5 –º–∏–Ω")
                    print(f"   ‚Ä¢ –í—Ä–µ–º—è —Å –ø–µ—Ä–µ—Ä—ã–≤–∞–º–∏: {calc.format_time(fatigue['adjusted_time'])}")

                print("\n" + "=" * 70 + "\n")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª")

        else:
            # –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á—ë—Ç –≤—Ä–µ–º–µ–Ω–∏
            reading_time = calc.calculate_reading_time(file_path)

            if reading_time:
                print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {reading_time['formatted']}\n")
                print(f"   –°–ª–æ–≤: {reading_time['words']:,}")
                print(f"   –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {reading_time['code_lines']}")
                print(f"   –í—Ä–µ–º—è –Ω–∞ —Ç–µ–∫—Å—Ç: {reading_time['reading_minutes']:.1f} –º–∏–Ω")
                print(f"   –í—Ä–µ–º—è –Ω–∞ –∫–æ–¥: {reading_time['code_minutes']:.1f} –º–∏–Ω")
                print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {reading_time['total_minutes']:.1f} –º–∏–Ω\n")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª")

    # –û–±–Ω–æ–≤–∏—Ç—å frontmatter
    elif args.update:
        calc.add_reading_time_to_articles()

    # –°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç
    elif args.report:
        report = calc.generate_report()
        output_file = root_dir / "READING_TIME_REPORT.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(report)

    # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
    elif args.complexity or args.readability or args.json or args.html:
        print("\nüîç –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π...\n")
        results = calc.analyze_all_articles()

        if not results:
            print("‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return

        print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(results)}\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        avg_time = sum(r['reading_time']['total_minutes'] for r in results) / len(results)
        avg_complexity = sum(r['complexity']['overall_complexity_score'] for r in results) / len(results)
        avg_flesch = sum(r['flesch_reading_ease']['flesch_score'] for r in results) / len(results)

        print("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {calc.format_time(avg_time)}")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {avg_complexity:.2f} / 3.0")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π Flesch Score: {avg_flesch:.1f}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        difficulty_dist = Counter(r['complexity']['difficulty_level'] for r in results)
        print(f"\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:")
        for level, count in sorted(difficulty_dist.items()):
            print(f"   ‚Ä¢ {level.replace('_', ' ').title()}: {count}")

        # –¢–æ–ø —Å–∞–º—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö
        print(f"\nüî• –¢–æ–ø-5 —Å–∞–º—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:")
        sorted_by_complexity = sorted(results, key=lambda x: x['complexity']['overall_complexity_score'], reverse=True)
        for i, r in enumerate(sorted_by_complexity[:5], 1):
            filename = Path(r['file']).stem
            score = r['complexity']['overall_complexity_score']
            level = r['complexity']['difficulty_level'].replace('_', ' ').title()
            print(f"   {i}. {filename}: {score:.2f} ({level})")

        # –¢–æ–ø —Å–∞–º—ã—Ö –ø—Ä–æ—Å—Ç—ã—Ö
        print(f"\n‚úÖ –¢–æ–ø-5 —Å–∞–º—ã—Ö –ø—Ä–æ—Å—Ç—ã—Ö —Å—Ç–∞—Ç–µ–π:")
        sorted_by_complexity_asc = sorted(results, key=lambda x: x['complexity']['overall_complexity_score'])
        for i, r in enumerate(sorted_by_complexity_asc[:5], 1):
            filename = Path(r['file']).stem
            score = r['complexity']['overall_complexity_score']
            level = r['complexity']['difficulty_level'].replace('_', ' ').title()
            print(f"   {i}. {filename}: {score:.2f} ({level})")

        # –≠–∫—Å–ø–æ—Ä—Ç—ã
        if args.json:
            json_path = root_dir / args.json if not Path(args.json).is_absolute() else Path(args.json)
            calc.export_analysis_json(results, str(json_path))

        if args.html:
            html_path = root_dir / args.html if not Path(args.html).is_absolute() else Path(args.html)
            calc.export_analysis_html(results, str(html_path))

        print()

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
