#!/usr/bin/env python3
"""
Quality Metrics - –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤

–ú–µ—Ç—Ä–∏–∫–∏:
- Completeness (–ø–æ–ª–Ω–æ—Ç–∞): –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
- Structure (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞): –∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —Å–ø–∏—Å–∫–∏
- Links (—Å—Å—ã–ª–∫–∏): –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
- Examples (–ø—Ä–∏–º–µ—Ä—ã): –∫–æ–¥, —Ç–∞–±–ª–∏—Ü—ã, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- Readability (—á–∏—Ç–∞–µ–º–æ—Å—Ç—å): –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- Freshness (—Å–≤–µ–∂–µ—Å—Ç—å): –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
"""

from pathlib import Path
import yaml
import re
from datetime import datetime, timedelta
from collections import defaultdict


class QualityAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ frontmatter
        self.required_fields = ['title', 'date', 'category', 'tags', 'status']

        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–æ–ª—è
        self.recommended_fields = ['author', 'source', 'subcategory', 'related']

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                fm = yaml.safe_load(match.group(1))
                body = match.group(2)
                return fm, body
        except:
            pass

        return None, None

    def analyze_completeness(self, frontmatter):
        """
        –û—Ü–µ–Ω–∏—Ç—å –ø–æ–ª–Ω–æ—Ç—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (0-100)
        """
        if not frontmatter:
            return 0

        score = 0
        total_points = 100

        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (60 –±–∞–ª–ª–æ–≤)
        required_points = 60
        field_points = required_points / len(self.required_fields)

        for field in self.required_fields:
            if field in frontmatter and frontmatter[field]:
                score += field_points

        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–æ–ª—è (30 –±–∞–ª–ª–æ–≤)
        recommended_points = 30
        field_points = recommended_points / len(self.recommended_fields)

        for field in self.recommended_fields:
            if field in frontmatter and frontmatter[field]:
                score += field_points

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (10 –±–∞–ª–ª–æ–≤)
        bonus_fields = ['dewey', 'pagerank', 'reading_time', 'difficulty']
        bonus_count = sum(1 for f in bonus_fields if f in frontmatter)
        score += min(10, bonus_count * 2.5)

        return min(100, round(score))

    def analyze_structure(self, content):
        """
        –û—Ü–µ–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (0-100)
        """
        if not content:
            return 0

        score = 0

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ (30 –±–∞–ª–ª–æ–≤)
        h1_count = len(re.findall(r'^# ', content, re.MULTILINE))
        h2_count = len(re.findall(r'^## ', content, re.MULTILINE))
        h3_count = len(re.findall(r'^### ', content, re.MULTILINE))

        # –•–æ—Ä–æ—à–æ: –Ω–µ—Ç h1 (–æ–Ω –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ), –µ—Å—Ç—å h2 –∏ h3
        if h1_count == 0 and h2_count >= 2:
            score += 30
        elif h2_count >= 1:
            score += 15

        # –°–ø–∏—Å–∫–∏ (20 –±–∞–ª–ª–æ–≤)
        lists = re.findall(r'^\s*[\*\-\+] ', content, re.MULTILINE)
        if len(lists) >= 5:
            score += 20
        elif len(lists) >= 2:
            score += 10

        # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã (—Ä–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞) (20 –±–∞–ª–ª–æ–≤)
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        if paragraphs:
            avg_para_length = sum(len(p.split()) for p in paragraphs) / len(paragraphs)
            # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ: 40-100 —Å–ª–æ–≤ –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ
            if 40 <= avg_para_length <= 100:
                score += 20
            elif 20 <= avg_para_length <= 150:
                score += 10

        # –¢–∞–±–ª–∏—Ü—ã (15 –±–∞–ª–ª–æ–≤)
        tables = re.findall(r'^\|', content, re.MULTILINE)
        if tables:
            score += 15

        # –¶–∏—Ç–∞—Ç—ã (15 –±–∞–ª–ª–æ–≤)
        quotes = re.findall(r'^> ', content, re.MULTILINE)
        if quotes:
            score += 15

        return min(100, score)

    def analyze_links(self, content):
        """
        –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (0-100)
        """
        if not content:
            return 0

        score = 0

        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (40 –±–∞–ª–ª–æ–≤)
        internal_links = re.findall(r'\[([^\]]+)\]\(([^h][^\)]+)\)', content)
        if len(internal_links) >= 3:
            score += 40
        elif len(internal_links) >= 1:
            score += 20

        # –í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (30 –±–∞–ª–ª–æ–≤)
        external_links = re.findall(r'\[([^\]]+)\]\((https?://[^\)]+)\)', content)
        if len(external_links) >= 2:
            score += 30
        elif len(external_links) >= 1:
            score += 15

        # –Ø–∫–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (15 –±–∞–ª–ª–æ–≤)
        anchors = re.findall(r'\[([^\]]+)\]\(#[^\)]+\)', content)
        if anchors:
            score += 15

        # –°—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (15 –±–∞–ª–ª–æ–≤)
        images = re.findall(r'!\[([^\]]*)\]\([^\)]+\)', content)
        if images:
            score += 15

        return min(100, score)

    def analyze_examples(self, content):
        """
        –û—Ü–µ–Ω–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π (0-100)
        """
        if not content:
            return 0

        score = 0

        # –ë–ª–æ–∫–∏ –∫–æ–¥–∞ (50 –±–∞–ª–ª–æ–≤)
        code_blocks = re.findall(r'```.*?```', content, re.DOTALL)
        if len(code_blocks) >= 3:
            score += 50
        elif len(code_blocks) >= 1:
            score += 25

        # –ò–Ω–ª–∞–π–Ω –∫–æ–¥ (20 –±–∞–ª–ª–æ–≤)
        inline_code = re.findall(r'`[^`]+`', content)
        if len(inline_code) >= 5:
            score += 20
        elif len(inline_code) >= 2:
            score += 10

        # –¢–∞–±–ª–∏—Ü—ã (15 –±–∞–ª–ª–æ–≤)
        tables = len(re.findall(r'\|.*\|', content))
        if tables >= 3:
            score += 15
        elif tables >= 1:
            score += 7

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (15 –±–∞–ª–ª–æ–≤)
        images = re.findall(r'!\[', content)
        if images:
            score += 15

        return min(100, score)

    def analyze_readability(self, content):
        """
        –û—Ü–µ–Ω–∏—Ç—å —á–∏—Ç–∞–µ–º–æ—Å—Ç—å (0-100)
        """
        if not content:
            return 0

        score = 50  # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª

        # –£–¥–∞–ª–∏—Ç—å –∫–æ–¥ –∏ —Å—Å—ã–ª–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        text = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
        text = re.sub(r'`[^`]+`', '', text)
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)

        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if sentences:
            # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–ª–æ–≤)
            words_per_sentence = []
            for sent in sentences:
                words = re.findall(r'\b[–∞-—è—ëa-z]+\b', sent.lower())
                words_per_sentence.append(len(words))

            if words_per_sentence:
                avg_words = sum(words_per_sentence) / len(words_per_sentence)

                # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ: 15-25 —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                if 15 <= avg_words <= 25:
                    score += 30
                elif 10 <= avg_words <= 30:
                    score += 15
                else:
                    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ
                    score -= 10

        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (—Ä–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞ vs –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text.lower())
        if words:
            unique_ratio = len(set(words)) / len(words)
            # –ß–µ–º –≤—ã—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, —Ç–µ–º –ª—É—á—à–µ
            score += int(unique_ratio * 20)

        return min(100, max(0, score))

    def analyze_freshness(self, frontmatter):
        """
        –û—Ü–µ–Ω–∏—Ç—å —Å–≤–µ–∂–µ—Å—Ç—å —Å—Ç–∞—Ç—å–∏ (0-100)
        """
        if not frontmatter or 'date' not in frontmatter:
            return 0

        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
            date_str = str(frontmatter['date'])
            if isinstance(frontmatter['date'], datetime):
                article_date = frontmatter['date']
            else:
                article_date = datetime.fromisoformat(date_str.split()[0])

            now = datetime.now()
            age = (now - article_date).days

            # –ß–µ–º –Ω–æ–≤–µ–µ, —Ç–µ–º –ª—É—á—à–µ
            if age <= 30:  # –ú–µ—Å—è—Ü
                return 100
            elif age <= 90:  # 3 –º–µ—Å—è—Ü–∞
                return 90
            elif age <= 180:  # 6 –º–µ—Å—è—Ü–µ–≤
                return 75
            elif age <= 365:  # –ì–æ–¥
                return 60
            elif age <= 730:  # 2 –≥–æ–¥–∞
                return 40
            else:
                return 20

        except:
            return 50  # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ

    def calculate_overall_score(self, metrics):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞

        –í–µ—Å–∞:
        - Completeness: 20%
        - Structure: 20%
        - Links: 15%
        - Examples: 15%
        - Readability: 20%
        - Freshness: 10%
        """
        weights = {
            'completeness': 0.20,
            'structure': 0.20,
            'links': 0.15,
            'examples': 0.15,
            'readability': 0.20,
            'freshness': 0.10
        }

        score = sum(metrics[key] * weights[key] for key in weights.keys())

        return round(score)

    def analyze_article(self, file_path):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏"""
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        metrics = {
            'completeness': self.analyze_completeness(frontmatter),
            'structure': self.analyze_structure(content),
            'links': self.analyze_links(content),
            'examples': self.analyze_examples(content),
            'readability': self.analyze_readability(content),
            'freshness': self.analyze_freshness(frontmatter)
        }

        overall = self.calculate_overall_score(metrics)

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞
        if overall >= 90:
            grade = 'A+'
            quality = 'Excellent'
        elif overall >= 80:
            grade = 'A'
            quality = 'Very Good'
        elif overall >= 70:
            grade = 'B'
            quality = 'Good'
        elif overall >= 60:
            grade = 'C'
            quality = 'Satisfactory'
        elif overall >= 50:
            grade = 'D'
            quality = 'Needs Improvement'
        else:
            grade = 'F'
            quality = 'Poor'

        return {
            **metrics,
            'overall': overall,
            'grade': grade,
            'quality': quality,
            'file': str(file_path.relative_to(self.root_dir)),
            'title': frontmatter.get('title', file_path.stem) if frontmatter else file_path.stem
        }

    def add_quality_scores_to_articles(self):
        """–î–æ–±–∞–≤–∏—Ç—å –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º"""
        print("üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π...\n")

        count = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            analysis = self.analyze_article(md_file)

            # –û–±–Ω–æ–≤–∏—Ç—å frontmatter
            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not frontmatter:
                continue

            # –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            frontmatter['quality_score'] = analysis['overall']
            frontmatter['quality_grade'] = analysis['grade']
            frontmatter['quality_metrics'] = {
                'completeness': analysis['completeness'],
                'structure': analysis['structure'],
                'links': analysis['links'],
                'examples': analysis['examples'],
                'readability': analysis['readability'],
                'freshness': analysis['freshness']
            }

            # –ó–∞–ø–∏—Å–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ
            try:
                new_content = "---\n"
                new_content += yaml.dump(frontmatter, allow_unicode=True, sort_keys=False)
                new_content += "---\n\n"
                new_content += content

                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(new_content)

                count += 1
                print(f"‚úÖ {md_file.relative_to(self.root_dir)} ‚Äî {analysis['grade']} ({analysis['overall']}/100)")

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ {md_file}: {e}")

        print(f"\n‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {count}")

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
        print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É...\n")

        articles = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            analysis = self.analyze_article(md_file)
            articles.append(analysis)

        lines = []
        lines.append("# üìä –û—Ç—á—ë—Ç: –ö–∞—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if articles:
            avg_score = sum(a['overall'] for a in articles) / len(articles)

            lines.append("## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
            lines.append(f"- **–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π**: {len(articles)}\n")
            lines.append(f"- **–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª**: {avg_score:.1f}/100\n\n")

            # –ü–æ –æ—Ü–µ–Ω–∫–∞–º
            by_grade = defaultdict(int)
            for article in articles:
                by_grade[article['grade']] += 1

            lines.append("## –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –æ—Ü–µ–Ω–∫–∞–º\n\n")
            for grade in ['A+', 'A', 'B', 'C', 'D', 'F']:
                count = by_grade.get(grade, 0)
                pct = (count / len(articles)) * 100 if articles else 0
                bar = '‚ñà' * int(pct / 5)
                lines.append(f"- **{grade}**: {count} ({pct:.1f}%) {bar}\n")

            # –¢–æ–ø —Å—Ç–∞—Ç–µ–π
            lines.append("\n## –¢–æ–ø-10 –ª—É—á—à–∏—Ö —Å—Ç–∞—Ç–µ–π\n\n")
            sorted_articles = sorted(articles, key=lambda x: x['overall'], reverse=True)

            for i, article in enumerate(sorted_articles[:10], 1):
                lines.append(f"### {i}. {article['title']} ‚Äî {article['grade']} ({article['overall']}/100)\n\n")
                lines.append(f"üìÇ `{article['file']}`\n\n")
                lines.append("**–ú–µ—Ç—Ä–∏–∫–∏:**\n")
                lines.append(f"- –ü–æ–ª–Ω–æ—Ç–∞: {article['completeness']}/100\n")
                lines.append(f"- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {article['structure']}/100\n")
                lines.append(f"- –°—Å—ã–ª–∫–∏: {article['links']}/100\n")
                lines.append(f"- –ü—Ä–∏–º–µ—Ä—ã: {article['examples']}/100\n")
                lines.append(f"- –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å: {article['readability']}/100\n")
                lines.append(f"- –°–≤–µ–∂–µ—Å—Ç—å: {article['freshness']}/100\n\n")

            # –°—Ç–∞—Ç—å–∏ —Ç—Ä–µ–±—É—é—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
            lines.append("\n## –°—Ç–∞—Ç—å–∏, —Ç—Ä–µ–±—É—é—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è\n\n")
            needs_improvement = [a for a in sorted_articles if a['overall'] < 70]

            if needs_improvement:
                for article in needs_improvement[-10:]:
                    lines.append(f"### {article['title']} ‚Äî {article['grade']} ({article['overall']}/100)\n\n")
                    lines.append(f"üìÇ `{article['file']}`\n\n")

                    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    lines.append("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:**\n\n")
                    if article['completeness'] < 70:
                        lines.append("- ‚ö†Ô∏è  –î–æ–ø–æ–ª–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (tags, author, related)\n")
                    if article['structure'] < 70:
                        lines.append("- ‚ö†Ô∏è  –£–ª—É—á—à–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–¥–æ–±–∞–≤–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏)\n")
                    if article['links'] < 70:
                        lines.append("- ‚ö†Ô∏è  –î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n")
                    if article['examples'] < 70:
                        lines.append("- ‚ö†Ô∏è  –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏–ª–∏ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–∏\n")
                    if article['readability'] < 70:
                        lines.append("- ‚ö†Ô∏è  –£–ª—É—á—à–∏—Ç—å —á–∏—Ç–∞–µ–º–æ—Å—Ç—å (—É–ø—Ä–æ—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n")
                    if article['freshness'] < 70:
                        lines.append("- ‚ö†Ô∏è  –û–±–Ω–æ–≤–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n")

                    lines.append("\n")
            else:
                lines.append("–í—Å–µ —Å—Ç–∞—Ç—å–∏ –∏–º–µ—é—Ç —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ! üéâ\n\n")

        return ''.join(lines)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Quality Metrics - –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π'
    )

    parser.add_argument(
        '-f', '--file',
        help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª'
    )

    parser.add_argument(
        '-u', '--update',
        action='store_true',
        help='–û–±–Ω–æ–≤–∏—Ç—å –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–æ –≤—Å–µ—Ö —Å—Ç–∞—Ç—å—è—Ö'
    )

    parser.add_argument(
        '-r', '--report',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    analyzer = QualityAnalyzer(root_dir)

    if args.file:
        file_path = root_dir / args.file
        analysis = analyzer.analyze_article(file_path)

        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞: {analysis['title']}\n")
        print(f"   –û–±—â–∏–π –±–∞–ª–ª: {analysis['overall']}/100 ({analysis['grade']} - {analysis['quality']})\n")
        print("   –î–µ—Ç–∞–ª–∏:")
        print(f"      –ü–æ–ª–Ω–æ—Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {analysis['completeness']}/100")
        print(f"      –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {analysis['structure']}/100")
        print(f"      –°—Å—ã–ª–∫–∏: {analysis['links']}/100")
        print(f"      –ü—Ä–∏–º–µ—Ä—ã: {analysis['examples']}/100")
        print(f"      –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å: {analysis['readability']}/100")
        print(f"      –°–≤–µ–∂–µ—Å—Ç—å: {analysis['freshness']}/100\n")

    elif args.update:
        analyzer.add_quality_scores_to_articles()

    elif args.report:
        report = analyzer.generate_report()
        output_file = root_dir / "QUALITY_REPORT.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(report)

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
