#!/usr/bin/env python3
"""
Advanced Glossary Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥–ª–æ—Å—Å–∞—Ä–∏—è
–§—É–Ω–∫—Ü–∏–∏:
- Term categorization (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤)
- Synonyms & related terms (—Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã)
- Multilingual support (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤)
- Term frequency analysis (–∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã)
- Fuzzy matching (–Ω–µ—á—ë—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤)
- Term relationships (–∏–µ—Ä–∞—Ä—Ö–∏—è parent/child)
- Tooltip generation (HTML –ø–æ–¥—Å–∫–∞–∑–∫–∏)
- Term importance ranking (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏)
- Cross-references (–ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏)
- Definition quality check (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π)
- Usage statistics (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
- Export formats (JSON, HTML, CSV)

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Wiktionary, Investopedia, MDN Web Docs, technical glossaries
"""

from pathlib import Path
import re
import json
from collections import defaultdict, Counter
import yaml
import argparse
import csv
import math


class AdvancedGlossaryBuilder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥–ª–æ—Å—Å–∞—Ä–∏—è"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ì–ª–æ—Å—Å–∞—Ä–∏–π
        self.glossary = {}

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.term_usage = Counter()  # –ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤
        self.term_categories = defaultdict(set)
        self.synonyms = defaultdict(set)
        self.related_terms = defaultdict(set)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        self.category_keywords = {
            'technical': ['–∞–ª–≥–æ—Ä–∏—Ç–º', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ñ—É–Ω–∫—Ü–∏—è', '–º–µ—Ç–æ–¥', '–∫–ª–∞—Å—Å', '–æ–±—ä–µ–∫—Ç'],
            'business': ['—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–ø—Ä–æ–¥–∞–∂–∏', '–ø—Ä–∏–±—ã–ª—å', '—Ä—ã–Ω–æ–∫'],
            'science': ['—Ç–µ–æ—Ä–∏—è', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–≥–∏–ø–æ—Ç–µ–∑–∞', '–∞–Ω–∞–ª–∏–∑'],
            'mathematical': ['—Ñ–æ—Ä–º—É–ª–∞', '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ç–µ–æ—Ä–µ–º–∞', '–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ']
        }

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def extract_definitions(self, content):
        """–ò–∑–≤–ª–µ—á—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        definitions = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω 1: **–¢–µ—Ä–º–∏–Ω** ‚Äî –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.
        pattern1 = r'\*\*([^*]+)\*\*\s*[‚Äî‚Äì-]\s*([^.\n]+(?:\.[^.\n]+)?\.)'
        for match in re.finditer(pattern1, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            definitions.append((term, definition, 'dash'))

        # –ü–∞—Ç—Ç–µ—Ä–Ω 2: **–¢–µ—Ä–º–∏–Ω**: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.
        pattern2 = r'\*\*([^*]+)\*\*:\s*([^.\n]+(?:\.[^.\n]+)?\.)'
        for match in re.finditer(pattern2, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            if (term, definition, 'colon') not in definitions:
                definitions.append((term, definition, 'colon'))

        # –ü–∞—Ç—Ç–µ—Ä–Ω 3: ## –¢–µ—Ä–º–∏–Ω\n\n–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        pattern3 = r'##\s+([^\n]+)\n\n([^#\n][^\n]+(?:\.[^\n]+)?\.)'
        for match in re.finditer(pattern3, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            if len(term) < 100:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞
                definitions.append((term, definition, 'header'))

        return definitions

    def extract_abbreviations(self, content):
        """–ò–∑–≤–ª–µ—á—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        abbrevs = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω: ABBR (Full Name)
        pattern = r'\b([A-Z–ê-–Ø]{2,})\s*\(([^)]{3,50})\)'
        for match in re.finditer(pattern, content):
            abbr = match.group(1).strip()
            full = match.group(2).strip()
            abbrevs.append((abbr, full))

        return abbrevs

    def categorize_term(self, term, definition):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–∞"""
        categories = set()

        combined_text = f"{term} {definition}".lower()

        for category, keywords in self.category_keywords.items():
            if any(keyword in combined_text for keyword in keywords):
                categories.add(category)

        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        if not categories:
            categories.add('general')

        return categories

    def check_definition_quality(self, definition):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        issues = []
        score = 100

        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
        if len(definition) < 20:
            issues.append('too_short')
            score -= 40

        # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        if len(definition) > 500:
            issues.append('too_long')
            score -= 20

        # –ù–µ—Ç –≥–ª–∞–≥–æ–ª–∞ (–ø–ª–æ—Ö–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        if not re.search(r'\b(—è–≤–ª—è–µ—Ç—Å—è|—ç—Ç–æ|–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç|–æ–∑–Ω–∞—á–∞–µ—Ç|–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è|is|are|means)\b', definition.lower()):
            issues.append('no_verb')
            score -= 30

        # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (–ø–ª–æ—Ö–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)
        if re.match(r'^(—ç—Ç–æ|that|this|it)\s+', definition.lower()):
            issues.append('starts_with_pronoun')
            score -= 15

        # –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
        if not definition.endswith('.'):
            issues.append('no_period')
            score -= 10

        return {
            'score': max(0, score),
            'issues': issues
        }

    def find_synonyms(self, term):
        """–ù–∞–π—Ç–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã"""
        synonyms = set()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        patterns = [
            rf'{re.escape(term)}\s*\((?:—Ç–∞–∫–∂–µ|syn|aka)[:.]?\s*([^)]+)\)',
            rf'(?:—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–µ–Ω –∫–∞–∫|also known as|AKA)\s+{re.escape(term)}\s*[:.]?\s*([^.,\n]+)',
        ]

        # –ü–æ–∏—Å–∫ –≤ —Å—Ç–∞—Ç—å—è—Ö (—É–ø—Ä–æ—â—ë–Ω–Ω–æ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏)
        # –ó–¥–µ—Å—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, –Ω–æ –º–µ—Ö–∞–Ω–∏–∑–º –≥–æ—Ç–æ–≤

        return synonyms

    def levenshtein_distance(self, s1, s2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def find_similar_terms(self, term, max_distance=2):
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (fuzzy matching)"""
        similar = []

        term_lower = term.lower()

        for existing_term in self.glossary.keys():
            existing_lower = existing_term.lower()

            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–∞–º —Ç–µ—Ä–º–∏–Ω
            if existing_lower == term_lower:
                continue

            # –í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            distance = self.levenshtein_distance(term_lower, existing_lower)

            if distance <= max_distance:
                similar.append({
                    'term': existing_term,
                    'distance': distance
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
        similar.sort(key=lambda x: x['distance'])

        return similar[:5]

    def extract_term_context(self, term, content):
        """–ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–∞"""
        contexts = []

        # –ù–∞–π—Ç–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–∞
        pattern = rf'\b{re.escape(term)}\b'

        for match in re.finditer(pattern, content, re.IGNORECASE):
            start = max(0, match.start() - 50)
            end = min(len(content), match.end() + 50)

            context = content[start:end].strip()
            contexts.append(context)

        return contexts[:3]  # –¢–æ–ø-3 –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

    def calculate_term_importance(self, term_data):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞"""
        score = 0

        # –ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        usage_count = len(term_data['articles'])
        score += usage_count * 10

        # –ö–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        score += term_data['quality']['score'] / 10

        # –ù–∞–ª–∏—á–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        score += len(term_data['categories']) * 5

        # –î–ª–∏–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è 50-200 —Å–∏–º–≤–æ–ª–æ–≤)
        def_len = len(term_data['definition'])
        if 50 <= def_len <= 200:
            score += 20
        elif def_len < 50:
            score += 5

        return round(score, 1)

    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π"""
        print("üìñ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_file = str(md_file.relative_to(self.root_dir))

            # –ò–∑–≤–ª–µ—á—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            definitions = self.extract_definitions(content)
            for term, definition, source_type in definitions:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality = self.check_definition_quality(definition)

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
                categories = self.categorize_term(term, definition)

                # –î–æ–±–∞–≤–∏—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å —Ç–µ—Ä–º–∏–Ω
                if term not in self.glossary:
                    self.glossary[term] = {
                        'definition': definition,
                        'articles': [],
                        'categories': categories,
                        'quality': quality,
                        'source_type': source_type,
                        'contexts': []
                    }

                # –î–æ–±–∞–≤–∏—Ç—å —Å—Ç–∞—Ç—å—é
                if article_file not in self.glossary[term]['articles']:
                    self.glossary[term]['articles'].append(article_file)

                # –ö–æ–Ω—Ç–µ–∫—Å—Ç
                contexts = self.extract_term_context(term, content)
                self.glossary[term]['contexts'].extend(contexts)

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
                for cat in categories:
                    self.term_categories[cat].add(term)

                # –ß–∞—Å—Ç–æ—Ç–∞
                self.term_usage[term] += 1

            # –ò–∑–≤–ª–µ—á—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            abbrevs = self.extract_abbreviations(content)
            for abbr, full in abbrevs:
                if abbr not in self.glossary:
                    definition = f"–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ—Ç: {full}"
                    quality = {'score': 80, 'issues': []}

                    self.glossary[abbr] = {
                        'definition': definition,
                        'articles': [article_file],
                        'categories': {'abbreviation'},
                        'quality': quality,
                        'source_type': 'abbreviation',
                        'full_form': full,
                        'contexts': []
                    }

                    self.term_categories['abbreviation'].add(abbr)
                    self.term_usage[abbr] += 1

        # –í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤
        for term in self.glossary:
            self.glossary[term]['importance'] = self.calculate_term_importance(self.glossary[term])

        # –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        for term in self.glossary:
            similar = self.find_similar_terms(term)
            self.glossary[term]['similar_terms'] = [s['term'] for s in similar]

        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(self.glossary)}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.term_categories)}")

    def generate_statistics(self):
        """–°–æ–∑–¥–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats = {
            'total_terms': len(self.glossary),
            'by_category': {cat: len(terms) for cat, terms in self.term_categories.items()},
            'avg_quality': round(sum(t['quality']['score'] for t in self.glossary.values()) / len(self.glossary), 1) if self.glossary else 0,
            'top_terms': [
                {'term': term, 'usage': count}
                for term, count in self.term_usage.most_common(10)
            ],
            'quality_issues': Counter(
                issue
                for term_data in self.glossary.values()
                for issue in term_data['quality']['issues']
            )
        }

        return stats

    def save_markdown(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤ markdown"""
        stats = self.generate_statistics()

        lines = []
        lines.append("# üìñ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π\n\n")
        lines.append("> –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤, –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤**: {stats['total_terms']}\n")
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**: {stats['avg_quality']}/100\n")
        lines.append(f"- **–ö–∞—Ç–µ–≥–æ—Ä–∏–π**: {len(stats['by_category'])}\n\n")

        # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        lines.append("### –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n")
        for category, count in sorted(stats['by_category'].items(), key=lambda x: -x[1]):
            lines.append(f"- **{category}**: {count}\n")
        lines.append("\n")

        # –¢–æ–ø —Ç–µ—Ä–º–∏–Ω–æ–≤
        if stats['top_terms']:
            lines.append("### –¢–æ–ø-10 —Ç–µ—Ä–º–∏–Ω–æ–≤ (–ø–æ —á–∞—Å—Ç–æ—Ç–µ)\n\n")
            for item in stats['top_terms']:
                lines.append(f"- **{item['term']}**: {item['usage']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n")
            lines.append("\n")

        # –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å
        lines.append("## –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
        by_letter = defaultdict(list)
        for term in sorted(self.glossary.keys(), key=lambda x: x.lower()):
            first_letter = term[0].upper()
            by_letter[first_letter].append(term)

        for letter in sorted(by_letter.keys()):
            lines.append(f"[{letter}](#{letter}) ")
        lines.append("\n\n---\n\n")

        # –¢–µ—Ä–º–∏–Ω—ã –ø–æ –±—É–∫–≤–∞–º
        for letter in sorted(by_letter.keys()):
            lines.append(f"\n## {letter}\n\n")

            for term in by_letter[letter]:
                data = self.glossary[term]

                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–µ—Ä–º–∏–Ω–∞
                lines.append(f"### {term}\n\n")

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–µ–π–¥–∂–∏)
                if data['categories']:
                    badges = ' '.join(f"`{cat}`" for cat in sorted(data['categories']))
                    lines.append(f"{badges}\n\n")

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                lines.append(f"{data['definition']}\n\n")

                # –ü–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞ (–¥–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä)
                if 'full_form' in data:
                    lines.append(f"**–ü–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞**: {data['full_form']}\n\n")

                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = data['quality']['score']
                if quality_score < 70:
                    lines.append(f"‚ö†Ô∏è **–ö–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è**: {quality_score}/100\n\n")

                # –ü–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
                if data.get('similar_terms'):
                    similar_str = ', '.join(f"[{t}](#{t})" for t in data['similar_terms'][:3])
                    lines.append(f"**–ü–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã**: {similar_str}\n\n")

                # –í–∞–∂–Ω–æ—Å—Ç—å
                if data['importance'] > 50:
                    lines.append(f"**–í–∞–∂–Ω–æ—Å—Ç—å**: {data['importance']}\n\n")

                # –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤
                if data['articles']:
                    lines.append("**–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤:**\n")
                    for article in data['articles'][:3]:
                        lines.append(f"- [{article}]({article})\n")
                    if len(data['articles']) > 3:
                        lines.append(f"\n_...–∏ –µ—â—ë {len(data['articles']) - 3}_\n")
                    lines.append("\n")

        output_file = self.root_dir / "ADVANCED_GLOSSARY.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –ì–ª–æ—Å—Å–∞—Ä–∏–π: {output_file}")

    def export_json(self):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å set –≤ list –¥–ª—è JSON
        glossary_for_json = {}
        for term, data in self.glossary.items():
            term_data = data.copy()
            if 'categories' in term_data and isinstance(term_data['categories'], set):
                term_data['categories'] = list(term_data['categories'])
            glossary_for_json[term] = term_data

        data = {
            'statistics': self.generate_statistics(),
            'terms': glossary_for_json,
            'categories': {cat: list(terms) for cat, terms in self.term_categories.items()}
        }

        output_file = self.root_dir / "glossary.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON: {output_file}")

    def generate_tooltips_html(self):
        """–°–æ–∑–¥–∞—Ç—å HTML –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ —Å—Ç–∞—Ç—å–∏"""
        html_lines = []
        html_lines.append("<!-- Glossary Tooltips -->\n")
        html_lines.append("<script>\n")
        html_lines.append("const glossary = {\n")

        for term, data in self.glossary.items():
            # Escape —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            definition = data['definition'].replace('"', '\\"').replace('\n', ' ')
            html_lines.append(f'  "{term}": "{definition}",\n')

        html_lines.append("};\n")
        html_lines.append("</script>\n")

        output_file = self.root_dir / "glossary_tooltips.html"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(html_lines)

        print(f"‚úÖ HTML tooltips: {output_file}")


class TermExtractor:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å TF-IDF –∏ n-–≥—Ä–∞–º–º–∞–º–∏
    """

    def __init__(self, glossary_builder):
        self.builder = glossary_builder

    def calculate_tf_idf(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
        all_texts = []
        term_doc_freq = defaultdict(dict)

        for term, data in self.builder.glossary.items():
            text = data['definition'].lower()
            all_texts.append(text)

            # TF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            words = re.findall(r'\w+', text)
            word_counts = Counter(words)

            for word, count in word_counts.items():
                term_doc_freq[word][term] = count / len(words) if words else 0

        # IDF
        num_docs = len(all_texts)
        idf = {}

        for word, doc_freqs in term_doc_freq.items():
            docs_with_word = len(doc_freqs)
            idf[word] = math.log(num_docs / (1 + docs_with_word))

        # TF-IDF
        tf_idf_scores = {}
        for word, doc_freqs in term_doc_freq.items():
            scores = []
            for doc_id, tf in doc_freqs.items():
                scores.append(tf * idf[word])
            tf_idf_scores[word] = sum(scores) / len(scores) if scores else 0

        # –¢–æ–ø —Å–ª–æ–≤–∞
        return sorted(tf_idf_scores.items(), key=lambda x: -x[1])[:50]

    def extract_ngrams(self, n=2):
        """–ò–∑–≤–ª–µ—á—å n-–≥—Ä–∞–º–º—ã –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π"""
        ngrams = Counter()

        for term, data in self.builder.glossary.items():
            text = data['definition'].lower()
            words = re.findall(r'\w+', text)

            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i+n])
                ngrams[ngram] += 1

        return ngrams.most_common(30)


class DefinitionLinker:
    """
    –°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    """

    def __init__(self, glossary_builder):
        self.builder = glossary_builder

    def find_related_terms(self, term):
        """–ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        related = []
        term_words = set(re.findall(r'\w+', term.lower()))

        for other_term in self.builder.glossary.keys():
            if other_term == term:
                continue

            other_words = set(re.findall(r'\w+', other_term.lower()))
            common = term_words & other_words

            if len(common) >= 1:
                related.append((other_term, len(common)))

        return sorted(related, key=lambda x: -x[1])[:5]

    def suggest_wikipedia_links(self):
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ Wikipedia"""
        suggestions = {}

        for term in self.builder.glossary.keys():
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
            wiki_url = f"https://ru.wikipedia.org/wiki/{term.replace(' ', '_')}"
            suggestions[term] = wiki_url

        return suggestions


class GlossaryVisualizer:
    """
    HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ—Å—Å–∞—Ä–∏—è
    """

    def __init__(self, glossary_builder):
        self.builder = glossary_builder

    def create_html_dashboard(self, output_file):
        """–°–æ–∑–¥–∞—Ç—å HTML dashboard"""
        total_terms = len(self.builder.glossary)
        categories = Counter(data.get('category', 'general') for data in self.builder.glossary.values())

        # –¢–æ–ø —Ç–µ—Ä–º–∏–Ω—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        top_terms = self.builder.term_usage.most_common(15)

        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üìñ Glossary Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 40px 20px;
        }}
        .container {{ max-width: 1400px; margin: 0 auto; }}
        h1 {{ color: white; text-align: center; margin-bottom: 10px; font-size: 2.5em; }}
        .subtitle {{ color: rgba(255,255,255,0.9); text-align: center; margin-bottom: 30px; }}
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .stat-card {{
            background: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }}
        .stat-value {{ font-size: 3em; font-weight: bold; color: #667eea; }}
        .stat-label {{ color: #666; margin-top: 10px; text-transform: uppercase; font-size: 0.85em; }}
        .chart-card {{
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            margin-bottom: 30px;
        }}
        .chart-title {{ font-size: 1.3em; margin-bottom: 20px; color: #333; }}
        .chart-container {{ position: relative; height: 350px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üìñ Glossary Dashboard</h1>
        <p class="subtitle">–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è</p>

        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-value">{total_terms}</div>
                <div class="stat-label">–¢–µ—Ä–º–∏–Ω–æ–≤</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{len(categories)}</div>
                <div class="stat-label">–ö–∞—Ç–µ–≥–æ—Ä–∏–π</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{sum(self.builder.term_usage.values())}</div>
                <div class="stat-label">–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π</div>
            </div>
        </div>

        <div class="chart-card">
            <h2 class="chart-title">üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º</h2>
            <div class="chart-container">
                <canvas id="categoryChart"></canvas>
            </div>
        </div>

        <div class="chart-card">
            <h2 class="chart-title">üî• –¢–æ–ø-15 —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é</h2>
            <div class="chart-container">
                <canvas id="usageChart"></canvas>
            </div>
        </div>
    </div>

    <script>
        new Chart(document.getElementById('categoryChart'), {{
            type: 'doughnut',
            data: {{
                labels: {json.dumps(list(categories.keys()))},
                datasets: [{{
                    data: {json.dumps(list(categories.values()))},
                    backgroundColor: ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#43e97b']
                }}]
            }},
            options: {{ responsive: true, maintainAspectRatio: false }}
        }});

        new Chart(document.getElementById('usageChart'), {{
            type: 'bar',
            data: {{
                labels: {json.dumps([t for t, c in top_terms])},
                datasets: [{{
                    label: '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π',
                    data: {json.dumps([c for t, c in top_terms])},
                    backgroundColor: '#667eea',
                    borderRadius: 8
                }}]
            }},
            options: {{
                responsive: true,
                maintainAspectRatio: false,
                indexAxis: 'y',
                plugins: {{ legend: {{ display: false }} }}
            }}
        }});
    </script>
</body>
</html>"""

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"üé® HTML dashboard: {output_file}")


class GlossaryValidator:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    """

    def __init__(self, glossary_builder):
        self.builder = glossary_builder

    def validate_quality(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π"""
        issues = []

        for term, data in self.builder.glossary.items():
            definition = data['definition']

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
            if len(definition) < 20:
                issues.append({
                    'term': term,
                    'issue': 'too_short',
                    'severity': 'warning',
                    'message': '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ (< 20 —Å–∏–º–≤–æ–ª–æ–≤)'
                })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
            if len(definition) > 500:
                issues.append({
                    'term': term,
                    'issue': 'too_long',
                    'severity': 'info',
                    'message': '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ (> 500 —Å–∏–º–≤–æ–ª–æ–≤)'
                })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ù–µ—Ç —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ
            if not definition.endswith('.'):
                issues.append({
                    'term': term,
                    'issue': 'no_period',
                    'severity': 'warning',
                    'message': '–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π'
                })

        return issues

    def calculate_quality_score(self, term):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–∞"""
        data = self.builder.glossary.get(term, {})
        definition = data.get('definition', '')

        score = 100

        # –®—Ç—Ä–∞—Ñ—ã
        if len(definition) < 20:
            score -= 30
        if len(definition) > 500:
            score -= 10
        if not definition.endswith('.'):
            score -= 5
        if 'category' not in data:
            score -= 10
        if 'synonyms' not in data or not data.get('synonyms'):
            score -= 15

        return max(0, score)


def main():
    parser = argparse.ArgumentParser(
        description='üìñ Advanced Glossary Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥–ª–æ—Å—Å–∞—Ä–∏—è',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  %(prog)s                    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π
  %(prog)s --html             # HTML dashboard
  %(prog)s --validate         # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
  %(prog)s --tfidf            # TF-IDF –∞–Ω–∞–ª–∏–∑
  %(prog)s --all              # –í—Å–µ –æ–ø—Ü–∏–∏
        """
    )

    parser.add_argument('--html', action='store_true',
                       help='üé® –°–æ–∑–¥–∞—Ç—å HTML dashboard —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏')
    parser.add_argument('--json', action='store_true',
                       help='üìÑ –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON')
    parser.add_argument('--csv', action='store_true',
                       help='üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV')
    parser.add_argument('--tooltips', action='store_true',
                       help='üí¨ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML tooltips')
    parser.add_argument('--validate', action='store_true',
                       help='‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π')
    parser.add_argument('--tfidf', action='store_true',
                       help='üìà TF-IDF –∞–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–≤')
    parser.add_argument('--ngrams', type=int, metavar='N',
                       help='üî§ –ò–∑–≤–ª–µ—á—å n-–≥—Ä–∞–º–º—ã (2-3)')
    parser.add_argument('--related', type=str, metavar='TERM',
                       help='üîó –ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã')
    parser.add_argument('--wiki-links', action='store_true',
                       help='üåê –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ Wikipedia')
    parser.add_argument('--all', action='store_true',
                       help='üöÄ –í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –æ–ø—Ü–∏–∏')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π
    builder = AdvancedGlossaryBuilder(root_dir)
    builder.build()
    builder.save_markdown()

    # HTML dashboard
    if args.html or args.all:
        visualizer = GlossaryVisualizer(builder)
        visualizer.create_html_dashboard(root_dir / "glossary_dashboard.html")

    # JSON —ç–∫—Å–ø–æ—Ä—Ç
    if args.json or args.all:
        builder.export_json()

    # CSV —ç–∫—Å–ø–æ—Ä—Ç
    if args.csv:
        csv_file = root_dir / "glossary.csv"
        with open(csv_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['term', 'definition', 'category'])
            writer.writeheader()
            for term, data in builder.glossary.items():
                writer.writerow({
                    'term': term,
                    'definition': data['definition'],
                    'category': data.get('category', 'general')
                })
        print(f"üìä CSV: {csv_file}")

    # Tooltips
    if args.tooltips or args.all:
        builder.generate_tooltips_html()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    if args.validate or args.all:
        validator = GlossaryValidator(builder)
        issues = validator.validate_quality()

        print(f"\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–∞–π–¥–µ–Ω–æ {len(issues)} –ø—Ä–æ–±–ª–µ–º")
        for issue in issues[:10]:
            print(f"   - {issue['term']}: {issue['message']}")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç
        report_file = root_dir / "glossary_validation.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# –í–∞–ª–∏–¥–∞—Ü–∏—è –≥–ª–æ—Å—Å–∞—Ä–∏—è\n\n")
            f.write(f"–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(issues)}\n\n")
            for issue in issues:
                f.write(f"- **{issue['term']}**: {issue['message']}\n")
        print(f"üìã –û—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {report_file}")

    # TF-IDF
    if args.tfidf or args.all:
        extractor = TermExtractor(builder)
        tfidf = extractor.calculate_tf_idf()

        print(f"\nüìà TF-IDF —Ç–æ–ø-20 —Å–ª–æ–≤:")
        for word, score in tfidf[:20]:
            print(f"   - {word}: {score:.4f}")

    # N-–≥—Ä–∞–º–º—ã
    if args.ngrams:
        extractor = TermExtractor(builder)
        ngrams = extractor.extract_ngrams(n=args.ngrams)

        print(f"\nüî§ –¢–æ–ø-20 {args.ngrams}-–≥—Ä–∞–º–º:")
        for ngram, count in ngrams[:20]:
            print(f"   - {ngram}: {count}")

    # –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    if args.related:
        linker = DefinitionLinker(builder)
        related = linker.find_related_terms(args.related)

        print(f"\nüîó –¢–µ—Ä–º–∏–Ω—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å '{args.related}':")
        for term, score in related:
            print(f"   - {term} (—Å—Ö–æ–∂–µ—Å—Ç—å: {score})")

    # Wikipedia —Å—Å—ã–ª–∫–∏
    if args.wiki_links:
        linker = DefinitionLinker(builder)
        links = linker.suggest_wikipedia_links()

        wiki_file = root_dir / "glossary_wikipedia_links.md"
        with open(wiki_file, 'w', encoding='utf-8') as f:
            f.write("# Wikipedia —Å—Å—ã–ª–∫–∏ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤\n\n")
            for term, url in list(links.items())[:50]:
                f.write(f"- [{term}]({url})\n")
        print(f"üåê Wikipedia —Å—Å—ã–ª–∫–∏: {wiki_file}")

    print("\n‚ú® –ì–ª–æ—Å—Å–∞—Ä–∏–π –≥–æ—Ç–æ–≤!")


if __name__ == "__main__":
    main()
