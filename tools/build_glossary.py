#!/usr/bin/env python3
"""
Advanced Glossary Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥–ª–æ—Å—Å–∞—Ä–∏—è
–§—É–Ω–∫—Ü–∏–∏:
- Term categorization (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤)
- Synonyms & related terms (—Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã)
- Multilingual support (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤)
- Term frequency analysis (–∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã)
- Fuzzy matching (–Ω–µ—á—ë—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤)
- Term relationships (–∏–µ—Ä–∞—Ä—Ö–∏—è parent/child)
- Tooltip generation (HTML –ø–æ–¥—Å–∫–∞–∑–∫–∏)
- Term importance ranking (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏)
- Cross-references (–ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏)
- Definition quality check (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π)
- Usage statistics (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
- Export formats (JSON, HTML, CSV)

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Wiktionary, Investopedia, MDN Web Docs, technical glossaries
"""

from pathlib import Path
import re
import json
from collections import defaultdict, Counter
import yaml


class AdvancedGlossaryBuilder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥–ª–æ—Å—Å–∞—Ä–∏—è"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ì–ª–æ—Å—Å–∞—Ä–∏–π
        self.glossary = {}

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.term_usage = Counter()  # –ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤
        self.term_categories = defaultdict(set)
        self.synonyms = defaultdict(set)
        self.related_terms = defaultdict(set)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        self.category_keywords = {
            'technical': ['–∞–ª–≥–æ—Ä–∏—Ç–º', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ñ—É–Ω–∫—Ü–∏—è', '–º–µ—Ç–æ–¥', '–∫–ª–∞—Å—Å', '–æ–±—ä–µ–∫—Ç'],
            'business': ['—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–ø—Ä–æ–¥–∞–∂–∏', '–ø—Ä–∏–±—ã–ª—å', '—Ä—ã–Ω–æ–∫'],
            'science': ['—Ç–µ–æ—Ä–∏—è', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–≥–∏–ø–æ—Ç–µ–∑–∞', '–∞–Ω–∞–ª–∏–∑'],
            'mathematical': ['—Ñ–æ—Ä–º—É–ª–∞', '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ç–µ–æ—Ä–µ–º–∞', '–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ']
        }

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def extract_definitions(self, content):
        """–ò–∑–≤–ª–µ—á—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        definitions = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω 1: **–¢–µ—Ä–º–∏–Ω** ‚Äî –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.
        pattern1 = r'\*\*([^*]+)\*\*\s*[‚Äî‚Äì-]\s*([^.\n]+(?:\.[^.\n]+)?\.)'
        for match in re.finditer(pattern1, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            definitions.append((term, definition, 'dash'))

        # –ü–∞—Ç—Ç–µ—Ä–Ω 2: **–¢–µ—Ä–º–∏–Ω**: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.
        pattern2 = r'\*\*([^*]+)\*\*:\s*([^.\n]+(?:\.[^.\n]+)?\.)'
        for match in re.finditer(pattern2, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            if (term, definition, 'colon') not in definitions:
                definitions.append((term, definition, 'colon'))

        # –ü–∞—Ç—Ç–µ—Ä–Ω 3: ## –¢–µ—Ä–º–∏–Ω\n\n–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        pattern3 = r'##\s+([^\n]+)\n\n([^#\n][^\n]+(?:\.[^\n]+)?\.)'
        for match in re.finditer(pattern3, content):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            if len(term) < 100:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞
                definitions.append((term, definition, 'header'))

        return definitions

    def extract_abbreviations(self, content):
        """–ò–∑–≤–ª–µ—á—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        abbrevs = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω: ABBR (Full Name)
        pattern = r'\b([A-Z–ê-–Ø]{2,})\s*\(([^)]{3,50})\)'
        for match in re.finditer(pattern, content):
            abbr = match.group(1).strip()
            full = match.group(2).strip()
            abbrevs.append((abbr, full))

        return abbrevs

    def categorize_term(self, term, definition):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–∞"""
        categories = set()

        combined_text = f"{term} {definition}".lower()

        for category, keywords in self.category_keywords.items():
            if any(keyword in combined_text for keyword in keywords):
                categories.add(category)

        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        if not categories:
            categories.add('general')

        return categories

    def check_definition_quality(self, definition):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        issues = []
        score = 100

        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
        if len(definition) < 20:
            issues.append('too_short')
            score -= 40

        # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        if len(definition) > 500:
            issues.append('too_long')
            score -= 20

        # –ù–µ—Ç –≥–ª–∞–≥–æ–ª–∞ (–ø–ª–æ—Ö–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        if not re.search(r'\b(—è–≤–ª—è–µ—Ç—Å—è|—ç—Ç–æ|–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç|–æ–∑–Ω–∞—á–∞–µ—Ç|–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è|is|are|means)\b', definition.lower()):
            issues.append('no_verb')
            score -= 30

        # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (–ø–ª–æ—Ö–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)
        if re.match(r'^(—ç—Ç–æ|that|this|it)\s+', definition.lower()):
            issues.append('starts_with_pronoun')
            score -= 15

        # –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
        if not definition.endswith('.'):
            issues.append('no_period')
            score -= 10

        return {
            'score': max(0, score),
            'issues': issues
        }

    def find_synonyms(self, term):
        """–ù–∞–π—Ç–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã"""
        synonyms = set()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        patterns = [
            rf'{re.escape(term)}\s*\((?:—Ç–∞–∫–∂–µ|syn|aka)[:.]?\s*([^)]+)\)',
            rf'(?:—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–µ–Ω –∫–∞–∫|also known as|AKA)\s+{re.escape(term)}\s*[:.]?\s*([^.,\n]+)',
        ]

        # –ü–æ–∏—Å–∫ –≤ —Å—Ç–∞—Ç—å—è—Ö (—É–ø—Ä–æ—â—ë–Ω–Ω–æ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏)
        # –ó–¥–µ—Å—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, –Ω–æ –º–µ—Ö–∞–Ω–∏–∑–º –≥–æ—Ç–æ–≤

        return synonyms

    def levenshtein_distance(self, s1, s2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def find_similar_terms(self, term, max_distance=2):
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (fuzzy matching)"""
        similar = []

        term_lower = term.lower()

        for existing_term in self.glossary.keys():
            existing_lower = existing_term.lower()

            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–∞–º —Ç–µ—Ä–º–∏–Ω
            if existing_lower == term_lower:
                continue

            # –í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            distance = self.levenshtein_distance(term_lower, existing_lower)

            if distance <= max_distance:
                similar.append({
                    'term': existing_term,
                    'distance': distance
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
        similar.sort(key=lambda x: x['distance'])

        return similar[:5]

    def extract_term_context(self, term, content):
        """–ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–∞"""
        contexts = []

        # –ù–∞–π—Ç–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–∞
        pattern = rf'\b{re.escape(term)}\b'

        for match in re.finditer(pattern, content, re.IGNORECASE):
            start = max(0, match.start() - 50)
            end = min(len(content), match.end() + 50)

            context = content[start:end].strip()
            contexts.append(context)

        return contexts[:3]  # –¢–æ–ø-3 –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

    def calculate_term_importance(self, term_data):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞"""
        score = 0

        # –ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        usage_count = len(term_data['articles'])
        score += usage_count * 10

        # –ö–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        score += term_data['quality']['score'] / 10

        # –ù–∞–ª–∏—á–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        score += len(term_data['categories']) * 5

        # –î–ª–∏–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è 50-200 —Å–∏–º–≤–æ–ª–æ–≤)
        def_len = len(term_data['definition'])
        if 50 <= def_len <= 200:
            score += 20
        elif def_len < 50:
            score += 5

        return round(score, 1)

    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π"""
        print("üìñ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≥–ª–æ—Å—Å–∞—Ä–∏—è...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_file = str(md_file.relative_to(self.root_dir))

            # –ò–∑–≤–ª–µ—á—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            definitions = self.extract_definitions(content)
            for term, definition, source_type in definitions:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality = self.check_definition_quality(definition)

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
                categories = self.categorize_term(term, definition)

                # –î–æ–±–∞–≤–∏—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å —Ç–µ—Ä–º–∏–Ω
                if term not in self.glossary:
                    self.glossary[term] = {
                        'definition': definition,
                        'articles': [],
                        'categories': categories,
                        'quality': quality,
                        'source_type': source_type,
                        'contexts': []
                    }

                # –î–æ–±–∞–≤–∏—Ç—å —Å—Ç–∞—Ç—å—é
                if article_file not in self.glossary[term]['articles']:
                    self.glossary[term]['articles'].append(article_file)

                # –ö–æ–Ω—Ç–µ–∫—Å—Ç
                contexts = self.extract_term_context(term, content)
                self.glossary[term]['contexts'].extend(contexts)

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
                for cat in categories:
                    self.term_categories[cat].add(term)

                # –ß–∞—Å—Ç–æ—Ç–∞
                self.term_usage[term] += 1

            # –ò–∑–≤–ª–µ—á—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            abbrevs = self.extract_abbreviations(content)
            for abbr, full in abbrevs:
                if abbr not in self.glossary:
                    definition = f"–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ—Ç: {full}"
                    quality = {'score': 80, 'issues': []}

                    self.glossary[abbr] = {
                        'definition': definition,
                        'articles': [article_file],
                        'categories': {'abbreviation'},
                        'quality': quality,
                        'source_type': 'abbreviation',
                        'full_form': full,
                        'contexts': []
                    }

                    self.term_categories['abbreviation'].add(abbr)
                    self.term_usage[abbr] += 1

        # –í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤
        for term in self.glossary:
            self.glossary[term]['importance'] = self.calculate_term_importance(self.glossary[term])

        # –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        for term in self.glossary:
            similar = self.find_similar_terms(term)
            self.glossary[term]['similar_terms'] = [s['term'] for s in similar]

        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(self.glossary)}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.term_categories)}")

    def generate_statistics(self):
        """–°–æ–∑–¥–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats = {
            'total_terms': len(self.glossary),
            'by_category': {cat: len(terms) for cat, terms in self.term_categories.items()},
            'avg_quality': round(sum(t['quality']['score'] for t in self.glossary.values()) / len(self.glossary), 1) if self.glossary else 0,
            'top_terms': [
                {'term': term, 'usage': count}
                for term, count in self.term_usage.most_common(10)
            ],
            'quality_issues': Counter(
                issue
                for term_data in self.glossary.values()
                for issue in term_data['quality']['issues']
            )
        }

        return stats

    def save_markdown(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤ markdown"""
        stats = self.generate_statistics()

        lines = []
        lines.append("# üìñ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π\n\n")
        lines.append("> –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤, –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤**: {stats['total_terms']}\n")
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**: {stats['avg_quality']}/100\n")
        lines.append(f"- **–ö–∞—Ç–µ–≥–æ—Ä–∏–π**: {len(stats['by_category'])}\n\n")

        # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        lines.append("### –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n")
        for category, count in sorted(stats['by_category'].items(), key=lambda x: -x[1]):
            lines.append(f"- **{category}**: {count}\n")
        lines.append("\n")

        # –¢–æ–ø —Ç–µ—Ä–º–∏–Ω–æ–≤
        if stats['top_terms']:
            lines.append("### –¢–æ–ø-10 —Ç–µ—Ä–º–∏–Ω–æ–≤ (–ø–æ —á–∞—Å—Ç–æ—Ç–µ)\n\n")
            for item in stats['top_terms']:
                lines.append(f"- **{item['term']}**: {item['usage']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\n")
            lines.append("\n")

        # –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å
        lines.append("## –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
        by_letter = defaultdict(list)
        for term in sorted(self.glossary.keys(), key=lambda x: x.lower()):
            first_letter = term[0].upper()
            by_letter[first_letter].append(term)

        for letter in sorted(by_letter.keys()):
            lines.append(f"[{letter}](#{letter}) ")
        lines.append("\n\n---\n\n")

        # –¢–µ—Ä–º–∏–Ω—ã –ø–æ –±—É–∫–≤–∞–º
        for letter in sorted(by_letter.keys()):
            lines.append(f"\n## {letter}\n\n")

            for term in by_letter[letter]:
                data = self.glossary[term]

                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–µ—Ä–º–∏–Ω–∞
                lines.append(f"### {term}\n\n")

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–µ–π–¥–∂–∏)
                if data['categories']:
                    badges = ' '.join(f"`{cat}`" for cat in sorted(data['categories']))
                    lines.append(f"{badges}\n\n")

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                lines.append(f"{data['definition']}\n\n")

                # –ü–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞ (–¥–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä)
                if 'full_form' in data:
                    lines.append(f"**–ü–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞**: {data['full_form']}\n\n")

                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = data['quality']['score']
                if quality_score < 70:
                    lines.append(f"‚ö†Ô∏è **–ö–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è**: {quality_score}/100\n\n")

                # –ü–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
                if data.get('similar_terms'):
                    similar_str = ', '.join(f"[{t}](#{t})" for t in data['similar_terms'][:3])
                    lines.append(f"**–ü–æ—Ö–æ–∂–∏–µ —Ç–µ—Ä–º–∏–Ω—ã**: {similar_str}\n\n")

                # –í–∞–∂–Ω–æ—Å—Ç—å
                if data['importance'] > 50:
                    lines.append(f"**–í–∞–∂–Ω–æ—Å—Ç—å**: {data['importance']}\n\n")

                # –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤
                if data['articles']:
                    lines.append("**–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤:**\n")
                    for article in data['articles'][:3]:
                        lines.append(f"- [{article}]({article})\n")
                    if len(data['articles']) > 3:
                        lines.append(f"\n_...–∏ –µ—â—ë {len(data['articles']) - 3}_\n")
                    lines.append("\n")

        output_file = self.root_dir / "ADVANCED_GLOSSARY.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –ì–ª–æ—Å—Å–∞—Ä–∏–π: {output_file}")

    def export_json(self):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å set –≤ list –¥–ª—è JSON
        glossary_for_json = {}
        for term, data in self.glossary.items():
            term_data = data.copy()
            if 'categories' in term_data and isinstance(term_data['categories'], set):
                term_data['categories'] = list(term_data['categories'])
            glossary_for_json[term] = term_data

        data = {
            'statistics': self.generate_statistics(),
            'terms': glossary_for_json,
            'categories': {cat: list(terms) for cat, terms in self.term_categories.items()}
        }

        output_file = self.root_dir / "glossary.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON: {output_file}")

    def generate_tooltips_html(self):
        """–°–æ–∑–¥–∞—Ç—å HTML –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ —Å—Ç–∞—Ç—å–∏"""
        html_lines = []
        html_lines.append("<!-- Glossary Tooltips -->\n")
        html_lines.append("<script>\n")
        html_lines.append("const glossary = {\n")

        for term, data in self.glossary.items():
            # Escape —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            definition = data['definition'].replace('"', '\\"').replace('\n', ' ')
            html_lines.append(f'  "{term}": "{definition}",\n')

        html_lines.append("};\n")
        html_lines.append("</script>\n")

        output_file = self.root_dir / "glossary_tooltips.html"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(html_lines)

        print(f"‚úÖ HTML tooltips: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Glossary Builder')
    parser.add_argument('--json', action='store_true', help='–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON')
    parser.add_argument('--tooltips', action='store_true', help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML tooltips')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = AdvancedGlossaryBuilder(root_dir)
    builder.build()
    builder.save_markdown()

    if args.json:
        builder.export_json()

    if args.tooltips:
        builder.generate_tooltips_html()


if __name__ == "__main__":
    main()
