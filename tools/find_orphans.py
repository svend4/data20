#!/usr/bin/env python3
"""
Advanced Orphan Finder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç
–§—É–Ω–∫—Ü–∏–∏:
- Orphan classification (–Ω–æ–≤—ã–µ, —Å—Ç–∞—Ä—ã–µ, –∫—Ä–∏—Ç–∏—á–Ω—ã–µ)
- Fix suggestions (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≥–¥–µ –¥–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏)
- Severity levels (high, medium, low)
- Integration candidates (—Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å—Å—ã–ª–∞—Ç—å—Å—è)
- Orphan age detection
- Link density analysis
- Category-based analysis
- Automatic fix generation
- JSON export
- Graph visualization data

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Wikipedia orphan detection, SEO tools, Content auditing tools
"""

from pathlib import Path
import re
import yaml
from datetime import datetime, timedelta
import json
from collections import defaultdict


class AdvancedOrphanFinder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.all_articles = {}  # path -> metadata
        self.incoming_links = defaultdict(set)  # target -> sources
        self.outgoing_links = defaultdict(set)  # source -> targets

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def analyze_article(self, file_path):
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å—é"""
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        article_path = str(file_path.relative_to(self.root_dir))

        # –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è/–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        mtime = file_path.stat().st_mtime
        modified_date = datetime.fromtimestamp(mtime)
        age_days = (datetime.now() - modified_date).days

        # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_length = len(content) if content else 0

        # –¢–µ–≥–∏/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        tags = []
        category = None
        if frontmatter:
            tags = frontmatter.get('tags', [])
            category = frontmatter.get('category', None)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.all_articles[article_path] = {
            'path': article_path,
            'frontmatter': frontmatter,
            'content_length': content_length,
            'modified_date': modified_date,
            'age_days': age_days,
            'tags': tags,
            'category': category,
            'file_path': file_path
        }

        return content

    def build_link_graph(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å—Å—ã–ª–æ–∫"""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π –∏ —Å—Å—ã–ª–æ–∫...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            # –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å—é
            content = self.analyze_article(md_file)
            article_path = str(md_file.relative_to(self.root_dir))

            if not content:
                continue

            # –ù–∞–π—Ç–∏ –≤—Å–µ markdown —Å—Å—ã–ª–∫–∏
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
            for text, link in links:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ —è–∫–æ—Ä—è
                if link.startswith('http') or link.startswith('#'):
                    continue

                # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                try:
                    target = (md_file.parent / link.split('#')[0]).resolve()
                    target_path = str(target.relative_to(self.root_dir))

                    # –ó–∞–ø–∏—Å–∞—Ç—å —Å–≤—è–∑—å
                    self.outgoing_links[article_path].add(target_path)
                    self.incoming_links[target_path].add(article_path)
                except:
                    pass

            # –°—Å—ã–ª–∫–∏ –∏–∑ frontmatter (related)
            frontmatter = self.all_articles[article_path]['frontmatter']
            if frontmatter and 'related' in frontmatter:
                related = frontmatter['related']
                if isinstance(related, list):
                    for link in related:
                        try:
                            target = (md_file.parent / link).resolve()
                            target_path = str(target.relative_to(self.root_dir))

                            self.outgoing_links[article_path].add(target_path)
                            self.incoming_links[target_path].add(article_path)
                        except:
                            pass

        print(f"   –°—Ç–∞—Ç–µ–π: {len(self.all_articles)}")
        print(f"   –°–≤—è–∑–µ–π: {sum(len(v) for v in self.outgoing_links.values())}\n")

    def classify_orphan(self, article_path):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ä–æ—Ç—É"""
        metadata = self.all_articles[article_path]

        age_days = metadata['age_days']
        content_length = metadata['content_length']
        outgoing_links_count = len(self.outgoing_links.get(article_path, set()))

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        classification = {
            'type': 'unknown',
            'severity': 'medium',
            'reason': []
        }

        # –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è (< 7 –¥–Ω–µ–π)
        if age_days < 7:
            classification['type'] = 'new'
            classification['severity'] = 'low'
            classification['reason'].append('–ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è, –≤–æ–∑–º–æ–∂–Ω–æ –µ—â–µ –Ω–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞')

        # –°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è (> 90 –¥–Ω–µ–π) –±–µ–∑ —Å—Å—ã–ª–æ–∫
        elif age_days > 90:
            classification['type'] = 'old_orphan'
            classification['severity'] = 'high'
            classification['reason'].append('–°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è –±–µ–∑ —Å—Å—ã–ª–æ–∫ - —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è')

        # –ö–æ—Ä–æ—Ç–∫–∞—è —Å—Ç–∞—Ç—å—è
        if content_length < 500:
            classification['type'] = 'stub'
            classification['severity'] = 'low'
            classification['reason'].append('–ö–æ—Ä–æ—Ç–∫–∞—è —Å—Ç–∞—Ç—å—è (stub)')

        # –°—Ç–∞—Ç—å—è —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞—Ä—É–∂—É –Ω–æ –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö
        if outgoing_links_count > 0:
            classification['type'] = 'isolated'
            classification['severity'] = 'medium'
            classification['reason'].append(f'–°—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ {outgoing_links_count} —Å—Ç–∞—Ç–µ–π, –Ω–æ –Ω–∞ –Ω–µ–µ –Ω–∏–∫—Ç–æ –Ω–µ —Å—Å—ã–ª–∞–µ—Ç—Å—è')

        # –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è
        if outgoing_links_count == 0:
            classification['type'] = 'completely_isolated'
            classification['severity'] = 'high'
            classification['reason'].append('–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–∞ (–Ω–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∏ –≤ –æ–¥–Ω—É —Å—Ç–æ—Ä–æ–Ω—É)')

        return classification

    def find_integration_candidates(self, orphan_path, max_candidates=5):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å–∏—Ä–æ—Ç—É"""
        orphan_metadata = self.all_articles[orphan_path]
        candidates = []

        orphan_tags = set(orphan_metadata.get('tags', []))
        orphan_category = orphan_metadata.get('category')

        for article_path, metadata in self.all_articles.items():
            if article_path == orphan_path:
                continue

            score = 0
            reasons = []

            # –û–±—â–∏–µ —Ç–µ–≥–∏
            article_tags = set(metadata.get('tags', []))
            common_tags = orphan_tags & article_tags
            if common_tags:
                score += len(common_tags) * 2
                reasons.append(f"–û–±—â–∏–µ —Ç–µ–≥–∏: {', '.join(common_tags)}")

            # –¢–∞ –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            if metadata.get('category') == orphan_category and orphan_category:
                score += 3
                reasons.append(f"–¢–∞ –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {orphan_category}")

            # –°–∏—Ä–æ—Ç–∞ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç—É —Å—Ç–∞—Ç—å—é
            if article_path in self.outgoing_links.get(orphan_path, set()):
                score += 5
                reasons.append("–°–∏—Ä–æ—Ç–∞ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç—É —Å—Ç–∞—Ç—å—é (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤–∑–∞–∏–º–Ω—É—é —Å—Å—ã–ª–∫—É)")

            # –≠—Ç–∞ —Å—Ç–∞—Ç—å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if Path(article_path).parent == Path(orphan_path).parent:
                score += 1
                reasons.append("–í —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")

            # –ú–Ω–æ–≥–æ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ (hub)
            outgoing_count = len(self.outgoing_links.get(article_path, set()))
            if outgoing_count > 5:
                score += 1
                reasons.append(f"Hub-—Å—Ç–∞—Ç—å—è ({outgoing_count} —Å—Å—ã–ª–æ–∫)")

            if score > 0:
                candidates.append({
                    'path': article_path,
                    'score': score,
                    'reasons': reasons
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ score
        candidates.sort(key=lambda x: -x['score'])

        return candidates[:max_candidates]

    def generate_fix_suggestion(self, orphan_path, candidate):
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é"""
        orphan_name = Path(orphan_path).stem
        candidate_name = Path(candidate['path']).stem

        suggestion = f"–í —Ñ–∞–π–ª–µ `{candidate['path']}` –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ `{orphan_path}`:\n"
        suggestion += f"```markdown\n"
        suggestion += f"[{orphan_name}]({Path(orphan_path).name})\n"
        suggestion += f"```\n"
        suggestion += f"–ü—Ä–∏—á–∏–Ω–∞: {', '.join(candidate['reasons'])}"

        return suggestion

    def find_orphans(self):
        """–ù–∞–π—Ç–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ä–æ—Ç"""
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ
        self.build_link_graph()

        print("üîç –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç...\n")

        # –ù–∞–π—Ç–∏ —Å–∏—Ä–æ—Ç—ã
        orphans = []

        for article_path in self.all_articles.keys():
            incoming_count = len(self.incoming_links.get(article_path, set()))

            if incoming_count == 0:
                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
                classification = self.classify_orphan(article_path)

                # –ù–∞–π—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
                candidates = self.find_integration_candidates(article_path)

                orphan_data = {
                    'path': article_path,
                    'metadata': self.all_articles[article_path],
                    'classification': classification,
                    'candidates': candidates
                }

                orphans.append(orphan_data)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = len(self.all_articles)
        linked = total - len(orphans)

        print(f"   –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {total}")
        print(f"   –°–æ —Å—Å—ã–ª–∫–∞–º–∏: {linked}")
        print(f"   –°–∏—Ä–æ—Ç—ã: {len(orphans)}\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        if orphans:
            types = defaultdict(int)
            for orphan in orphans:
                types[orphan['classification']['type']] += 1

            print("   –¢–∏–ø—ã —Å–∏—Ä–æ—Ç:")
            for orphan_type, count in sorted(types.items(), key=lambda x: -x[1]):
                print(f"      {orphan_type}: {count}")
            print()

        return orphans

    def generate_report(self, orphans):
        """–°–æ–∑–¥–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîç –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ—Ç—á—ë—Ç: –°—Ç–∞—Ç—å–∏-—Å–∏—Ä–æ—Ç—ã\n\n")
        lines.append("> –°—Ç–∞—Ç—å–∏ –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏\n\n")

        lines.append(f"**–ù–∞–π–¥–µ–Ω–æ —Å–∏—Ä–æ—Ç**: {len(orphans)}\n\n")

        if not orphans:
            lines.append("‚úÖ –ù–µ—Ç —Å—Ç–∞—Ç–µ–π-—Å–∏—Ä–æ—Ç! –í—Å–µ —Å—Ç–∞—Ç—å–∏ —Å–≤—è–∑–∞–Ω—ã.\n")
        else:
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ severity
            by_severity = defaultdict(list)
            for orphan in orphans:
                severity = orphan['classification']['severity']
                by_severity[severity].append(orphan)

            # High severity
            if 'high' in by_severity:
                lines.append("## üî¥ –ö—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–∏—Ä–æ—Ç—ã (High Severity)\n\n")
                lines.append("–¢—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n\n")

                for orphan in by_severity['high']:
                    self._add_orphan_section(lines, orphan)

            # Medium severity
            if 'medium' in by_severity:
                lines.append("\n## üü° –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (Medium Severity)\n\n")

                for orphan in by_severity['medium']:
                    self._add_orphan_section(lines, orphan)

            # Low severity
            if 'low' in by_severity:
                lines.append("\n## üü¢ –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (Low Severity)\n\n")

                for orphan in by_severity['low']:
                    self._add_orphan_section(lines, orphan)

        output_file = self.root_dir / "ORPHANED_ARTICLES_ADVANCED.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def _add_orphan_section(self, lines, orphan):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é –¥–ª—è –æ–¥–Ω–æ–π —Å–∏—Ä–æ—Ç—ã"""
        path = orphan['path']
        classification = orphan['classification']
        metadata = orphan['metadata']
        candidates = orphan['candidates']

        lines.append(f"### {Path(path).stem}\n\n")
        lines.append(f"- **–ü—É—Ç—å**: `{path}`\n")
        lines.append(f"- **–¢–∏–ø**: {classification['type']}\n")
        lines.append(f"- **Severity**: {classification['severity']}\n")
        lines.append(f"- **–í–æ–∑—Ä–∞—Å—Ç**: {metadata['age_days']} –¥–Ω–µ–π\n")
        lines.append(f"- **–†–∞–∑–º–µ—Ä**: {metadata['content_length']} —Å–∏–º–≤–æ–ª–æ–≤\n")

        if classification['reason']:
            lines.append(f"- **–ü—Ä–∏—á–∏–Ω—ã**:\n")
            for reason in classification['reason']:
                lines.append(f"  - {reason}\n")

        # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        if candidates:
            lines.append(f"\n**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏** (—Ç–æ–ø-{len(candidates)}):\n\n")

            for i, candidate in enumerate(candidates, 1):
                lines.append(f"{i}. **{Path(candidate['path']).stem}** (score: {candidate['score']})\n")
                lines.append(f"   - –§–∞–π–ª: `{candidate['path']}`\n")
                for reason in candidate['reasons']:
                    lines.append(f"   - {reason}\n")
                lines.append("\n")

        lines.append("\n---\n\n")

    def export_json(self, orphans):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'total_articles': len(self.all_articles),
            'total_orphans': len(orphans),
            'orphans': []
        }

        for orphan in orphans:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å datetime –≤ string
            metadata = orphan['metadata'].copy()
            metadata['modified_date'] = metadata['modified_date'].isoformat()
            metadata.pop('file_path', None)  # –£–¥–∞–ª–∏—Ç—å Path object

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å frontmatter (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å date –æ–±—ä–µ–∫—Ç—ã)
            frontmatter = metadata.get('frontmatter')
            if frontmatter:
                frontmatter_clean = {}
                for key, value in frontmatter.items():
                    if hasattr(value, 'isoformat'):  # datetime –∏–ª–∏ date
                        frontmatter_clean[key] = value.isoformat()
                    else:
                        frontmatter_clean[key] = value
                metadata['frontmatter'] = frontmatter_clean

            data['orphans'].append({
                'path': orphan['path'],
                'metadata': metadata,
                'classification': orphan['classification'],
                'candidates': orphan['candidates']
            })

        output_file = self.root_dir / "orphans_analysis.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Orphan Finder')
    parser.add_argument('--json', action='store_true',
                       help='–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    finder = AdvancedOrphanFinder(root_dir)
    orphans = finder.find_orphans()
    finder.generate_report(orphans)

    if args.json:
        finder.export_json(orphans)


if __name__ == "__main__":
    main()
