#!/usr/bin/env python3
"""
Archive Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∞—Ä—Ö–∏–≤–æ–≤
–°–æ–∑–¥–∞—ë—Ç –∞—Ä—Ö–∏–≤—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –±—ç–∫–∞–ø—ã
- –†–æ—Ç–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤
- –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (MD5/SHA256)
- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ manifest
- –ü—Ä–æ–≥—Ä–µ—Å—Å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –ì–∏–±–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –≤–∫–ª—é—á–µ–Ω–∏—è/–∏—Å–∫–ª—é—á–µ–Ω–∏—è

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: tar, rsync, Time Machine, Duplicity
"""

from pathlib import Path
import zipfile
import tarfile
from datetime import datetime
import json
import hashlib
import shutil
import re
import argparse
from typing import Dict, List, Optional
from collections import defaultdict


class IncrementalArchiver:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞—Ä—Ö–∏–≤–∞—Ç–æ—Ä
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ–ª–Ω—ã—Ö, –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤
    """

    def __init__(self, root_dir, backup_dir):
        self.root_dir = Path(root_dir)
        self.backup_dir = Path(backup_dir)
        self.snapshot_db = self.backup_dir / "snapshots.json"
        self.snapshots = self._load_snapshots()

    def _load_snapshots(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É —Å–Ω–∏–º–∫–æ–≤ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if self.snapshot_db.exists():
            with open(self.snapshot_db, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'full': [], 'incremental': [], 'differential': []}

    def _save_snapshots(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞–∑—É —Å–Ω–∏–º–∫–æ–≤"""
        with open(self.snapshot_db, 'w', encoding='utf-8') as f:
            json.dump(self.snapshots, f, ensure_ascii=False, indent=2)

    def create_snapshot(self, files: List[Path], backup_type: str, archive_path: Path) -> Dict:
        """
        –°–æ–∑–¥–∞—Ç—å —Å–Ω–∏–º–æ–∫ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ñ–∞–π–ª–æ–≤

        Args:
            files: —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
            backup_type: 'full', 'incremental', 'differential'
            archive_path: –ø—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É

        Returns:
            snapshot dictionary
        """
        snapshot = {
            'timestamp': datetime.now().isoformat(),
            'type': backup_type,
            'archive': str(archive_path.name),
            'files': {},
            'total_size': 0
        }

        for file_path in files:
            stat = file_path.stat()
            rel_path = str(file_path.relative_to(self.root_dir))

            snapshot['files'][rel_path] = {
                'size': stat.st_size,
                'mtime': stat.st_mtime,
                'md5': self._quick_hash(file_path)
            }

            snapshot['total_size'] += stat.st_size

        # –î–æ–±–∞–≤–∏—Ç—å –≤ –±–∞–∑—É
        self.snapshots[backup_type].append(snapshot)
        self._save_snapshots()

        return snapshot

    def _quick_hash(self, file_path: Path) -> str:
        """–ë—ã—Å—Ç—Ä—ã–π —Ö–µ—à —Ñ–∞–π–ª–∞ (–ø–µ—Ä–≤—ã–µ 8KB + —Ä–∞–∑–º–µ—Ä)"""
        hash_md5 = hashlib.md5()

        with open(file_path, 'rb') as f:
            # –ß–∏—Ç–∞—Ç—å –ø–µ—Ä–≤—ã–µ 8KB
            chunk = f.read(8192)
            hash_md5.update(chunk)

        # –î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        hash_md5.update(str(file_path.stat().st_size).encode())

        return hash_md5.hexdigest()

    def get_differential_files(self, all_files: List[Path]) -> List[Path]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞
        (–≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ü–û–õ–ù–û–ì–û –±—ç–∫–∞–ø–∞)
        """
        if not self.snapshots['full']:
            return all_files

        last_full = self.snapshots['full'][-1]
        last_full_files = last_full['files']

        changed = []

        for file_path in all_files:
            rel_path = str(file_path.relative_to(self.root_dir))

            # –ù–æ–≤—ã–π —Ñ–∞–π–ª –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ full backup
            if rel_path not in last_full_files:
                changed.append(file_path)
            else:
                current_hash = self._quick_hash(file_path)
                if current_hash != last_full_files[rel_path]['md5']:
                    changed.append(file_path)

        return changed

    def get_incremental_files(self, all_files: List[Path]) -> List[Path]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞
        (—Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –ü–û–°–õ–ï–î–ù–ï–ì–û –±—ç–∫–∞–ø–∞ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞)
        """
        # –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π snapshot –ª—é–±–æ–≥–æ —Ç–∏–ø–∞
        all_snapshots = []

        for backup_type in ['full', 'incremental', 'differential']:
            all_snapshots.extend(self.snapshots[backup_type])

        if not all_snapshots:
            return all_files

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_snapshots.sort(key=lambda x: x['timestamp'], reverse=True)
        last_snapshot = all_snapshots[0]
        last_files = last_snapshot['files']

        changed = []

        for file_path in all_files:
            rel_path = str(file_path.relative_to(self.root_dir))

            if rel_path not in last_files:
                changed.append(file_path)
            else:
                current_hash = self._quick_hash(file_path)
                if current_hash != last_files[rel_path]['md5']:
                    changed.append(file_path)

        return changed

    def get_backup_chain(self, backup_type: str = 'full') -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ü–µ–ø–æ—á–∫—É –±—ç–∫–∞–ø–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
        if backup_type == 'full':
            # –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π full backup
            return [self.snapshots['full'][-1]] if self.snapshots['full'] else []

        elif backup_type == 'differential':
            # Last full + last differential
            chain = []
            if self.snapshots['full']:
                chain.append(self.snapshots['full'][-1])
            if self.snapshots['differential']:
                chain.append(self.snapshots['differential'][-1])
            return chain

        elif backup_type == 'incremental':
            # Last full + all incrementals since then
            chain = []

            if not self.snapshots['full']:
                return []

            last_full = self.snapshots['full'][-1]
            last_full_time = last_full['timestamp']

            chain.append(last_full)

            # –î–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Å –º–æ–º–µ–Ω—Ç–∞ last full
            for inc in self.snapshots['incremental']:
                if inc['timestamp'] > last_full_time:
                    chain.append(inc)

            return chain

        return []


class CompressionOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å–∂–∞—Ç–∏—è
    –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Å–∂–∞—Ç–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
    """

    def __init__(self):
        # –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —Å–∂–∞—Ç–∏–µ
        self.compression_map = {
            'already_compressed': {
                'extensions': ['.jpg', '.jpeg', '.png', '.gif', '.mp4', '.zip', '.gz', '.pdf'],
                'level': zipfile.ZIP_STORED  # No compression
            },
            'text': {
                'extensions': ['.md', '.txt', '.json', '.yaml', '.yml', '.xml', '.html', '.css', '.js'],
                'level': zipfile.ZIP_DEFLATED  # Best compression
            },
            'code': {
                'extensions': ['.py', '.java', '.cpp', '.c', '.h', '.rb', '.go', '.rs'],
                'level': zipfile.ZIP_DEFLATED
            }
        }

    def get_optimal_compression(self, file_path: Path) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è –¥–ª—è —Ñ–∞–π–ª–∞"""
        ext = file_path.suffix.lower()

        for category, info in self.compression_map.items():
            if ext in info['extensions']:
                return info['level']

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Å–∂–∏–º–∞—Ç—å
        return zipfile.ZIP_DEFLATED

    def analyze_compression_efficiency(self, files: List[Path]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è –¥–ª—è –Ω–∞–±–æ—Ä–∞ —Ñ–∞–π–ª–æ–≤"""
        categories = defaultdict(lambda: {'count': 0, 'size': 0})

        for file_path in files:
            ext = file_path.suffix.lower()
            size = file_path.stat().st_size

            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category = 'other'
            for cat, info in self.compression_map.items():
                if ext in info['extensions']:
                    category = cat
                    break

            categories[category]['count'] += 1
            categories[category]['size'] += size

        return dict(categories)

    def estimate_compression_ratio(self, files: List[Path]) -> float:
        """
        –û—Ü–µ–Ω–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—É—é —Å—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è

        Returns:
            Expected compression ratio (e.g., 2.0 = 50% reduction)
        """
        analysis = self.analyze_compression_efficiency(files)

        # –¢–∏–ø–∏—á–Ω—ã–µ —Å—Ç–µ–ø–µ–Ω–∏ —Å–∂–∞—Ç–∏—è –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        typical_ratios = {
            'already_compressed': 1.0,  # –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            'text': 3.0,                # –¢–µ–∫—Å—Ç —Å–∂–∏–º–∞–µ—Ç—Å—è —Ö–æ—Ä–æ—à–æ
            'code': 2.5,                # –ö–æ–¥ —Å–∂–∏–º–∞–µ—Ç—Å—è —Ö–æ—Ä–æ—à–æ
            'other': 2.0                # –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
        }

        total_size = sum(cat['size'] for cat in analysis.values())

        if total_size == 0:
            return 1.0

        weighted_ratio = 0

        for category, stats in analysis.items():
            weight = stats['size'] / total_size
            ratio = typical_ratios.get(category, 2.0)
            weighted_ratio += weight * ratio

        return weighted_ratio


class ArchiveValidator:
    """
    –í–∞–ª–∏–¥–∞—Ç–æ—Ä –∞—Ä—Ö–∏–≤–æ–≤
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤
    """

    def __init__(self):
        self.validation_results = []

    def validate_zip(self, archive_path: Path) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è ZIP –∞—Ä—Ö–∏–≤–∞"""
        result = {
            'archive': str(archive_path),
            'valid': False,
            'errors': [],
            'file_count': 0,
            'corrupted_files': []
        }

        try:
            with zipfile.ZipFile(archive_path, 'r') as zipf:
                # –¢–µ—Å—Ç –∞—Ä—Ö–∏–≤–∞
                bad_file = zipf.testzip()

                if bad_file:
                    result['errors'].append(f"Corrupted file: {bad_file}")
                    result['corrupted_files'].append(bad_file)
                else:
                    result['valid'] = True

                # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã
                result['file_count'] = len(zipf.namelist())

                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å manifest
                if 'manifest.json' in zipf.namelist():
                    manifest_data = zipf.read('manifest.json')
                    manifest = json.loads(manifest_data)
                    result['manifest'] = manifest
                else:
                    result['errors'].append("Missing manifest.json")

        except Exception as e:
            result['errors'].append(str(e))

        self.validation_results.append(result)

        return result

    def validate_tarball(self, archive_path: Path) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è TAR/TAR.GZ –∞—Ä—Ö–∏–≤–∞"""
        result = {
            'archive': str(archive_path),
            'valid': False,
            'errors': [],
            'file_count': 0
        }

        try:
            with tarfile.open(archive_path, 'r:*') as tarf:
                members = tarf.getmembers()
                result['file_count'] = len(members)

                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤—Å–µ —á–ª–µ–Ω—ã
                for member in members:
                    try:
                        if member.isfile():
                            tarf.extractfile(member)
                    except Exception as e:
                        result['errors'].append(f"Error reading {member.name}: {e}")

                if not result['errors']:
                    result['valid'] = True

        except Exception as e:
            result['errors'].append(str(e))

        self.validation_results.append(result)

        return result

    def batch_validate(self, backup_dir: Path) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        archives = list(backup_dir.glob("*.zip")) + list(backup_dir.glob("*.tar.gz"))

        summary = {
            'total': len(archives),
            'valid': 0,
            'invalid': 0,
            'details': []
        }

        for archive in archives:
            if archive.suffix == '.zip':
                result = self.validate_zip(archive)
            elif archive.name.endswith('.tar.gz'):
                result = self.validate_tarball(archive)
            else:
                continue

            summary['details'].append(result)

            if result['valid']:
                summary['valid'] += 1
            else:
                summary['invalid'] += 1

        return summary

    def verify_hash(self, archive_path: Path, expected_hash: str, algorithm: str = 'md5') -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ö–µ—à –∞—Ä—Ö–∏–≤–∞"""
        hash_func = hashlib.md5() if algorithm == 'md5' else hashlib.sha256()

        with open(archive_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hash_func.update(chunk)

        actual_hash = hash_func.hexdigest()

        return actual_hash == expected_hash


class TimelineBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å timeline –≤–µ—Ä—Å–∏–π
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –±—ç–∫–∞–ø–æ–≤, –∞–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏
    """

    def __init__(self, backup_dir: Path):
        self.backup_dir = backup_dir

    def load_all_manifests(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ manifests –∏–∑ backup –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        manifests = []

        for manifest_path in self.backup_dir.glob("*.manifest.json"):
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest = json.load(f)
                manifest['archive_file'] = manifest_path.stem.replace('.manifest', '')
                manifests.append(manifest)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        manifests.sort(key=lambda x: x['timestamp'])

        return manifests

    def build_timeline(self) -> Dict:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å timeline –±—ç–∫–∞–ø–æ–≤"""
        manifests = self.load_all_manifests()

        timeline = {
            'start_date': manifests[0]['timestamp'] if manifests else None,
            'end_date': manifests[-1]['timestamp'] if manifests else None,
            'total_backups': len(manifests),
            'events': []
        }

        for manifest in manifests:
            event = {
                'timestamp': manifest['timestamp'],
                'archive': manifest.get('archive_file', 'unknown'),
                'file_count': manifest.get('total_files', 0),
                'total_size': manifest.get('total_size', 0),
                'type': self._detect_backup_type(manifest.get('archive_file', ''))
            }

            timeline['events'].append(event)

        return timeline

    def _detect_backup_type(self, archive_name: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –±—ç–∫–∞–ø–∞ –ø–æ –∏–º–µ–Ω–∏ –∞—Ä—Ö–∏–≤–∞"""
        if '_full' in archive_name:
            return 'full'
        elif '_incremental' in archive_name:
            return 'incremental'
        elif '_differential' in archive_name:
            return 'differential'
        return 'unknown'

    def analyze_growth_rate(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
        manifests = self.load_all_manifests()

        if len(manifests) < 2:
            return {'error': 'Not enough backups for analysis'}

        sizes = [m.get('total_size', 0) for m in manifests]
        timestamps = [datetime.fromisoformat(m['timestamp']) for m in manifests]

        # –í—ã—á–∏—Å–ª–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –±—ç–∫–∞–ø–∞–º–∏
        changes = []

        for i in range(1, len(sizes)):
            delta_size = sizes[i] - sizes[i-1]
            delta_time = (timestamps[i] - timestamps[i-1]).total_seconds() / 86400  # days

            if delta_time > 0:
                growth_rate = delta_size / delta_time  # bytes per day

                changes.append({
                    'from': manifests[i-1]['timestamp'],
                    'to': manifests[i]['timestamp'],
                    'delta_bytes': delta_size,
                    'delta_days': delta_time,
                    'growth_rate_per_day': growth_rate
                })

        avg_growth = sum(c['growth_rate_per_day'] for c in changes) / len(changes) if changes else 0

        return {
            'total_backups': len(manifests),
            'initial_size': sizes[0],
            'current_size': sizes[-1],
            'total_growth': sizes[-1] - sizes[0],
            'average_growth_per_day': avg_growth,
            'changes': changes
        }

    def generate_html_timeline(self, output_file: Path):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML timeline –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é"""
        timeline = self.build_timeline()
        growth = self.analyze_growth_rate()

        html = """<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Backup Timeline</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        h1 {
            color: #667eea;
            margin-bottom: 30px;
            font-size: 2.5em;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
        }
        .stat-card h3 {
            font-size: 0.9em;
            opacity: 0.9;
            margin-bottom: 8px;
        }
        .stat-card .value {
            font-size: 2em;
            font-weight: bold;
        }
        .timeline {
            position: relative;
            padding-left: 30px;
        }
        .timeline::before {
            content: '';
            position: absolute;
            left: 10px;
            top: 0;
            bottom: 0;
            width: 2px;
            background: #667eea;
        }
        .event {
            position: relative;
            margin-bottom: 30px;
            padding-left: 30px;
        }
        .event::before {
            content: '';
            position: absolute;
            left: -24px;
            top: 5px;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #667eea;
            border: 3px solid white;
            box-shadow: 0 0 0 2px #667eea;
        }
        .event.full::before { background: #28a745; box-shadow: 0 0 0 2px #28a745; }
        .event.incremental::before { background: #ffc107; box-shadow: 0 0 0 2px #ffc107; }
        .event.differential::before { background: #17a2b8; box-shadow: 0 0 0 2px #17a2b8; }
        .event h3 {
            color: #333;
            margin-bottom: 5px;
        }
        .event .details {
            font-size: 0.9em;
            color: #666;
        }
        .badge {
            display: inline-block;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 0.75em;
            font-weight: bold;
            margin-right: 5px;
        }
        .badge.full { background: #28a745; color: white; }
        .badge.incremental { background: #ffc107; color: #333; }
        .badge.differential { background: #17a2b8; color: white; }
    </style>
</head>
<body>
    <div class="container">
        <h1>üìÖ Backup Timeline</h1>

        <div class="stats">
            <div class="stat-card">
                <h3>Total Backups</h3>
                <div class="value">""" + str(timeline['total_backups']) + """</div>
            </div>
            <div class="stat-card">
                <h3>Current Size</h3>
                <div class="value">""" + f"{growth.get('current_size', 0) / (1024*1024):.1f} MB" + """</div>
            </div>
            <div class="stat-card">
                <h3>Total Growth</h3>
                <div class="value">""" + f"{growth.get('total_growth', 0) / (1024*1024):.1f} MB" + """</div>
            </div>
            <div class="stat-card">
                <h3>Avg Growth/Day</h3>
                <div class="value">""" + f"{growth.get('average_growth_per_day', 0) / 1024:.1f} KB" + """</div>
            </div>
        </div>

        <div class="timeline">
"""

        for event in timeline['events']:
            size_mb = event['total_size'] / (1024 * 1024)
            timestamp = datetime.fromisoformat(event['timestamp']).strftime('%Y-%m-%d %H:%M')

            html += f"""            <div class="event {event['type']}">
                <h3>
                    <span class="badge {event['type']}">{event['type'].upper()}</span>
                    {timestamp}
                </h3>
                <div class="details">
                    {event['archive']}<br>
                    Files: {event['file_count']} | Size: {size_mb:.2f} MB
                </div>
            </div>
"""

        html += """        </div>
    </div>
</body>
</html>"""

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)


class AdvancedArchiveBuilder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∞—Ä—Ö–∏–≤–æ–≤"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.backups_dir = self.root_dir / "backups"
        self.backups_dir.mkdir(exist_ok=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–æ—Ç–∞—Ü–∏–∏
        self.max_backups = 10  # –ú–∞–∫—Å–∏–º—É–º –∞—Ä—Ö–∏–≤–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        self.exclude_patterns = [
            r'\.git',
            r'__pycache__',
            r'\.pyc$',
            r'\.DS_Store',
            r'Thumbs\.db'
        ]

    def calculate_file_hash(self, file_path, algorithm='md5'):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à —Ñ–∞–π–ª–∞"""
        hash_func = hashlib.md5() if algorithm == 'md5' else hashlib.sha256()

        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hash_func.update(chunk)

        return hash_func.hexdigest()

    def should_exclude(self, file_path):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å —Ñ–∞–π–ª"""
        file_str = str(file_path)

        for pattern in self.exclude_patterns:
            if re.search(pattern, file_str):
                return True

        return False

    def collect_files(self):
        """–°–æ–±—Ä–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏"""
        files = []

        for file_path in self.knowledge_dir.rglob("*"):
            if file_path.is_file() and not self.should_exclude(file_path):
                files.append(file_path)

        return files

    def get_file_metadata(self, file_path):
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞"""
        stat = file_path.stat()

        return {
            'path': str(file_path.relative_to(self.root_dir)),
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
            'md5': self.calculate_file_hash(file_path, 'md5')
        }

    def create_manifest(self, files):
        """–°–æ–∑–¥–∞—Ç—å manifest —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        manifest = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(files),
            'total_size': sum(f.stat().st_size for f in files),
            'files': []
        }

        print(f"   –°–æ–∑–¥–∞–Ω–∏–µ manifest –¥–ª—è {len(files)} —Ñ–∞–π–ª–æ–≤...")

        for file_path in files:
            manifest['files'].append(self.get_file_metadata(file_path))

        return manifest

    def load_previous_manifest(self, archive_type='zip'):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π manifest –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞"""
        # –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞—Ä—Ö–∏–≤
        pattern = f"knowledge_backup_*_full.{archive_type}"
        archives = sorted(self.backups_dir.glob(pattern))

        if not archives:
            return None

        last_archive = archives[-1]
        manifest_path = last_archive.with_suffix('.manifest.json')

        if manifest_path.exists():
            with open(manifest_path, 'r', encoding='utf-8') as f:
                return json.load(f)

        return None

    def get_changed_files(self, files):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞"""
        previous_manifest = self.load_previous_manifest()

        if not previous_manifest:
            # –ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –±—ç–∫–∞–ø–∞ - –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–æ–≤—ã–µ
            return files

        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å: –ø—É—Ç—å -> md5
        previous_hashes = {
            f['path']: f['md5']
            for f in previous_manifest.get('files', [])
        }

        changed_files = []

        for file_path in files:
            rel_path = str(file_path.relative_to(self.root_dir))
            current_md5 = self.calculate_file_hash(file_path, 'md5')

            # –§–∞–π–ª –Ω–æ–≤—ã–π –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è
            if rel_path not in previous_hashes or previous_hashes[rel_path] != current_md5:
                changed_files.append(file_path)

        return changed_files

    def create_zip(self, backup_type='full', compression_level=zipfile.ZIP_DEFLATED):
        """–°–æ–∑–¥–∞—Ç—å ZIP –∞—Ä—Ö–∏–≤"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_name = f"knowledge_backup_{timestamp}_{backup_type}.zip"
        archive_path = self.backups_dir / archive_name

        print(f"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞ ({backup_type})...\n")

        # –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª—ã
        all_files = self.collect_files()

        if backup_type == 'incremental':
            files_to_backup = self.get_changed_files(all_files)
            print(f"   –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∏–∑ {len(all_files)}")
        else:
            files_to_backup = all_files
            print(f"   –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} —Ñ–∞–π–ª–æ–≤")

        if not files_to_backup:
            print("   ‚ö†Ô∏è  –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ç–∫–∞–ø–∞")
            return None

        # –°–æ–∑–¥–∞—Ç—å manifest
        manifest = self.create_manifest(files_to_backup)

        # –°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤
        with zipfile.ZipFile(archive_path, 'w', compression_level) as zipf:
            for i, file_path in enumerate(files_to_backup, 1):
                arcname = file_path.relative_to(self.root_dir)
                zipf.write(file_path, arcname)

                if i % 10 == 0 or i == len(files_to_backup):
                    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(files_to_backup)} —Ñ–∞–π–ª–æ–≤...", end='\r')

            # –î–æ–±–∞–≤–∏—Ç—å manifest –≤ –∞—Ä—Ö–∏–≤
            manifest_json = json.dumps(manifest, ensure_ascii=False, indent=2)
            zipf.writestr('manifest.json', manifest_json)

        print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å manifest –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤
        manifest_path = archive_path.with_suffix('.manifest.json')
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        file_size = archive_path.stat().st_size / 1024  # KB
        compression_ratio = (manifest['total_size'] / archive_path.stat().st_size) if archive_path.stat().st_size > 0 else 0

        print(f"‚úÖ ZIP –∞—Ä—Ö–∏–≤: {archive_path.name}")
        print(f"   –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
        print(f"   –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {compression_ratio:.2f}x")
        print(f"   –§–∞–π–ª–æ–≤: {len(files_to_backup)}")
        print(f"   MD5: {self.calculate_file_hash(archive_path, 'md5')}")

        # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
        if self.verify_archive(archive_path, 'zip'):
            print(f"   ‚úÖ –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏!")

        return archive_path

    def create_tar_gz(self, backup_type='full', compression_level=9):
        """–°–æ–∑–¥–∞—Ç—å TAR.GZ –∞—Ä—Ö–∏–≤"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_name = f"knowledge_backup_{timestamp}_{backup_type}.tar.gz"
        archive_path = self.backups_dir / archive_name

        print(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ TAR.GZ –∞—Ä—Ö–∏–≤–∞ ({backup_type})...\n")

        # –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª—ã
        all_files = self.collect_files()

        if backup_type == 'incremental':
            files_to_backup = self.get_changed_files(all_files)
            print(f"   –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∏–∑ {len(all_files)}")
        else:
            files_to_backup = all_files
            print(f"   –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} —Ñ–∞–π–ª–æ–≤")

        if not files_to_backup:
            print("   ‚ö†Ô∏è  –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ç–∫–∞–ø–∞")
            return None

        # –°–æ–∑–¥–∞—Ç—å manifest
        manifest = self.create_manifest(files_to_backup)

        # –°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤
        with tarfile.open(archive_path, f'w:gz', compresslevel=compression_level) as tarf:
            for i, file_path in enumerate(files_to_backup, 1):
                arcname = file_path.relative_to(self.root_dir)
                tarf.add(file_path, arcname=arcname)

                if i % 10 == 0 or i == len(files_to_backup):
                    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(files_to_backup)} —Ñ–∞–π–ª–æ–≤...", end='\r')

        print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å manifest
        manifest_path = archive_path.with_suffix('.manifest.json')
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        file_size = archive_path.stat().st_size / 1024  # KB
        compression_ratio = (manifest['total_size'] / archive_path.stat().st_size) if archive_path.stat().st_size > 0 else 0

        print(f"‚úÖ TAR.GZ –∞—Ä—Ö–∏–≤: {archive_path.name}")
        print(f"   –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
        print(f"   –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {compression_ratio:.2f}x")
        print(f"   –§–∞–π–ª–æ–≤: {len(files_to_backup)}")
        print(f"   MD5: {self.calculate_file_hash(archive_path, 'md5')}")

        return archive_path

    def verify_archive(self, archive_path, archive_type='zip'):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏–≤–∞"""
        try:
            if archive_type == 'zip':
                with zipfile.ZipFile(archive_path, 'r') as zipf:
                    # testzip() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è –ø–µ—Ä–≤–æ–≥–æ –±–∏—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–ª–∏ None
                    return zipf.testzip() is None
            elif archive_type in ['tar.gz', 'tar']:
                with tarfile.open(archive_path, 'r:*') as tarf:
                    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤—Å–µ —á–ª–µ–Ω—ã
                    for member in tarf.getmembers():
                        pass
                    return True
        except:
            return False

    def rotate_backups(self, archive_type='zip'):
        """–†–æ—Ç–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤"""
        print(f"\nüîÑ –†–æ—Ç–∞—Ü–∏—è –±—ç–∫–∞–ø–æ–≤ ({archive_type})...")

        # –ù–∞–π—Ç–∏ –≤—Å–µ –∞—Ä—Ö–∏–≤—ã –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞
        pattern = f"knowledge_backup_*.{archive_type}"
        archives = sorted(self.backups_dir.glob(pattern))

        if len(archives) <= self.max_backups:
            print(f"   –ê—Ä—Ö–∏–≤–æ–≤: {len(archives)}/{self.max_backups}")
            return

        # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ
        to_delete = archives[:-self.max_backups]

        for archive_path in to_delete:
            print(f"   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∞—Ä—Ö–∏–≤–∞: {archive_path.name}")
            archive_path.unlink()

            # –£–¥–∞–ª–∏—Ç—å manifest
            manifest_path = archive_path.with_suffix('.manifest.json')
            if manifest_path.exists():
                manifest_path.unlink()

        print(f"   –£–¥–∞–ª–µ–Ω–æ: {len(to_delete)} –∞—Ä—Ö–∏–≤–æ–≤")
        print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {len(archives) - len(to_delete)}")

    def list_backups(self):
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—ç–∫–∞–ø–æ–≤"""
        print("\nüìã –°–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤:\n")

        archives = sorted(self.backups_dir.glob("knowledge_backup_*"))

        if not archives:
            print("   –ù–µ—Ç –±—ç–∫–∞–ø–æ–≤")
            return

        for archive_path in archives:
            if archive_path.suffix in ['.zip', '.gz']:
                stat = archive_path.stat()
                size_kb = stat.st_size / 1024
                modified = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')

                print(f"   {archive_path.name}")
                print(f"      –†–∞–∑–º–µ—Ä: {size_kb:.1f} KB")
                print(f"      –î–∞—Ç–∞: {modified}")

                # –ü–æ–∫–∞–∑–∞—Ç—å manifest –µ—Å–ª–∏ –µ—Å—Ç—å
                manifest_path = archive_path.with_suffix('.manifest.json')
                if manifest_path.exists():
                    with open(manifest_path, 'r', encoding='utf-8') as f:
                        manifest = json.load(f)
                    print(f"      –§–∞–π–ª–æ–≤: {manifest.get('total_files', 0)}")

                print()


def main():
    parser = argparse.ArgumentParser(
        description='üì¶ Advanced Archive Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∞—Ä—Ö–∏–≤–æ–≤',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s --full                                   # –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø (ZIP)
  %(prog)s --incremental                            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø
  %(prog)s --differential                           # –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø
  %(prog)s --format tar.gz --full                   # TAR.GZ –∞—Ä—Ö–∏–≤
  %(prog)s --format both --full                     # –û–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞ (ZIP + TAR.GZ)
  %(prog)s --list                                   # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—ç–∫–∞–ø–æ–≤
  %(prog)s --validate                               # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤
  %(prog)s --timeline timeline.html                 # –°–æ–∑–¥–∞—Ç—å HTML timeline
  %(prog)s --compression-analysis                   # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è
  %(prog)s --all                                    # –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø + –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã

–¢–∏–ø—ã –±—ç–∫–∞–ø–æ–≤:
  - Full: –ø–æ–ª–Ω—ã–π –±—ç–∫–∞–ø –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
  - Incremental: —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±—ç–∫–∞–ø–∞
  - Differential: –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ü–û–õ–ù–û–ì–û –±—ç–∫–∞–ø–∞
        """
    )

    # –¢–∏–ø—ã –±—ç–∫–∞–ø–æ–≤ (–≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ)
    backup_group = parser.add_mutually_exclusive_group()
    backup_group.add_argument(
        '--full',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π –±—ç–∫–∞–ø'
    )
    backup_group.add_argument(
        '--incremental',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø (–∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±—ç–∫–∞–ø–∞)'
    )
    backup_group.add_argument(
        '--differential',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø (–∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ full)'
    )

    # –§–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞
    parser.add_argument(
        '-f', '--format',
        choices=['zip', 'tar.gz', 'both'],
        default='zip',
        help='–§–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: zip)'
    )

    # –ê–Ω–∞–ª–∏–∑ –∏ –æ—Ç—á—ë—Ç—ã
    parser.add_argument(
        '-l', '--list',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—ç–∫–∞–ø–æ–≤'
    )

    parser.add_argument(
        '--validate',
        action='store_true',
        help='–í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∞—Ä—Ö–∏–≤–æ–≤'
    )

    parser.add_argument(
        '--timeline',
        metavar='FILE',
        help='–°–æ–∑–¥–∞—Ç—å HTML timeline –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –±—ç–∫–∞–ø–æ–≤'
    )

    parser.add_argument(
        '--compression-analysis',
        action='store_true',
        help='–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è –¥–ª—è —Ñ–∞–π–ª–æ–≤'
    )

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
    parser.add_argument(
        '-r', '--rotate',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–æ—Ç–∞—Ü–∏—é —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤'
    )

    parser.add_argument(
        '--max-backups',
        type=int,
        default=10,
        help='–ú–∞–∫—Å–∏–º—É–º –±—ç–∫–∞–ø–æ–≤ –ø—Ä–∏ —Ä–æ—Ç–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)'
    )

    # –í—Å—ë —Å—Ä–∞–∑—É
    parser.add_argument(
        '--all',
        action='store_true',
        help='–ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø + –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –∏ —ç–∫—Å–ø–æ—Ä—Ç—ã'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent
    backups_dir = root_dir / "backups"
    backups_dir.mkdir(exist_ok=True)

    builder = AdvancedArchiveBuilder(root_dir)
    builder.max_backups = args.max_backups

    # –¢–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤
    if args.list and not args.all:
        builder.list_backups()
        return

    # –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è
    if args.validate and not args.all:
        print("\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∞—Ä—Ö–∏–≤–æ–≤...\n")
        validator = ArchiveValidator()
        result = validator.batch_validate(backups_dir)

        print(f"–í—Å–µ–≥–æ –∞—Ä—Ö–∏–≤–æ–≤: {result['total']}")
        print(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö: {result['valid']}")
        print(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö: {result['invalid']}")

        if result['invalid'] > 0:
            print("\n–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–æ–∫:")
            for detail in result['details']:
                if not detail['valid']:
                    print(f"  ‚ùå {detail['archive']}")
                    for error in detail['errors']:
                        print(f"      - {error}")

        return

    # –¢–æ–ª—å–∫–æ timeline
    if args.timeline and not args.all:
        print(f"\nüìÖ –°–æ–∑–¥–∞–Ω–∏–µ timeline –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...\n")
        timeline_builder = TimelineBuilder(backups_dir)
        timeline_builder.generate_html_timeline(Path(args.timeline))
        print(f"‚úÖ Timeline —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {args.timeline}")
        return

    # –¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç–∏—è
    if args.compression_analysis and not args.all:
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è...\n")
        files = builder.collect_files()
        optimizer = CompressionOptimizer()

        analysis = optimizer.analyze_compression_efficiency(files)
        ratio = optimizer.estimate_compression_ratio(files)

        print("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ñ–∞–π–ª–æ–≤:")
        for category, stats in analysis.items():
            size_mb = stats['size'] / (1024 * 1024)
            print(f"  {category:20s}: {stats['count']:4d} —Ñ–∞–π–ª–æ–≤, {size_mb:6.2f} MB")

        print(f"\n–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {ratio:.2f}x")
        return

    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –±—ç–∫–∞–ø–∞
    if args.all or args.full:
        backup_type = 'full'
    elif args.incremental:
        backup_type = 'incremental'
    elif args.differential:
        backup_type = 'differential'
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - full
        backup_type = 'full'

    # –°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤
    if args.format in ['zip', 'both']:
        archive_path = builder.create_zip(backup_type=backup_type)

        if args.rotate or args.all:
            builder.rotate_backups('zip')

    if args.format in ['tar.gz', 'both']:
        archive_path = builder.create_tar_gz(backup_type=backup_type)

        if args.rotate or args.all:
            builder.rotate_backups('tar.gz')

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã –ø—Ä–∏ --all
    if args.all:
        print("\n" + "="*60)
        print("–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ê–ù–ê–õ–ò–ó–´")
        print("="*60)

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        print("\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∞—Ä—Ö–∏–≤–æ–≤...")
        validator = ArchiveValidator()
        result = validator.batch_validate(backups_dir)
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö: {result['valid']}/{result['total']}")

        # Timeline
        timeline_file = root_dir / "backup_timeline.html"
        print(f"\nüìÖ –°–æ–∑–¥–∞–Ω–∏–µ timeline...")
        timeline_builder = TimelineBuilder(backups_dir)
        timeline_builder.generate_html_timeline(timeline_file)
        print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {timeline_file}")

        # –ê–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç–∏—è
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç–∏—è...")
        files = builder.collect_files()
        optimizer = CompressionOptimizer()
        ratio = optimizer.estimate_compression_ratio(files)
        print(f"   –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {ratio:.2f}x")

        # –°–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤
        builder.list_backups()

        print("\n‚ú® –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")


if __name__ == "__main__":
    main()
