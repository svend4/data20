#!/usr/bin/env python3
"""
Archive Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∞—Ä—Ö–∏–≤–æ–≤
–°–æ–∑–¥–∞—ë—Ç –∞—Ä—Ö–∏–≤—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –±—ç–∫–∞–ø—ã
- –†–æ—Ç–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤
- –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (MD5/SHA256)
- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ manifest
- –ü—Ä–æ–≥—Ä–µ—Å—Å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –ì–∏–±–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –≤–∫–ª—é—á–µ–Ω–∏—è/–∏—Å–∫–ª—é—á–µ–Ω–∏—è

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: tar, rsync, Time Machine, Duplicity
"""

from pathlib import Path
import zipfile
import tarfile
from datetime import datetime
import json
import hashlib
import shutil
import re


class AdvancedArchiveBuilder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∞—Ä—Ö–∏–≤–æ–≤"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.backups_dir = self.root_dir / "backups"
        self.backups_dir.mkdir(exist_ok=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–æ—Ç–∞—Ü–∏–∏
        self.max_backups = 10  # –ú–∞–∫—Å–∏–º—É–º –∞—Ä—Ö–∏–≤–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        self.exclude_patterns = [
            r'\.git',
            r'__pycache__',
            r'\.pyc$',
            r'\.DS_Store',
            r'Thumbs\.db'
        ]

    def calculate_file_hash(self, file_path, algorithm='md5'):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à —Ñ–∞–π–ª–∞"""
        hash_func = hashlib.md5() if algorithm == 'md5' else hashlib.sha256()

        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hash_func.update(chunk)

        return hash_func.hexdigest()

    def should_exclude(self, file_path):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å —Ñ–∞–π–ª"""
        file_str = str(file_path)

        for pattern in self.exclude_patterns:
            if re.search(pattern, file_str):
                return True

        return False

    def collect_files(self):
        """–°–æ–±—Ä–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏"""
        files = []

        for file_path in self.knowledge_dir.rglob("*"):
            if file_path.is_file() and not self.should_exclude(file_path):
                files.append(file_path)

        return files

    def get_file_metadata(self, file_path):
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞"""
        stat = file_path.stat()

        return {
            'path': str(file_path.relative_to(self.root_dir)),
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
            'md5': self.calculate_file_hash(file_path, 'md5')
        }

    def create_manifest(self, files):
        """–°–æ–∑–¥–∞—Ç—å manifest —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        manifest = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(files),
            'total_size': sum(f.stat().st_size for f in files),
            'files': []
        }

        print(f"   –°–æ–∑–¥–∞–Ω–∏–µ manifest –¥–ª—è {len(files)} —Ñ–∞–π–ª–æ–≤...")

        for file_path in files:
            manifest['files'].append(self.get_file_metadata(file_path))

        return manifest

    def load_previous_manifest(self, archive_type='zip'):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π manifest –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞"""
        # –ù–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞—Ä—Ö–∏–≤
        pattern = f"knowledge_backup_*_full.{archive_type}"
        archives = sorted(self.backups_dir.glob(pattern))

        if not archives:
            return None

        last_archive = archives[-1]
        manifest_path = last_archive.with_suffix('.manifest.json')

        if manifest_path.exists():
            with open(manifest_path, 'r', encoding='utf-8') as f:
                return json.load(f)

        return None

    def get_changed_files(self, files):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞"""
        previous_manifest = self.load_previous_manifest()

        if not previous_manifest:
            # –ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –±—ç–∫–∞–ø–∞ - –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–æ–≤—ã–µ
            return files

        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å: –ø—É—Ç—å -> md5
        previous_hashes = {
            f['path']: f['md5']
            for f in previous_manifest.get('files', [])
        }

        changed_files = []

        for file_path in files:
            rel_path = str(file_path.relative_to(self.root_dir))
            current_md5 = self.calculate_file_hash(file_path, 'md5')

            # –§–∞–π–ª –Ω–æ–≤—ã–π –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è
            if rel_path not in previous_hashes or previous_hashes[rel_path] != current_md5:
                changed_files.append(file_path)

        return changed_files

    def create_zip(self, backup_type='full', compression_level=zipfile.ZIP_DEFLATED):
        """–°–æ–∑–¥–∞—Ç—å ZIP –∞—Ä—Ö–∏–≤"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_name = f"knowledge_backup_{timestamp}_{backup_type}.zip"
        archive_path = self.backups_dir / archive_name

        print(f"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞ ({backup_type})...\n")

        # –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª—ã
        all_files = self.collect_files()

        if backup_type == 'incremental':
            files_to_backup = self.get_changed_files(all_files)
            print(f"   –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∏–∑ {len(all_files)}")
        else:
            files_to_backup = all_files
            print(f"   –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} —Ñ–∞–π–ª–æ–≤")

        if not files_to_backup:
            print("   ‚ö†Ô∏è  –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ç–∫–∞–ø–∞")
            return None

        # –°–æ–∑–¥–∞—Ç—å manifest
        manifest = self.create_manifest(files_to_backup)

        # –°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤
        with zipfile.ZipFile(archive_path, 'w', compression_level) as zipf:
            for i, file_path in enumerate(files_to_backup, 1):
                arcname = file_path.relative_to(self.root_dir)
                zipf.write(file_path, arcname)

                if i % 10 == 0 or i == len(files_to_backup):
                    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(files_to_backup)} —Ñ–∞–π–ª–æ–≤...", end='\r')

            # –î–æ–±–∞–≤–∏—Ç—å manifest –≤ –∞—Ä—Ö–∏–≤
            manifest_json = json.dumps(manifest, ensure_ascii=False, indent=2)
            zipf.writestr('manifest.json', manifest_json)

        print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å manifest –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤
        manifest_path = archive_path.with_suffix('.manifest.json')
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        file_size = archive_path.stat().st_size / 1024  # KB
        compression_ratio = (manifest['total_size'] / archive_path.stat().st_size) if archive_path.stat().st_size > 0 else 0

        print(f"‚úÖ ZIP –∞—Ä—Ö–∏–≤: {archive_path.name}")
        print(f"   –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
        print(f"   –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {compression_ratio:.2f}x")
        print(f"   –§–∞–π–ª–æ–≤: {len(files_to_backup)}")
        print(f"   MD5: {self.calculate_file_hash(archive_path, 'md5')}")

        # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
        if self.verify_archive(archive_path, 'zip'):
            print(f"   ‚úÖ –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏!")

        return archive_path

    def create_tar_gz(self, backup_type='full', compression_level=9):
        """–°–æ–∑–¥–∞—Ç—å TAR.GZ –∞—Ä—Ö–∏–≤"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archive_name = f"knowledge_backup_{timestamp}_{backup_type}.tar.gz"
        archive_path = self.backups_dir / archive_name

        print(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ TAR.GZ –∞—Ä—Ö–∏–≤–∞ ({backup_type})...\n")

        # –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª—ã
        all_files = self.collect_files()

        if backup_type == 'incremental':
            files_to_backup = self.get_changed_files(all_files)
            print(f"   –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∏–∑ {len(all_files)}")
        else:
            files_to_backup = all_files
            print(f"   –ü–æ–ª–Ω—ã–π –±—ç–∫–∞–ø: {len(files_to_backup)} —Ñ–∞–π–ª–æ–≤")

        if not files_to_backup:
            print("   ‚ö†Ô∏è  –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ç–∫–∞–ø–∞")
            return None

        # –°–æ–∑–¥–∞—Ç—å manifest
        manifest = self.create_manifest(files_to_backup)

        # –°–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤
        with tarfile.open(archive_path, f'w:gz', compresslevel=compression_level) as tarf:
            for i, file_path in enumerate(files_to_backup, 1):
                arcname = file_path.relative_to(self.root_dir)
                tarf.add(file_path, arcname=arcname)

                if i % 10 == 0 or i == len(files_to_backup):
                    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(files_to_backup)} —Ñ–∞–π–ª–æ–≤...", end='\r')

        print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å manifest
        manifest_path = archive_path.with_suffix('.manifest.json')
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        file_size = archive_path.stat().st_size / 1024  # KB
        compression_ratio = (manifest['total_size'] / archive_path.stat().st_size) if archive_path.stat().st_size > 0 else 0

        print(f"‚úÖ TAR.GZ –∞—Ä—Ö–∏–≤: {archive_path.name}")
        print(f"   –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
        print(f"   –°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {compression_ratio:.2f}x")
        print(f"   –§–∞–π–ª–æ–≤: {len(files_to_backup)}")
        print(f"   MD5: {self.calculate_file_hash(archive_path, 'md5')}")

        return archive_path

    def verify_archive(self, archive_path, archive_type='zip'):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏–≤–∞"""
        try:
            if archive_type == 'zip':
                with zipfile.ZipFile(archive_path, 'r') as zipf:
                    # testzip() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è –ø–µ—Ä–≤–æ–≥–æ –±–∏—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–ª–∏ None
                    return zipf.testzip() is None
            elif archive_type in ['tar.gz', 'tar']:
                with tarfile.open(archive_path, 'r:*') as tarf:
                    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤—Å–µ —á–ª–µ–Ω—ã
                    for member in tarf.getmembers():
                        pass
                    return True
        except:
            return False

    def rotate_backups(self, archive_type='zip'):
        """–†–æ—Ç–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤"""
        print(f"\nüîÑ –†–æ—Ç–∞—Ü–∏—è –±—ç–∫–∞–ø–æ–≤ ({archive_type})...")

        # –ù–∞–π—Ç–∏ –≤—Å–µ –∞—Ä—Ö–∏–≤—ã –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞
        pattern = f"knowledge_backup_*.{archive_type}"
        archives = sorted(self.backups_dir.glob(pattern))

        if len(archives) <= self.max_backups:
            print(f"   –ê—Ä—Ö–∏–≤–æ–≤: {len(archives)}/{self.max_backups}")
            return

        # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ
        to_delete = archives[:-self.max_backups]

        for archive_path in to_delete:
            print(f"   –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∞—Ä—Ö–∏–≤–∞: {archive_path.name}")
            archive_path.unlink()

            # –£–¥–∞–ª–∏—Ç—å manifest
            manifest_path = archive_path.with_suffix('.manifest.json')
            if manifest_path.exists():
                manifest_path.unlink()

        print(f"   –£–¥–∞–ª–µ–Ω–æ: {len(to_delete)} –∞—Ä—Ö–∏–≤–æ–≤")
        print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {len(archives) - len(to_delete)}")

    def list_backups(self):
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±—ç–∫–∞–ø–æ–≤"""
        print("\nüìã –°–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤:\n")

        archives = sorted(self.backups_dir.glob("knowledge_backup_*"))

        if not archives:
            print("   –ù–µ—Ç –±—ç–∫–∞–ø–æ–≤")
            return

        for archive_path in archives:
            if archive_path.suffix in ['.zip', '.gz']:
                stat = archive_path.stat()
                size_kb = stat.st_size / 1024
                modified = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')

                print(f"   {archive_path.name}")
                print(f"      –†–∞–∑–º–µ—Ä: {size_kb:.1f} KB")
                print(f"      –î–∞—Ç–∞: {modified}")

                # –ü–æ–∫–∞–∑–∞—Ç—å manifest –µ—Å–ª–∏ –µ—Å—Ç—å
                manifest_path = archive_path.with_suffix('.manifest.json')
                if manifest_path.exists():
                    with open(manifest_path, 'r', encoding='utf-8') as f:
                        manifest = json.load(f)
                    print(f"      –§–∞–π–ª–æ–≤: {manifest.get('total_files', 0)}")

                print()


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Archive Builder')
    parser.add_argument('-f', '--format', choices=['zip', 'tar.gz', 'both'], default='zip',
                       help='–§–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞')
    parser.add_argument('-t', '--type', choices=['full', 'incremental'], default='full',
                       help='–¢–∏–ø –±—ç–∫–∞–ø–∞')
    parser.add_argument('-l', '--list', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –±—ç–∫–∞–ø–æ–≤')
    parser.add_argument('-r', '--rotate', action='store_true',
                       help='–í—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–æ—Ç–∞—Ü–∏—é —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤')
    parser.add_argument('--max-backups', type=int, default=10,
                       help='–ú–∞–∫—Å–∏–º—É–º –±—ç–∫–∞–ø–æ–≤ –ø—Ä–∏ —Ä–æ—Ç–∞—Ü–∏–∏')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = AdvancedArchiveBuilder(root_dir)
    builder.max_backups = args.max_backups

    if args.list:
        builder.list_backups()
        return

    if args.format in ['zip', 'both']:
        builder.create_zip(backup_type=args.type)
        if args.rotate:
            builder.rotate_backups('zip')

    if args.format in ['tar.gz', 'both']:
        builder.create_tar_gz(backup_type=args.type)
        if args.rotate:
            builder.rotate_backups('tar.gz')


if __name__ == "__main__":
    main()
