#!/usr/bin/env python3
"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
- Boolean operators (AND, OR, NOT)
- TF-IDF ranking
- Fuzzy matching
- Phrase search
"""

import os
import re
from pathlib import Path
from collections import defaultdict, Counter
import math


class AdvancedSearch:
    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.documents = {}
        self.idf = {}
        self.load_documents()
        self.calculate_idf()

    def load_documents(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å frontmatter
                content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)

                file_key = str(md_file.relative_to(self.root_dir))
                self.documents[file_key] = {
                    'content': content,
                    'path': md_file
                }
            except:
                pass

        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")

    def tokenize(self, text):
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
        # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞, –º–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞)
        words = re.findall(r'\b[–∞-—è—ëa-z]{2,}\b', text.lower())
        return words

    def calculate_idf(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å IDF (Inverse Document Frequency) –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        print("üî¢ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IDF...")

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –≤ —Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
        df = defaultdict(int)

        for doc_data in self.documents.values():
            words = set(self.tokenize(doc_data['content']))
            for word in words:
                df[word] += 1

        # –í—ã—á–∏—Å–ª–∏—Ç—å IDF
        num_docs = len(self.documents)
        for word, doc_freq in df.items():
            self.idf[word] = math.log(num_docs / doc_freq)

    def calculate_tf_idf(self, doc_content, query_terms):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF score –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        doc_words = self.tokenize(doc_content)
        doc_length = len(doc_words)

        if doc_length == 0:
            return 0.0

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å TF (Term Frequency)
        tf = Counter(doc_words)

        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF score
        score = 0.0
        for term in query_terms:
            if term in tf:
                # TF * IDF
                term_tf = tf[term] / doc_length
                term_idf = self.idf.get(term, 0)
                score += term_tf * term_idf

        return score

    def boolean_search(self, query):
        """
        Boolean –ø–æ–∏—Å–∫ —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏ AND, OR, NOT
        –ü—Ä–∏–º–µ—Ä—ã:
        - "docker AND kubernetes"
        - "python OR javascript"
        - "programming NOT java"
        - "(docker OR kubernetes) AND NOT windows"
        """
        # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä boolean –∑–∞–ø—Ä–æ—Å–æ–≤
        query = query.strip()

        # –†–∞–∑–±–∏—Ç—å –ø–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º
        if ' AND ' in query:
            parts = [p.strip() for p in query.split(' AND ')]
            results = None

            for part in parts:
                part_results = self.boolean_search(part)
                if results is None:
                    results = part_results
                else:
                    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (AND)
                    results = {k: v for k, v in results.items() if k in part_results}

            return results if results else {}

        elif ' OR ' in query:
            parts = [p.strip() for p in query.split(' OR ')]
            results = {}

            for part in parts:
                part_results = self.boolean_search(part)
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (OR)
                results.update(part_results)

            return results

        elif ' NOT ' in query:
            parts = query.split(' NOT ', 1)
            positive = self.boolean_search(parts[0].strip())
            negative = self.boolean_search(parts[1].strip())

            # –ò—Å–∫–ª—é—á–∏—Ç—å (NOT)
            return {k: v for k, v in positive.items() if k not in negative}

        elif query.startswith('(') and query.endswith(')'):
            # –£–±—Ä–∞—Ç—å —Å–∫–æ–±–∫–∏
            return self.boolean_search(query[1:-1])

        else:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            return self.simple_search(query)

    def simple_search(self, term):
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ —Å TF-IDF"""
        term_lower = term.lower()
        results = {}

        for doc_id, doc_data in self.documents.items():
            content = doc_data['content'].lower()

            if term_lower in content:
                # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF score
                score = self.calculate_tf_idf(content, [term_lower])
                results[doc_id] = score

        return results

    def phrase_search(self, phrase):
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–π —Ñ—Ä–∞–∑—ã"""
        phrase_lower = phrase.lower()
        results = {}

        for doc_id, doc_data in self.documents.items():
            content = doc_data['content'].lower()

            if phrase_lower in content:
                # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π
                count = content.count(phrase_lower)
                results[doc_id] = count * 10.0  # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ

        return results

    def search(self, query, limit=10):
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
        - Boolean operators (AND, OR, NOT)
        - Phrase search ("exact phrase")
        - Simple terms
        """
        print(f"\nüîç –ü–æ–∏—Å–∫: {query}\n")

        # –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–π —Ñ—Ä–∞–∑—ã (–≤ –∫–∞–≤—ã—á–∫–∞—Ö)
        if query.startswith('"') and query.endswith('"'):
            phrase = query[1:-1]
            results = self.phrase_search(phrase)
            search_type = "Phrase search"

        # Boolean search
        elif any(op in query for op in [' AND ', ' OR ', ' NOT ']):
            results = self.boolean_search(query)
            search_type = "Boolean search"

        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
        else:
            results = self.simple_search(query)
            search_type = "Simple search"

        if not results:
            print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)

        print(f"üìä {search_type}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(sorted_results)}\n")
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏):\n")

        for i, (doc_id, score) in enumerate(sorted_results[:limit], 1):
            print(f"{i}. {doc_id}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.4f}")

            # –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            content = self.documents[doc_id]['content']
            context = self.get_context(content, query)
            if context:
                print(f"   {context}")

            print()

        if len(sorted_results) > limit:
            print(f"   ...–∏ –µ—â—ë {len(sorted_results) - limit} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    def get_context(self, content, query, window=100):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∑–∞–ø—Ä–æ—Å–∞"""
        # –£–ø—Ä–æ—Å—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å (—É–±—Ä–∞—Ç—å –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã)
        simple_query = re.sub(r'\s+(AND|OR|NOT)\s+', ' ', query)
        simple_query = simple_query.replace('"', '').strip()

        # –ù–∞–π—Ç–∏ –ø–µ—Ä–≤–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ
        pos = content.lower().find(simple_query.lower().split()[0])

        if pos == -1:
            return content[:window] + "..."

        start = max(0, pos - window // 2)
        end = min(len(content), pos + window // 2)

        context = content[start:end].strip()

        if start > 0:
            context = '...' + context
        if end < len(content):
            context = context + '...'

        return context


def main():
    import sys

    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python advanced_search.py <–∑–∞–ø—Ä–æ—Å>")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print('  python tools/advanced_search.py "docker"')
        print('  python tools/advanced_search.py "docker AND kubernetes"')
        print('  python tools/advanced_search.py "python OR javascript"')
        print('  python tools/advanced_search.py "programming NOT java"')
        print('  python tools/advanced_search.py "\\"exact phrase\\""')
        return

    query = ' '.join(sys.argv[1:])

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    searcher = AdvancedSearch(root_dir)
    searcher.search(query, limit=20)


if __name__ == "__main__":
    main()
