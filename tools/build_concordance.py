#!/usr/bin/env python3
"""
Concordance Builder - –°—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
–°–æ–∑–¥–∞—ë—Ç –ø–æ–ª–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Å–ª–æ–≤ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Ö –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Concordantia Sacrorum Bibliorum (1230 –≥.)
"""

import os
import re
from pathlib import Path
from collections import defaultdict
import json


class ConcordanceBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–∞ - –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–≥–æ —É–∫–∞–∑–∞—Ç–µ–ª—è –≤—Å–µ—Ö –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤
    —Å —É–∫–∞–∑–∞–Ω–∏–µ–º, –≥–¥–µ –æ–Ω–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.concordance = defaultdict(list)

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞)
        self.stop_words = {
            # –†—É—Å—Å–∫–∏–µ
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫',
            '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫',
            '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ',
            '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å',
            '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏',
            '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å',
            '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π',
            '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è',
            '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑',
            '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂',
            '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π',
            '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π',
            '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö',
            '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π',
            '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏',
            '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ',
            '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π',
            '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π',
            '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É',

            # English
            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
            'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
            'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they',
            'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one',
            'all', 'would', 'there', 'their', 'what', 'so', 'up', 'out',
            'if', 'about', 'who', 'get', 'which', 'go', 'me', 'when',
            'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know',
            'take', 'people', 'into', 'year', 'your', 'good', 'some',
            'could', 'them', 'see', 'other', 'than', 'then', 'now',
            'look', 'only', 'come', 'its', 'over', 'think', 'also',
            'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first',
            'well', 'way', 'even', 'new', 'want', 'because', 'any',
            'these', 'give', 'day', 'most', 'us', 'is', 'was', 'are',
            'been', 'has', 'had', 'were', 'said', 'did', 'having',
            'may', 'should', 'does', 'being'
        }

    def extract_words(self, text, file_path):
        """
        –ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: —Å–ø–∏—Å–æ–∫ (—Å–ª–æ–≤–æ, –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        """
        words = []
        lines = text.split('\n')

        for line_num, line in enumerate(lines, 1):
            # –£–¥–∞–ª–∏—Ç—å markdown —Ä–∞–∑–º–µ—Ç–∫—É
            clean_line = re.sub(r'[#*`\[\]()]', ' ', line)

            # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞)
            found_words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', clean_line.lower())

            for word in found_words:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                if word in self.stop_words:
                    continue

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–∏—Å–ª–∞
                if word.isdigit():
                    continue

                # –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å–ª–æ–≤–∞ –≤–æ–∫—Ä—É–≥)
                context = self.get_context(clean_line, word, window=40)

                words.append({
                    'word': word,
                    'line': line_num,
                    'context': context,
                    'file': str(file_path.relative_to(self.root_dir))
                })

        return words

    def get_context(self, line, word, window=40):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞"""
        # –ù–∞–π—Ç–∏ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞
        pos = line.lower().find(word.lower())
        if pos == -1:
            return line[:window]

        # –í–∑—è—Ç—å –æ–∫–Ω–æ –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞
        start = max(0, pos - window // 2)
        end = min(len(line), pos + len(word) + window // 2)

        context = line[start:end].strip()

        # –î–æ–±–∞–≤–∏—Ç—å –º–Ω–æ–≥–æ—Ç–æ—á–∏—è –µ—Å–ª–∏ –æ–±—Ä–µ–∑–∞–Ω–æ
        if start > 0:
            context = '...' + context
        if end < len(line):
            context = context + '...'

        return context

    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å"""
        print("üìñ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–∞ (concordance)...")
        print("   –í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ —Å—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –ë–∏–±–ª–∏–∏\n")

        total_words = 0
        total_files = 0

        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            total_files += 1

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å frontmatter
                content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)

                # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞
                words = self.extract_words(content, md_file)
                total_words += len(words)

                # –î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å
                for entry in words:
                    word = entry['word']
                    self.concordance[word].append({
                        'file': entry['file'],
                        'line': entry['line'],
                        'context': entry['context']
                    })

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {md_file}: {e}")

        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤: {total_words}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.concordance)}")

    def save(self, output_file):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å –≤ JSON"""
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        sorted_concordance = dict(sorted(self.concordance.items()))

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(sorted_concordance, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –ö–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_file}")

    def save_markdown(self, output_file):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å –≤ markdown —Ñ–æ—Ä–º–∞—Ç–µ"""
        lines = []
        lines.append("# Concordance - –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å —Å–ª–æ–≤\n")
        lines.append(f"> –°–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π\n")
        lines.append(f"> –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.concordance)}\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
        current_letter = None

        for word in sorted(self.concordance.keys()):
            entries = self.concordance[word]

            # –ù–æ–≤–∞—è –±—É–∫–≤–∞ - –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
            first_letter = word[0].upper()
            if first_letter != current_letter:
                current_letter = first_letter
                lines.append(f"\n## {current_letter}\n\n")

            # –°–ª–æ–≤–æ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            lines.append(f"### {word} ({len(entries)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n\n")

            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 5 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            for entry in entries[:5]:
                lines.append(f"- **{entry['file']}:{entry['line']}**  \n")
                lines.append(f"  _{entry['context']}_\n\n")

            if len(entries) > 5:
                lines.append(f"  _...–∏ –µ—â—ë {len(entries) - 5} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π_\n\n")

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown –≤–µ—Ä—Å–∏—è: {output_file}")

    def search_word(self, word):
        """–ü–æ–∏—Å–∫ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ"""
        word_lower = word.lower()

        if word_lower not in self.concordance:
            print(f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ")
            return []

        entries = self.concordance[word_lower]
        print(f"\nüìñ –°–ª–æ–≤–æ '{word}' –Ω–∞–π–¥–µ–Ω–æ –≤ {len(entries)} –º–µ—Å—Ç–∞—Ö:\n")

        for i, entry in enumerate(entries[:20], 1):
            print(f"{i}. {entry['file']}:{entry['line']}")
            print(f"   {entry['context']}\n")

        if len(entries) > 20:
            print(f"   ...–∏ –µ—â—ë {len(entries) - 20} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

        return entries

    def get_top_words(self, n=50):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤"""
        word_counts = [(word, len(entries))
                      for word, entries in self.concordance.items()]

        word_counts.sort(key=lambda x: x[1], reverse=True)

        print(f"\nüìä –¢–æ–ø-{n} —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:\n")

        for i, (word, count) in enumerate(word_counts[:n], 1):
            print(f"{i:3d}. {word:20s} - {count:4d} —Ä–∞–∑")

        return word_counts[:n]


def main():
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = ConcordanceBuilder(root_dir)

    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å
    builder.build()

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    output_dir = root_dir
    builder.save(output_dir / "concordance.json")
    builder.save_markdown(output_dir / "CONCORDANCE.md")

    # –ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø —Å–ª–æ–≤
    builder.get_top_words(30)

    print("\nüí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
    print("   python tools/search_concordance.py <—Å–ª–æ–≤–æ>")


if __name__ == "__main__":
    main()
