#!/usr/bin/env python3
"""
Advanced Concordance Search - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ
–§—É–Ω–∫—Ü–∏–∏:
- Fuzzy search —Å Levenshtein distance
- Regex search
- Boolean operators (AND, OR, NOT)
- Wildcard search (*, ?)
- Phrase search ("...")
- KWIC (Key Word In Context)
- Context highlighting
- Export results (JSON, TXT, CSV)
- Search statistics

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: grep, ack, ag, ripgrep, Elasticsearch
"""

import json
import sys
from pathlib import Path
import re
from collections import Counter


class AdvancedConcordanceSearch:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ"""

    def __init__(self, concordance_file):
        self.concordance_file = concordance_file
        self.concordance = None
        self.load_concordance()

    def load_concordance(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å"""
        if not self.concordance_file.exists():
            print("‚ùå –ö–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞:")
            print("   python tools/build_concordance.py")
            return False

        with open(self.concordance_file, 'r', encoding='utf-8') as f:
            self.concordance = json.load(f)

        return True

    def levenshtein_distance(self, s1, s2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)

        for i, c1 in enumerate(s1):
            current_row = [i + 1]

            for j, c2 in enumerate(s2):
                # –°—Ç–æ–∏–º–æ—Å—Ç—å –≤—Å—Ç–∞–≤–∫–∏, —É–¥–∞–ª–µ–Ω–∏—è, –∑–∞–º–µ–Ω—ã
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)

                current_row.append(min(insertions, deletions, substitutions))

            previous_row = current_row

        return previous_row[-1]

    def fuzzy_search(self, word, max_distance=2):
        """–ù–µ—á—ë—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å Levenshtein distance"""
        if not self.concordance:
            return []

        word_lower = word.lower()
        matches = []

        for concordance_word in self.concordance.keys():
            distance = self.levenshtein_distance(word_lower, concordance_word)

            if distance <= max_distance:
                matches.append((concordance_word, distance))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ distance
        matches.sort(key=lambda x: x[1])

        return matches

    def regex_search(self, pattern):
        """–ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é"""
        if not self.concordance:
            return []

        try:
            regex = re.compile(pattern, re.IGNORECASE)
        except re.error as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ regex: {e}")
            return []

        matches = []

        for word, entries in self.concordance.items():
            if regex.search(word):
                matches.append((word, entries))

        return matches

    def wildcard_search(self, pattern):
        """–ü–æ–∏—Å–∫ —Å wildcards (* –∏ ?)"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å wildcard –≤ regex
        regex_pattern = pattern.replace('*', '.*').replace('?', '.')
        regex_pattern = '^' + regex_pattern + '$'

        return self.regex_search(regex_pattern)

    def boolean_search(self, query):
        """Boolean –ø–æ–∏—Å–∫ (AND, OR, NOT)"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è: —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ AND/OR/NOT
        # –ü—Ä–∏–º–µ—Ä: "docker AND python" –∏–ª–∏ "docker OR kubernetes"

        if ' AND ' in query.upper():
            words = [w.strip().lower() for w in re.split(r'\s+AND\s+', query, flags=re.IGNORECASE)]
            return self._search_and(words)

        elif ' OR ' in query.upper():
            words = [w.strip().lower() for w in re.split(r'\s+OR\s+', query, flags=re.IGNORECASE)]
            return self._search_or(words)

        elif ' NOT ' in query.upper():
            parts = re.split(r'\s+NOT\s+', query, flags=re.IGNORECASE)
            if len(parts) == 2:
                include = parts[0].strip().lower()
                exclude = parts[1].strip().lower()
                return self._search_not(include, exclude)

        # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
        return self.exact_search(query)

    def _search_and(self, words):
        """–ü–æ–∏—Å–∫ —Å AND - –≤—Å–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å"""
        if not self.concordance:
            return []

        # –ù–∞–π—Ç–∏ –æ–±—â–∏–µ —Ñ–∞–π–ª—ã –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤
        file_sets = []

        for word in words:
            if word in self.concordance:
                files = set(entry['file'] for entry in self.concordance[word])
                file_sets.append(files)

        if not file_sets:
            return []

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–Ω–æ–∂–µ—Å—Ç–≤
        common_files = file_sets[0]
        for file_set in file_sets[1:]:
            common_files &= file_set

        # –°–æ–±—Ä–∞—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ –æ–±—â–∏—Ö —Ñ–∞–π–ª–æ–≤
        results = []
        for word in words:
            if word in self.concordance:
                for entry in self.concordance[word]:
                    if entry['file'] in common_files:
                        results.append((word, entry))

        return results

    def _search_or(self, words):
        """–ü–æ–∏—Å–∫ —Å OR - –ª—é–±–æ–µ —Å–ª–æ–≤–æ"""
        if not self.concordance:
            return []

        results = []

        for word in words:
            if word in self.concordance:
                for entry in self.concordance[word]:
                    results.append((word, entry))

        return results

    def _search_not(self, include_word, exclude_word):
        """–ü–æ–∏—Å–∫ —Å NOT - –∏—Å–∫–ª—é—á–∏—Ç—å —Å–ª–æ–≤–æ"""
        if not self.concordance:
            return []

        # –§–∞–π–ª—ã —Å exclude_word
        exclude_files = set()
        if exclude_word in self.concordance:
            exclude_files = set(entry['file'] for entry in self.concordance[exclude_word])

        # –ò—Å–∫–∞—Ç—å include_word, –Ω–æ –Ω–µ –≤ exclude_files
        results = []
        if include_word in self.concordance:
            for entry in self.concordance[include_word]:
                if entry['file'] not in exclude_files:
                    results.append((include_word, entry))

        return results

    def exact_search(self, word):
        """–¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å–ª–æ–≤–∞"""
        if not self.concordance:
            return []

        word_lower = word.lower()

        if word_lower not in self.concordance:
            return []

        entries = self.concordance[word_lower]
        return [(word_lower, entry) for entry in entries]

    def highlight_context(self, context, word):
        """–ü–æ–¥—Å–≤–µ—Ç–∏—Ç—å —Å–ª–æ–≤–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ANSI escape codes –¥–ª—è —Ü–≤–µ—Ç–∞
        highlighted = re.sub(
            f'({re.escape(word)})',
            r'\033[1;31m\1\033[0m',  # –ö—Ä–∞—Å–Ω—ã–π —Ü–≤–µ—Ç
            context,
            flags=re.IGNORECASE
        )
        return highlighted

    def kwic_display(self, results, context_width=40):
        """KWIC (Key Word In Context) –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
        print("\n" + "=" * 80)
        print("KWIC Display".center(80))
        print("=" * 80 + "\n")

        for word, entry in results[:50]:
            context = entry['context']

            # –ù–∞–π—Ç–∏ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            match = re.search(re.escape(word), context, re.IGNORECASE)

            if match:
                start = match.start()
                end = match.end()

                # –í—ã—Ä–µ–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞
                left_context = context[max(0, start - context_width):start]
                keyword = context[start:end]
                right_context = context[end:min(len(context), end + context_width)]

                # –í—ã—Ä–æ–≤–Ω—è—Ç—å
                print(f"{left_context:>{context_width}} ", end='')
                print(f"\033[1;31m{keyword}\033[0m", end='')
                print(f" {right_context:<{context_width}}")
                print(f"  ‚Üí {entry['file']}:{entry['line']}\n")

    def generate_statistics(self, results):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        if not results:
            return

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã
        files = Counter(entry['file'] for _, entry in results)

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–∞
        words = Counter(word for word, _ in results)

        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞:\n")
        print(f"   –í—Å–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(results)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(files)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(words)}\n")

        print("   –¢–æ–ø-5 —Ñ–∞–π–ª–æ–≤:")
        for file, count in files.most_common(5):
            print(f"      {file}: {count}")

        if len(words) > 1:
            print("\n   –¢–æ–ø-5 —Å–ª–æ–≤:")
            for word, count in words.most_common(5):
                print(f"      {word}: {count}")

    def export_results(self, results, output_format='txt', output_file=None):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not results:
            print("   –ù–µ—á–µ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å")
            return

        if output_format == 'json':
            data = [
                {'word': word, **entry}
                for word, entry in results
            ]

            output = json.dumps(data, ensure_ascii=False, indent=2)

        elif output_format == 'csv':
            lines = ['word,file,line,context']
            for word, entry in results:
                context = entry['context'].replace('"', '""')
                lines.append(f'"{word}","{entry["file"]}",{entry["line"]},"{context}"')

            output = '\n'.join(lines)

        else:  # txt
            lines = []
            for word, entry in results:
                lines.append(f"Word: {word}")
                lines.append(f"File: {entry['file']}:{entry['line']}")
                lines.append(f"Context: {entry['context']}")
                lines.append("")

            output = '\n'.join(lines)

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(output)
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_file}")
        else:
            print(output)

    def search(self, query, mode='exact', max_results=50, display='list', export=None):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫"""
        if not self.concordance:
            return

        # –í—ã–±—Ä–∞—Ç—å —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
        if mode == 'fuzzy':
            matches = self.fuzzy_search(query)
            results = []
            for word, distance in matches:
                for entry in self.concordance[word]:
                    results.append((word, entry))
                    if len(results) >= max_results:
                        break

        elif mode == 'regex':
            matches = self.regex_search(query)
            results = []
            for word, entries in matches:
                for entry in entries:
                    results.append((word, entry))

        elif mode == 'wildcard':
            matches = self.wildcard_search(query)
            results = []
            for word, entries in matches:
                for entry in entries:
                    results.append((word, entry))

        elif mode == 'boolean':
            results = self.boolean_search(query)

        else:  # exact
            results = self.exact_search(query)

        if not results:
            print(f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è '{query}'")

            # –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
            fuzzy_matches = self.fuzzy_search(query, max_distance=2)
            if fuzzy_matches:
                print(f"\n–ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞:")
                for word, distance in fuzzy_matches[:10]:
                    print(f"  - {word} (distance: {distance})")

            return

        print(f"\nüìñ –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n")

        # –û—Ç–æ–±—Ä–∞–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if display == 'kwic':
            self.kwic_display(results)
        else:
            for i, (word, entry) in enumerate(results[:max_results], 1):
                print(f"{i}. {entry['file']}:{entry['line']}")
                highlighted = self.highlight_context(entry['context'], word)
                print(f"   {highlighted}\n")

            if len(results) > max_results:
                print(f"   ...–∏ –µ—â—ë {len(results) - max_results} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.generate_statistics(results)

        # –≠–∫—Å–ø–æ—Ä—Ç
        if export:
            self.export_results(results, export_format=export['format'], output_file=export.get('file'))


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Concordance Search')
    parser.add_argument('query', nargs='?', help='–°–ª–æ–≤–æ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞')
    parser.add_argument('-m', '--mode', choices=['exact', 'fuzzy', 'regex', 'wildcard', 'boolean'],
                       default='exact', help='–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞')
    parser.add_argument('-d', '--display', choices=['list', 'kwic'], default='list',
                       help='–§–æ—Ä–º–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è')
    parser.add_argument('-n', '--max-results', type=int, default=50,
                       help='–ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('-e', '--export', choices=['txt', 'json', 'csv'],
                       help='–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('-o', '--output', help='–§–∞–π–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞')

    args = parser.parse_args()

    if not args.query:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python search_concordance.py <—Å–ª–æ–≤–æ> [–æ–ø—Ü–∏–∏]")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python tools/search_concordance.py docker")
        print("  python tools/search_concordance.py 'docker*' -m wildcard")
        print("  python tools/search_concordance.py 'doc.*' -m regex")
        print("  python tools/search_concordance.py 'docker AND python' -m boolean")
        print("  python tools/search_concordance.py —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ -m fuzzy")
        print("  python tools/search_concordance.py docker -d kwic")
        print("  python tools/search_concordance.py docker -e json -o results.json")
        return

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent
    concordance_file = root_dir / "concordance.json"

    searcher = AdvancedConcordanceSearch(concordance_file)

    export_config = None
    if args.export:
        export_config = {
            'format': args.export,
            'file': args.output
        }

    searcher.search(
        args.query,
        mode=args.mode,
        max_results=args.max_results,
        display=args.display,
        export=export_config
    )


if __name__ == "__main__":
    main()
