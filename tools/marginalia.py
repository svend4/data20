#!/usr/bin/env python3
"""
Marginalia - –ó–∞–º–µ—Ç–∫–∏ –Ω–∞ –ø–æ–ª—è—Ö
–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: –°—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤—ã–µ —Ä—É–∫–æ–ø–∏—Å–∏ —Å –º–∞—Ä–≥–∏–Ω–∞–ª–∏—è–º–∏

–ü–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∑–∞–º–µ—Ç–∫–∏ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫ —Å—Ç–∞—Ç—å—è–º,
–∫–∞–∫ –º–æ–Ω–∞—Ö–∏ –¥–µ–ª–∞–ª–∏ –Ω–∞ –ø–æ–ª—è—Ö –º–∞–Ω—É—Å–∫—Ä–∏–ø—Ç–æ–≤.
"""

from pathlib import Path
import yaml
import re
import json
from datetime import datetime
import argparse
from typing import Dict, List, Set, Tuple
from collections import defaultdict, Counter


class AnnotationExtractor:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ markdown —Ñ–∞–π–ª–æ–≤
    –ü–æ–∏—Å–∫ inline –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, highlights, TODO, FIXME –∏ —Ç.–¥.
    """

    def __init__(self):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        self.patterns = {
            'html_comment': re.compile(r'<!--\s*(.*?)\s*-->', re.DOTALL),
            'todo': re.compile(r'<!--?\s*TODO:?\s*(.*?)(?:-->)?', re.IGNORECASE),
            'fixme': re.compile(r'<!--?\s*FIXME:?\s*(.*?)(?:-->)?', re.IGNORECASE),
            'note': re.compile(r'<!--?\s*NOTE:?\s*(.*?)(?:-->)?', re.IGNORECASE),
            'warning': re.compile(r'<!--?\s*WARNING:?\s*(.*?)(?:-->)?', re.IGNORECASE),
            'highlight': re.compile(r'==([^=]+)=='),  # ==highlighted text==
            'question': re.compile(r'<!--?\s*\?:?\s*(.*?)(?:-->)?', re.IGNORECASE),
        }

    def extract_from_text(self, text: str, file_path: str = None) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ—á—å –≤—Å–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞

        Returns:
            List of annotations with type, text, line_number, context
        """
        annotations = []
        lines = text.split('\n')

        for line_num, line in enumerate(lines, 1):
            for ann_type, pattern in self.patterns.items():
                matches = pattern.finditer(line)

                for match in matches:
                    annotation = {
                        'type': ann_type,
                        'text': match.group(1 if match.lastindex else 0).strip(),
                        'line': line_num,
                        'context': line.strip(),
                        'file': file_path or 'unknown'
                    }

                    annotations.append(annotation)

        return annotations

    def extract_from_file(self, file_path: Path) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()

            return self.extract_from_text(text, str(file_path))

        except Exception as e:
            return []

    def scan_directory(self, directory: Path, pattern: str = "**/*.md") -> Dict[str, List[Dict]]:
        """
        –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –∏–∑–≤–ª–µ—á—å –≤—Å–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏

        Returns:
            {file_path: [annotations]}
        """
        results = {}

        for file_path in directory.glob(pattern):
            if file_path.is_file():
                annotations = self.extract_from_file(file_path)

                if annotations:
                    results[str(file_path)] = annotations

        return results

    def get_statistics(self, annotations_by_file: Dict[str, List[Dict]]) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º"""
        total = sum(len(anns) for anns in annotations_by_file.values())

        by_type = Counter()
        by_file = {}

        for file_path, annotations in annotations_by_file.items():
            by_file[file_path] = len(annotations)

            for ann in annotations:
                by_type[ann['type']] += 1

        return {
            'total': total,
            'files_with_annotations': len(annotations_by_file),
            'by_type': dict(by_type),
            'by_file': by_file
        }


class CrossReferenceBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–µ—Ç–∏ cross-references –º–µ–∂–¥—É –∑–∞–º–µ—Ç–∫–∞–º–∏
    –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    """

    def __init__(self, notes_db: Dict):
        self.notes = notes_db
        self.references = defaultdict(list)  # note_id -> [referenced_note_ids]
        self.backlinks = defaultdict(list)   # note_id -> [notes that reference this]

    def extract_references(self) -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å –≤—Å–µ cross-references –∏–∑ –∑–∞–º–µ—Ç–æ–∫

        –ü–∞—Ç—Ç–µ—Ä–Ω—ã:
        - #123 - —Å—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–º–µ—Ç–∫—É –ø–æ ID
        - @article_name - —Å—Å—ã–ª–∫–∞ –Ω–∞ –¥—Ä—É–≥—É—é —Å—Ç–∞—Ç—å—é
        - [[term]] - —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ç/—Ç–µ—Ä–º–∏–Ω
        """
        ref_patterns = {
            'note_id': re.compile(r'#(\d+)'),
            'article': re.compile(r'@([\w/.-]+)'),
            'concept': re.compile(r'\[\[([^\]]+)\]\]')
        }

        for article_path, notes in self.notes.items():
            for note in notes:
                note_key = f"{article_path}#{note['id']}"
                refs = []

                # –ò–∑–≤–ª–µ—á—å –≤—Å–µ —Ç–∏–ø—ã —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤
                for ref_type, pattern in ref_patterns.items():
                    matches = pattern.findall(note['text'])

                    for match in matches:
                        refs.append({
                            'type': ref_type,
                            'target': match
                        })

                if refs:
                    self.references[note_key] = refs

                    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å backlinks
                    for ref in refs:
                        if ref['type'] == 'note_id':
                            # –ù–∞–π—Ç–∏ —Ü–µ–ª–µ–≤—É—é –∑–∞–º–µ—Ç–∫—É
                            target_key = f"{article_path}#{ref['target']}"
                            self.backlinks[target_key].append(note_key)

        return {
            'references': dict(self.references),
            'backlinks': dict(self.backlinks)
        }

    def build_reference_graph(self) -> Dict:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤

        Returns:
            {
                'nodes': [note_keys],
                'edges': [(from, to)],
                'clusters': [connected_components]
            }
        """
        self.extract_references()

        # Nodes
        nodes = set()
        for note_key in self.references.keys():
            nodes.add(note_key)
        for note_key in self.backlinks.keys():
            nodes.add(note_key)

        # Edges
        edges = []
        for from_note, refs in self.references.items():
            for ref in refs:
                if ref['type'] == 'note_id':
                    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å article_path –∏–∑ from_note
                    article_path = '#'.join(from_note.split('#')[:-1])
                    to_note = f"{article_path}#{ref['target']}"
                    edges.append((from_note, to_note))

        # Find connected components (–ø—Ä–æ—Å—Ç–æ–π DFS)
        visited = set()
        clusters = []

        def dfs(node, cluster):
            visited.add(node)
            cluster.add(node)

            # –ò—Å—Ö–æ–¥—è—â–∏–µ
            for ref in self.references.get(node, []):
                if ref['type'] == 'note_id':
                    article_path = '#'.join(node.split('#')[:-1])
                    target = f"{article_path}#{ref['target']}"
                    if target not in visited:
                        dfs(target, cluster)

            # –í—Ö–æ–¥—è—â–∏–µ
            for backlink in self.backlinks.get(node, []):
                if backlink not in visited:
                    dfs(backlink, cluster)

        for node in nodes:
            if node not in visited:
                cluster = set()
                dfs(node, cluster)
                if cluster:
                    clusters.append(list(cluster))

        return {
            'nodes': list(nodes),
            'edges': edges,
            'clusters': clusters,
            'total_nodes': len(nodes),
            'total_edges': len(edges),
            'total_clusters': len(clusters)
        }

    def find_orphaned_notes(self) -> List[str]:
        """–ù–∞–π—Ç–∏ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏ (–±–µ–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤)"""
        graph = self.build_reference_graph()
        all_notes = set()

        for article_path, notes in self.notes.items():
            for note in notes:
                all_notes.add(f"{article_path}#{note['id']}")

        orphaned = all_notes - set(graph['nodes'])

        return list(orphaned)


class ContextAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫–∏, sentiment, –≤–∞–∂–Ω–æ—Å—Ç–∏
    """

    def __init__(self):
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º–∞—Ç–∏–∫–∏
        self.topic_keywords = {
            'technical': ['algorithm', 'implementation', 'code', 'function', 'class', 'API', 'database'],
            'conceptual': ['theory', 'concept', 'principle', 'philosophy', 'idea', 'paradigm'],
            'practical': ['example', 'tutorial', 'guide', 'howto', 'demo', 'practice'],
            'research': ['paper', 'study', 'research', 'analysis', 'experiment', 'findings'],
            'reference': ['see', 'related', 'link', 'source', 'citation', 'reference']
        }

        # Sentiment keywords
        self.sentiment_keywords = {
            'positive': ['good', 'excellent', 'useful', 'helpful', 'important', 'great', 'best'],
            'negative': ['bad', 'wrong', 'error', 'issue', 'problem', 'bug', 'broken'],
            'neutral': ['note', 'remark', 'comment', 'mention', 'see', 'check']
        }

    def analyze_note(self, note: Dict) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–¥–Ω—É –∑–∞–º–µ—Ç–∫—É

        Returns:
            {
                'topics': [detected_topics],
                'sentiment': 'positive'/'negative'/'neutral',
                'importance': 0-10,
                'keywords': [extracted_keywords]
            }
        """
        text = note['text'].lower()

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–µ–º—ã
        topics = []
        for topic, keywords in self.topic_keywords.items():
            if any(kw in text for kw in keywords):
                topics.append(topic)

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å sentiment
        sentiment_scores = {}
        for sentiment, keywords in self.sentiment_keywords.items():
            sentiment_scores[sentiment] = sum(1 for kw in keywords if kw in text)

        sentiment = max(sentiment_scores.items(), key=lambda x: x[1])[0] if any(sentiment_scores.values()) else 'neutral'

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å
        importance = 5  # baseline

        # –§–∞–∫—Ç–æ—Ä—ã –≤–∞–∂–Ω–æ—Å—Ç–∏
        if note['type'] in ['warning', 'fixme']:
            importance += 3
        elif note['type'] in ['todo', 'question']:
            importance += 2
        elif note['type'] in ['idea']:
            importance += 1

        if len(text) > 100:
            importance += 1  # –¥–ª–∏–Ω–Ω–∞—è –∑–∞–º–µ—Ç–∫–∞ = –≤–∞–∂–Ω–∞—è

        if any(marker in text for marker in ['important', 'critical', 'urgent', 'must']):
            importance += 2

        importance = min(10, importance)  # cap at 10

        # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥)
        words = re.findall(r'\b\w+\b', text)
        word_freq = Counter(w for w in words if len(w) > 4)  # —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤
        keywords = [word for word, count in word_freq.most_common(5)]

        return {
            'topics': topics,
            'sentiment': sentiment,
            'importance': importance,
            'keywords': keywords
        }

    def analyze_all_notes(self, notes_db: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∑–∞–º–µ—Ç–∫–∏ –≤ –±–∞–∑–µ"""
        analysis = {
            'by_article': {},
            'overall': {
                'topics': Counter(),
                'sentiment': Counter(),
                'avg_importance': 0,
                'all_keywords': Counter()
            }
        }

        total_importance = 0
        total_notes = 0

        for article_path, notes in notes_db.items():
            article_analysis = []

            for note in notes:
                note_analysis = self.analyze_note(note)
                article_analysis.append({
                    'note_id': note['id'],
                    **note_analysis
                })

                # Aggregate
                for topic in note_analysis['topics']:
                    analysis['overall']['topics'][topic] += 1

                analysis['overall']['sentiment'][note_analysis['sentiment']] += 1
                total_importance += note_analysis['importance']
                total_notes += 1

                for keyword in note_analysis['keywords']:
                    analysis['overall']['all_keywords'][keyword] += 1

            analysis['by_article'][article_path] = article_analysis

        if total_notes > 0:
            analysis['overall']['avg_importance'] = total_importance / total_notes

        # Convert Counters to dicts
        analysis['overall']['topics'] = dict(analysis['overall']['topics'])
        analysis['overall']['sentiment'] = dict(analysis['overall']['sentiment'])
        analysis['overall']['top_keywords'] = dict(analysis['overall']['all_keywords'].most_common(20))

        return analysis


class VisualizationGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –º–∞—Ä–≥–∏–Ω–∞–ª–∏–π
    HTML —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
    """

    def __init__(self, notes_db: Dict):
        self.notes = notes_db

    def generate_html_overview(self, output_file: Path, include_analysis: bool = True):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML overview –≤—Å–µ—Ö –º–∞—Ä–≥–∏–Ω–∞–ª–∏–π"""

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_notes = sum(len(notes) for notes in self.notes.values())
        total_articles = len(self.notes)

        unresolved_count = 0
        for notes in self.notes.values():
            unresolved_count += sum(1 for note in notes if not note.get('resolved', False))

        # Type distribution
        type_counts = Counter()
        for notes in self.notes.values():
            for note in notes:
                type_counts[note['type']] += 1

        html = """<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Marginalia Overview</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        h1 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 2.5em;
        }
        .subtitle {
            color: #666;
            margin-bottom: 30px;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 40px;
        }
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
        }
        .stat-card h3 {
            font-size: 0.9em;
            opacity: 0.9;
            margin-bottom: 8px;
        }
        .stat-card .value {
            font-size: 2em;
            font-weight: bold;
        }
        .article-section {
            margin-bottom: 30px;
            border: 1px solid #eee;
            border-radius: 8px;
            padding: 20px;
            background: #f8f9fa;
        }
        .article-title {
            color: #333;
            font-size: 1.2em;
            margin-bottom: 15px;
            font-weight: 600;
        }
        .note-card {
            background: white;
            padding: 15px;
            margin-bottom: 10px;
            border-left: 4px solid #667eea;
            border-radius: 4px;
        }
        .note-card.comment { border-left-color: #667eea; }
        .note-card.warning { border-left-color: #ffc107; }
        .note-card.idea { border-left-color: #28a745; }
        .note-card.question { border-left-color: #17a2b8; }
        .note-card.todo { border-left-color: #ff5722; }
        .note-header {
            display: flex;
            justify-content: space-between;
            margin-bottom: 8px;
            font-size: 0.85em;
            color: #666;
        }
        .note-text {
            color: #333;
        }
        .badge {
            display: inline-block;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 0.75em;
            font-weight: bold;
        }
        .badge.comment { background: #667eea; color: white; }
        .badge.warning { background: #ffc107; color: #333; }
        .badge.idea { background: #28a745; color: white; }
        .badge.question { background: #17a2b8; color: white; }
        .badge.todo { background: #ff5722; color: white; }
        .resolved {
            opacity: 0.6;
        }
        .chart {
            margin: 20px 0;
        }
        .chart-bar {
            display: flex;
            align-items: center;
            margin-bottom: 8px;
        }
        .chart-label {
            width: 120px;
            font-size: 0.9em;
        }
        .chart-bar-fill {
            height: 24px;
            background: linear-gradient(90deg, #667eea, #764ba2);
            border-radius: 4px;
            color: white;
            padding: 0 10px;
            font-size: 0.85em;
            display: flex;
            align-items: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üìù Marginalia Overview</h1>
        <p class="subtitle">–ó–∞–º–µ—Ç–∫–∏ –Ω–∞ –ø–æ–ª—è—Ö –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π</p>

        <div class="stats">
            <div class="stat-card">
                <h3>Total Notes</h3>
                <div class="value">""" + str(total_notes) + """</div>
            </div>
            <div class="stat-card">
                <h3>Articles</h3>
                <div class="value">""" + str(total_articles) + """</div>
            </div>
            <div class="stat-card">
                <h3>Unresolved</h3>
                <div class="value">""" + str(unresolved_count) + """</div>
            </div>
            <div class="stat-card">
                <h3>Avg per Article</h3>
                <div class="value">""" + f"{total_notes/total_articles:.1f}" if total_articles > 0 else "0" + """</div>
            </div>
        </div>

        <h2 style="margin-bottom: 20px;">Distribution by Type</h2>
        <div class="chart">
"""

        max_count = max(type_counts.values()) if type_counts else 1

        for note_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):
            width_pct = (count / max_count) * 100

            html += f"""            <div class="chart-bar">
                <div class="chart-label">{note_type}</div>
                <div class="chart-bar-fill" style="width: {width_pct}%;">{count}</div>
            </div>
"""

        html += """        </div>

        <h2 style="margin: 40px 0 20px;">Notes by Article</h2>
"""

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–º–µ—Ç–æ–∫
        sorted_articles = sorted(self.notes.items(), key=lambda x: -len(x[1]))

        for article_path, notes in sorted_articles[:10]:  # Top 10
            html += f"""        <div class="article-section">
            <div class="article-title">{article_path} ({len(notes)} notes)</div>
"""

            for note in notes[:5]:  # Show first 5 notes
                resolved_class = "resolved" if note.get('resolved') else ""

                html += f"""            <div class="note-card {note['type']} {resolved_class}">
                <div class="note-header">
                    <span>
                        <span class="badge {note['type']}">{note['type']}</span>
                        #{note['id']} @ {note['position']}
                    </span>
                    <span>{note['author']} ¬∑ {note['date'][:10]}</span>
                </div>
                <div class="note-text">{note['text'][:150]}{'...' if len(note['text']) > 150 else ''}</div>
            </div>
"""

            html += """        </div>
"""

        html += """    </div>
</body>
</html>"""

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)


class MarginaliaManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –º–∞—Ä–≥–∏–Ω–∞–ª–∏–π - –∑–∞–º–µ—Ç–æ–∫ –Ω–∞ –ø–æ–ª—è—Ö —Å—Ç–∞—Ç–µ–π
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.marginalia_db = self.root_dir / ".marginalia" / "notes.json"

        # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.marginalia_db.parent.mkdir(exist_ok=True)

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–∞—Ä–≥–∏–Ω–∞–ª–∏–∏
        self.notes = self.load_notes()

    def load_notes(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –º–∞—Ä–≥–∏–Ω–∞–ª–∏–∏"""
        if self.marginalia_db.exists():
            with open(self.marginalia_db, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_notes(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞—Ä–≥–∏–Ω–∞–ª–∏–∏"""
        with open(self.marginalia_db, 'w', encoding='utf-8') as f:
            json.dump(self.notes, f, ensure_ascii=False, indent=2)

    def add_note(self, article_file, note_text, position=None, note_type="comment", author="User"):
        """
        –î–æ–±–∞–≤–∏—Ç—å –∑–∞–º–µ—Ç–∫—É –∫ —Å—Ç–∞—Ç—å–µ

        position –º–æ–∂–µ—Ç –±—ã—Ç—å:
        - "line:42" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        - "section:Introduction" - —Å–µ–∫—Ü–∏—è
        - "paragraph:3" - –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        - None - –æ–±—â–∞—è –∑–∞–º–µ—Ç–∫–∞ –∫ —Å—Ç–∞—Ç—å–µ
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—É—Ç—å
        article_path = str(Path(article_file).relative_to(self.root_dir))

        if article_path not in self.notes:
            self.notes[article_path] = []

        note = {
            'id': len(self.notes[article_path]) + 1,
            'text': note_text,
            'position': position or "general",
            'type': note_type,  # comment, warning, idea, question, cross-reference
            'author': author,
            'date': datetime.now().isoformat(),
            'resolved': False
        }

        self.notes[article_path].append(note)
        self.save_notes()

        return note

    def get_notes(self, article_file):
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∑–∞–º–µ—Ç–∫–∏ –¥–ª—è —Å—Ç–∞—Ç—å–∏"""
        article_path = str(Path(article_file).relative_to(self.root_dir))
        return self.notes.get(article_path, [])

    def update_note(self, article_file, note_id, **updates):
        """–û–±–Ω–æ–≤–∏—Ç—å –∑–∞–º–µ—Ç–∫—É"""
        article_path = str(Path(article_file).relative_to(self.root_dir))

        if article_path not in self.notes:
            return None

        for note in self.notes[article_path]:
            if note['id'] == note_id:
                note.update(updates)
                note['modified'] = datetime.now().isoformat()
                self.save_notes()
                return note

        return None

    def delete_note(self, article_file, note_id):
        """–£–¥–∞–ª–∏—Ç—å –∑–∞–º–µ—Ç–∫—É"""
        article_path = str(Path(article_file).relative_to(self.root_dir))

        if article_path not in self.notes:
            return False

        self.notes[article_path] = [
            note for note in self.notes[article_path]
            if note['id'] != note_id
        ]

        self.save_notes()
        return True

    def mark_resolved(self, article_file, note_id):
        """–û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–º–µ—Ç–∫—É –∫–∞–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—É—é"""
        return self.update_note(article_file, note_id, resolved=True)

    def get_all_notes_by_type(self, note_type=None):
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∑–∞–º–µ—Ç–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞"""
        all_notes = []

        for article_path, notes in self.notes.items():
            for note in notes:
                if note_type is None or note['type'] == note_type:
                    all_notes.append({
                        'article': article_path,
                        **note
                    })

        return all_notes

    def get_unresolved_notes(self):
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –Ω–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏"""
        unresolved = []

        for article_path, notes in self.notes.items():
            for note in notes:
                if not note.get('resolved', False):
                    unresolved.append({
                        'article': article_path,
                        **note
                    })

        return unresolved

    def export_to_markdown(self, article_file, output_file=None):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ä–≥–∏–Ω–∞–ª–∏–∏ —Å—Ç–∞—Ç—å–∏ –≤ markdown"""
        notes = self.get_notes(article_file)

        if not notes:
            return None

        lines = []
        lines.append(f"# üìù –ú–∞—Ä–≥–∏–Ω–∞–ª–∏–∏: {Path(article_file).name}\n\n")
        lines.append(f"> –ó–∞–º–µ—Ç–∫–∏ –Ω–∞ –ø–æ–ª—è—Ö –¥–ª—è —Å—Ç–∞—Ç—å–∏ `{article_file}`\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø—É
        by_type = {}
        for note in notes:
            note_type = note['type']
            if note_type not in by_type:
                by_type[note_type] = []
            by_type[note_type].append(note)

        # –ò–∫–æ–Ω–∫–∏ –¥–ª—è —Ç–∏–ø–æ–≤
        type_icons = {
            'comment': 'üí¨',
            'warning': '‚ö†Ô∏è',
            'idea': 'üí°',
            'question': '‚ùì',
            'cross-reference': 'üîó',
            'todo': '‚úÖ'
        }

        for note_type, type_notes in sorted(by_type.items()):
            icon = type_icons.get(note_type, 'üìå')
            lines.append(f"## {icon} {note_type.title()}\n\n")

            for note in type_notes:
                status = "‚úì" if note.get('resolved') else "‚óã"
                lines.append(f"### {status} –ó–∞–º–µ—Ç–∫–∞ #{note['id']}\n\n")
                lines.append(f"**–ü–æ–∑–∏—Ü–∏—è**: {note['position']}  \n")
                lines.append(f"**–ê–≤—Ç–æ—Ä**: {note['author']}  \n")
                lines.append(f"**–î–∞—Ç–∞**: {note['date'][:10]}  \n")
                if note.get('resolved'):
                    lines.append(f"**–°—Ç–∞—Ç—É—Å**: –†–∞–∑—Ä–µ—à–µ–Ω–æ ‚úì  \n")
                lines.append(f"\n{note['text']}\n\n")
                lines.append("---\n\n")

        content = ''.join(lines)

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)

        return content

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ–±—â–∏–π –æ—Ç—á—ë—Ç –ø–æ –≤—Å–µ–º –º–∞—Ä–≥–∏–Ω–∞–ª–∏—è–º"""
        lines = []
        lines.append("# üìù –û—Ç—á—ë—Ç –ø–æ –º–∞—Ä–≥–∏–Ω–∞–ª–∏—è–º\n\n")

        total_notes = sum(len(notes) for notes in self.notes.values())
        total_articles = len(self.notes)
        unresolved = len(self.get_unresolved_notes())

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ –∑–∞–º–µ—Ç–æ–∫**: {total_notes}\n")
        lines.append(f"- **–°—Ç–∞—Ç–µ–π —Å –∑–∞–º–µ—Ç–∫–∞–º–∏**: {total_articles}\n")
        lines.append(f"- **–ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫**: {unresolved}\n\n")

        # –ü–æ —Ç–∏–ø–∞–º
        by_type = {}
        for notes in self.notes.values():
            for note in notes:
                note_type = note['type']
                by_type[note_type] = by_type.get(note_type, 0) + 1

        lines.append("## –ü–æ —Ç–∏–ø–∞–º\n\n")
        for note_type, count in sorted(by_type.items(), key=lambda x: -x[1]):
            lines.append(f"- **{note_type}**: {count}\n")

        lines.append("\n## –ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏\n\n")
        unresolved_notes = self.get_unresolved_notes()

        if unresolved_notes:
            for note in unresolved_notes[:20]:
                lines.append(f"### {note['article']}\n\n")
                lines.append(f"- **ID**: #{note['id']}\n")
                lines.append(f"- **–¢–∏–ø**: {note['type']}\n")
                lines.append(f"- **–ü–æ–∑–∏—Ü–∏—è**: {note['position']}\n")
                lines.append(f"- **–¢–µ–∫—Å—Ç**: {note['text'][:100]}...\n\n")
        else:
            lines.append("–í—Å–µ –∑–∞–º–µ—Ç–∫–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã! üéâ\n\n")

        # –¢–æ–ø —Å—Ç–∞—Ç–µ–π —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–º–µ—Ç–æ–∫
        lines.append("\n## –°—Ç–∞—Ç—å–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–º–µ—Ç–æ–∫\n\n")

        article_counts = [(article, len(notes)) for article, notes in self.notes.items()]
        article_counts.sort(key=lambda x: -x[1])

        for article, count in article_counts[:10]:
            lines.append(f"- **{article}**: {count} –∑–∞–º–µ—Ç–æ–∫\n")

        return ''.join(lines)

    def print_notes(self, article_file=None):
        """–í—ã–≤–µ—Å—Ç–∏ –∑–∞–º–µ—Ç–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        if article_file:
            notes = self.get_notes(article_file)
            print(f"\nüìù –ú–∞—Ä–≥–∏–Ω–∞–ª–∏–∏ –¥–ª—è {article_file}:\n")

            if not notes:
                print("   –ó–∞–º–µ—Ç–æ–∫ –Ω–µ—Ç\n")
                return

            for note in notes:
                status = "‚úì" if note.get('resolved') else "‚óã"
                print(f"{status} #{note['id']} [{note['type']}] @ {note['position']}")
                print(f"   {note['text']}")
                print(f"   ‚Äî {note['author']}, {note['date'][:10]}\n")
        else:
            # –í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∑–∞–º–µ—Ç–∫–∞–º
            total = sum(len(notes) for notes in self.notes.values())
            unresolved = len(self.get_unresolved_notes())

            print(f"\nüìù –í—Å–µ–≥–æ –º–∞—Ä–≥–∏–Ω–∞–ª–∏–π: {total}")
            print(f"   –ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö: {unresolved}")
            print(f"   –°—Ç–∞—Ç–µ–π —Å –∑–∞–º–µ—Ç–∫–∞–º–∏: {len(self.notes)}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Marginalia - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–º–µ—Ç–∫–∞–º–∏ –Ω–∞ –ø–æ–ª—è—Ö —Å—Ç–∞—Ç–µ–π'
    )

    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')

    # add - –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–º–µ—Ç–∫—É
    add_parser = subparsers.add_parser('add', help='–î–æ–±–∞–≤–∏—Ç—å –∑–∞–º–µ—Ç–∫—É')
    add_parser.add_argument('article', help='–ü—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ')
    add_parser.add_argument('text', help='–¢–µ–∫—Å—Ç –∑–∞–º–µ—Ç–∫–∏')
    add_parser.add_argument('-p', '--position', help='–ü–æ–∑–∏—Ü–∏—è (line:N, section:Name)')
    add_parser.add_argument('-t', '--type', default='comment',
                           choices=['comment', 'warning', 'idea', 'question', 'cross-reference', 'todo'],
                           help='–¢–∏–ø –∑–∞–º–µ—Ç–∫–∏')
    add_parser.add_argument('-a', '--author', default='User', help='–ê–≤—Ç–æ—Ä')

    # list - –ø–æ–∫–∞–∑–∞—Ç—å –∑–∞–º–µ—Ç–∫–∏
    list_parser = subparsers.add_parser('list', help='–ü–æ–∫–∞–∑–∞—Ç—å –∑–∞–º–µ—Ç–∫–∏')
    list_parser.add_argument('article', nargs='?', help='–ü—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    list_parser.add_argument('-t', '--type', help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É')
    list_parser.add_argument('-u', '--unresolved', action='store_true', help='–¢–æ–ª—å–∫–æ –Ω–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ')

    # resolve - –æ—Ç–º–µ—Ç–∏—Ç—å –∫–∞–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—É—é
    resolve_parser = subparsers.add_parser('resolve', help='–û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–º–µ—Ç–∫—É –∫–∞–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—É—é')
    resolve_parser.add_argument('article', help='–ü—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ')
    resolve_parser.add_argument('note_id', type=int, help='ID –∑–∞–º–µ—Ç–∫–∏')

    # delete - —É–¥–∞–ª–∏—Ç—å –∑–∞–º–µ—Ç–∫—É
    delete_parser = subparsers.add_parser('delete', help='–£–¥–∞–ª–∏—Ç—å –∑–∞–º–µ—Ç–∫—É')
    delete_parser.add_argument('article', help='–ü—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ')
    delete_parser.add_argument('note_id', type=int, help='ID –∑–∞–º–µ—Ç–∫–∏')

    # export - —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
    export_parser = subparsers.add_parser('export', help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–º–µ—Ç–∫–∏')
    export_parser.add_argument('article', help='–ü—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ')
    export_parser.add_argument('-o', '--output', help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')

    # report - –æ—Ç—á—ë—Ç
    subparsers.add_parser('report', help='–°–æ–∑–¥–∞—Ç—å –æ–±—â–∏–π –æ—Ç—á—ë—Ç')

    # scan - —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –Ω–∞ inline –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    scan_parser = subparsers.add_parser('scan', help='–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –Ω–∞ inline –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏')
    scan_parser.add_argument('-d', '--directory', default='knowledge', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è')
    scan_parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É')

    # cross-ref - –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ cross-references
    crossref_parser = subparsers.add_parser('cross-ref', help='–ê–Ω–∞–ª–∏–∑ cross-references –º–µ–∂–¥—É –∑–∞–º–µ—Ç–∫–∞–º–∏')
    crossref_parser.add_argument('--orphaned', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏')
    crossref_parser.add_argument('--graph', action='store_true', help='–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤')

    # analyze - –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    analyze_parser = subparsers.add_parser('analyze', help='–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–º–µ—Ç–æ–∫')
    analyze_parser.add_argument('-t', '--topics', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–º—ã')
    analyze_parser.add_argument('-s', '--sentiment', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å sentiment')
    analyze_parser.add_argument('--importance', action='store_true', help='–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏')

    # visualize - HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    visualize_parser = subparsers.add_parser('visualize', help='–°–æ–∑–¥–∞—Ç—å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é')
    visualize_parser.add_argument('-o', '--output', default='marginalia_overview.html', help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')

    # all - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    all_parser = subparsers.add_parser('all', help='–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ + –≤—Å–µ —ç–∫—Å–ø–æ—Ä—Ç—ã')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    manager = MarginaliaManager(root_dir)

    if args.command == 'add':
        article_path = root_dir / args.article
        note = manager.add_note(
            article_path,
            args.text,
            position=args.position,
            note_type=args.type,
            author=args.author
        )
        print(f"‚úÖ –ó–∞–º–µ—Ç–∫–∞ #{note['id']} –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∫ {args.article}")

    elif args.command == 'list':
        if args.article:
            article_path = root_dir / args.article
            manager.print_notes(article_path)
        elif args.unresolved:
            notes = manager.get_unresolved_notes()
            print(f"\n‚ö†Ô∏è  –ù–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫: {len(notes)}\n")
            for note in notes:
                print(f"#{note['id']} {note['article']} @ {note['position']}")
                print(f"   {note['text'][:80]}...\n")
        elif args.type:
            notes = manager.get_all_notes_by_type(args.type)
            print(f"\nüìù –ó–∞–º–µ—Ç–æ–∫ —Ç–∏–ø–∞ '{args.type}': {len(notes)}\n")
            for note in notes:
                print(f"#{note['id']} {note['article']}")
                print(f"   {note['text'][:80]}...\n")
        else:
            manager.print_notes()

    elif args.command == 'resolve':
        article_path = root_dir / args.article
        if manager.mark_resolved(article_path, args.note_id):
            print(f"‚úÖ –ó–∞–º–µ—Ç–∫–∞ #{args.note_id} –æ—Ç–º–µ—á–µ–Ω–∞ –∫–∞–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω–∞—è")
        else:
            print(f"‚ùå –ó–∞–º–µ—Ç–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    elif args.command == 'delete':
        article_path = root_dir / args.article
        if manager.delete_note(article_path, args.note_id):
            print(f"‚úÖ –ó–∞–º–µ—Ç–∫–∞ #{args.note_id} —É–¥–∞–ª–µ–Ω–∞")
        else:
            print(f"‚ùå –ó–∞–º–µ—Ç–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    elif args.command == 'export':
        article_path = root_dir / args.article
        output = args.output or f"{article_path.stem}_marginalia.md"
        content = manager.export_to_markdown(article_path, output)
        if content:
            print(f"‚úÖ –ú–∞—Ä–≥–∏–Ω–∞–ª–∏–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output}")
        else:
            print(f"‚ö†Ô∏è  –ù–µ—Ç –∑–∞–º–µ—Ç–æ–∫ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")

    elif args.command == 'report':
        report = manager.generate_report()
        output_file = root_dir / "MARGINALIA_REPORT.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω: {output_file}")
        print(report)

    elif args.command == 'scan':
        print(f"\nüîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {args.directory}...\n")

        extractor = AnnotationExtractor()
        scan_dir = root_dir / args.directory
        results = extractor.scan_directory(scan_dir)

        if args.stats:
            stats = extractor.get_statistics(results)

            print(f"–í—Å–µ–≥–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {stats['total']}")
            print(f"–§–∞–π–ª–æ–≤ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏: {stats['files_with_annotations']}\n")

            print("–ü–æ —Ç–∏–ø–∞–º:")
            for ann_type, count in sorted(stats['by_type'].items(), key=lambda x: -x[1]):
                print(f"  {ann_type:15s}: {count}")

            print("\n–¢–æ–ø —Ñ–∞–π–ª–æ–≤:")
            top_files = sorted(stats['by_file'].items(), key=lambda x: -x[1])[:10]
            for file_path, count in top_files:
                print(f"  {count:3d} - {file_path}")

        else:
            # –í—ã–≤–µ—Å—Ç–∏ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            for file_path, annotations in results.items():
                print(f"\nüìÑ {file_path}:")
                for ann in annotations:
                    print(f"  Line {ann['line']}: [{ann['type']}] {ann['text'][:80]}")

        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {sum(len(anns) for anns in results.values())}")

    elif args.command == 'cross-ref':
        print("\nüîó –ê–Ω–∞–ª–∏–∑ cross-references...\n")

        builder = CrossReferenceBuilder(manager.notes)
        graph = builder.build_reference_graph()

        if args.orphaned:
            orphaned = builder.find_orphaned_notes()
            print(f"–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫: {len(orphaned)}\n")

            for note_key in orphaned[:20]:
                print(f"  {note_key}")

        elif args.graph:
            print(f"–ì—Ä–∞—Ñ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤:")
            print(f"  –£–∑–ª–æ–≤: {graph['total_nodes']}")
            print(f"  –†—ë–±–µ—Ä: {graph['total_edges']}")
            print(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {graph['total_clusters']}\n")

            print("–ö–ª–∞—Å—Ç–µ—Ä—ã:")
            for i, cluster in enumerate(graph['clusters'][:5], 1):
                print(f"\n  –ö–ª–∞—Å—Ç–µ—Ä {i} ({len(cluster)} –∑–∞–º–µ—Ç–æ–∫):")
                for note in cluster[:5]:
                    print(f"    - {note}")

        else:
            refs = builder.extract_references()
            print(f"References: {len(refs['references'])}")
            print(f"Backlinks: {len(refs['backlinks'])}\n")

            print("–ó–∞–º–µ—Ç–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤:")
            top_refs = sorted(
                [(k, len(v)) for k, v in refs['references'].items()],
                key=lambda x: -x[1]
            )[:10]

            for note_key, ref_count in top_refs:
                print(f"  {note_key}: {ref_count} refs")

    elif args.command == 'analyze':
        print("\nüìä –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–º–µ—Ç–æ–∫...\n")

        analyzer = ContextAnalyzer()
        analysis = analyzer.analyze_all_notes(manager.notes)

        if args.topics:
            print("–¢–µ–º—ã:")
            for topic, count in sorted(analysis['overall']['topics'].items(), key=lambda x: -x[1]):
                print(f"  {topic:15s}: {count}")

        elif args.sentiment:
            print("Sentiment:")
            for sentiment, count in sorted(analysis['overall']['sentiment'].items(), key=lambda x: -x[1]):
                print(f"  {sentiment:10s}: {count}")

        elif args.importance:
            print(f"–°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {analysis['overall']['avg_importance']:.1f}/10\n")

            # –ù–∞–π—Ç–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏
            all_notes_with_importance = []

            for article_path, notes in manager.notes.items():
                for note in notes:
                    note_analysis = analyzer.analyze_note(note)
                    all_notes_with_importance.append({
                        'article': article_path,
                        'id': note['id'],
                        'importance': note_analysis['importance'],
                        'text': note['text']
                    })

            all_notes_with_importance.sort(key=lambda x: -x['importance'])

            print("–¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫:\n")
            for i, note in enumerate(all_notes_with_importance[:10], 1):
                print(f"{i}. {note['article']}#{note['id']} (–≤–∞–∂–Ω–æ—Å—Ç—å: {note['importance']}/10)")
                print(f"   {note['text'][:100]}...\n")

        else:
            # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            print(f"–°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {analysis['overall']['avg_importance']:.1f}/10\n")

            print("–¢–æ–ø —Ç–µ–º—ã:")
            for topic, count in sorted(analysis['overall']['topics'].items(), key=lambda x: -x[1])[:5]:
                print(f"  {topic}: {count}")

            print("\nSentiment:")
            for sentiment, count in sorted(analysis['overall']['sentiment'].items(), key=lambda x: -x[1]):
                print(f"  {sentiment}: {count}")

            print("\n–¢–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
            for keyword, count in list(analysis['overall']['top_keywords'].items())[:10]:
                print(f"  {keyword}: {count}")

    elif args.command == 'visualize':
        print(f"\nüé® –°–æ–∑–¥–∞–Ω–∏–µ HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...\n")

        visualizer = VisualizationGenerator(manager.notes)
        output_path = root_dir / args.output
        visualizer.generate_html_overview(output_path)

        print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {output_path}")

    elif args.command == 'all':
        print("\n" + "="*60)
        print("–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –ú–ê–†–ì–ò–ù–ê–õ–ò–ô")
        print("="*60)

        # 1. Scan inline annotations
        print("\nüîç 1. –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ inline –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        extractor = AnnotationExtractor()
        scan_dir = root_dir / "knowledge"
        results = extractor.scan_directory(scan_dir)
        stats = extractor.get_statistics(results)
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {stats['total']} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ {stats['files_with_annotations']} —Ñ–∞–π–ª–∞—Ö")

        # 2. Cross-reference analysis
        print("\nüîó 2. –ê–Ω–∞–ª–∏–∑ cross-references...")
        builder = CrossReferenceBuilder(manager.notes)
        graph = builder.build_reference_graph()
        orphaned = builder.find_orphaned_notes()
        print(f"   –ì—Ä–∞—Ñ: {graph['total_nodes']} —É–∑–ª–æ–≤, {graph['total_edges']} —Ä—ë–±–µ—Ä, {graph['total_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        print(f"   –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {len(orphaned)}")

        # 3. Context analysis
        print("\nüìä 3. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
        analyzer = ContextAnalyzer()
        analysis = analyzer.analyze_all_notes(manager.notes)
        print(f"   –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {analysis['overall']['avg_importance']:.1f}/10")
        print(f"   –¢–µ–º—ã: {len(analysis['overall']['topics'])}")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {len(analysis['overall']['top_keywords'])}")

        # 4. Generate reports
        print("\nüìù 4. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤...")

        # Markdown report
        report = manager.generate_report()
        report_file = root_dir / "MARGINALIA_REPORT.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"   ‚úÖ Markdown: {report_file}")

        # HTML visualization
        visualizer = VisualizationGenerator(manager.notes)
        html_file = root_dir / "marginalia_overview.html"
        visualizer.generate_html_overview(html_file)
        print(f"   ‚úÖ HTML: {html_file}")

        print("\n‚ú® –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
        print("\nüí° –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print(f"   - {report_file}")
        print(f"   - {html_file}")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
