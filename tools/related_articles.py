#!/usr/bin/env python3
"""
Related Articles - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π
–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Ç–µ–≥–æ–≤ –∏ —Å–≤—è–∑–µ–π

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ Amazon, YouTube
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import json
import math
import argparse
from typing import Dict, List, Tuple


class RelatedArticlesEngine:
    """–î–≤–∏–∂–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–µ–π
        self.articles = {}
        self.links = defaultdict(lambda: {'outgoing': [], 'incoming': []})

        # TF-IDF –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.word_counts = defaultdict(lambda: defaultdict(int))
        self.document_freq = defaultdict(int)

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def tokenize(self, text):
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
        # –£–¥–∞–ª–∏—Ç—å markdown —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
        text = re.sub(r'[#*`\[\]()]', ' ', text)
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())
        return words

    def build_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å"""
        print("üéØ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...\n")

        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))

            self.articles[article_path] = {
                'title': frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem,
                'tags': frontmatter.get('tags', []) if frontmatter else [],
                'category': frontmatter.get('category', '') if frontmatter else '',
                'subcategory': frontmatter.get('subcategory', '') if frontmatter else '',
                'difficulty': frontmatter.get('difficulty', '—Å—Ä–µ–¥–Ω–∏–π') if frontmatter else '—Å—Ä–µ–¥–Ω–∏–π'
            }

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            words = self.tokenize(content)
            unique_words = set(words)

            for word in words:
                self.word_counts[article_path][word] += 1

            for word in unique_words:
                self.document_freq[word] += 1

            # –ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)

            for text, link in links:
                if link.startswith('http'):
                    continue

                try:
                    target = (md_file.parent / link.split('#')[0]).resolve()

                    if target.exists() and target.is_relative_to(self.root_dir):
                        target_path = str(target.relative_to(self.root_dir))

                        if target_path not in self.links[article_path]['outgoing']:
                            self.links[article_path]['outgoing'].append(target_path)

                        if article_path not in self.links[target_path]['incoming']:
                            self.links[target_path]['incoming'].append(article_path)
                except:
                    pass

        print(f"   –°—Ç–∞—Ç–µ–π –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(self.articles)}\n")

    def calculate_tf_idf_similarity(self, article1, article2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        if article1 not in self.word_counts or article2 not in self.word_counts:
            return 0.0

        words1 = self.word_counts[article1]
        words2 = self.word_counts[article2]

        # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        all_words = set(words1.keys()) | set(words2.keys())

        # TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
        vector1 = []
        vector2 = []

        total_docs = len(self.articles)

        for word in all_words:
            # TF-IDF –¥–ª—è article1
            tf1 = words1.get(word, 0)
            idf = math.log(total_docs / (1 + self.document_freq[word]))
            vector1.append(tf1 * idf)

            # TF-IDF –¥–ª—è article2
            tf2 = words2.get(word, 0)
            vector2.append(tf2 * idf)

        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        dot_product = sum(v1 * v2 for v1, v2 in zip(vector1, vector2))
        magnitude1 = math.sqrt(sum(v ** 2 for v in vector1))
        magnitude2 = math.sqrt(sum(v ** 2 for v in vector2))

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

    def calculate_tag_similarity(self, article1, article2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ (Jaccard)"""
        tags1 = set(self.articles[article1]['tags'])
        tags2 = set(self.articles[article2]['tags'])

        if not tags1 or not tags2:
            return 0.0

        intersection = len(tags1 & tags2)
        union = len(tags1 | tags2)

        return intersection / union if union > 0 else 0.0

    def calculate_link_score(self, article1, article2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ—Ü–µ–Ω–∫—É —Å–≤—è–∑–µ–π"""
        score = 0.0

        # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞
        if article2 in self.links[article1]['outgoing']:
            score += 1.0

        # –û–±—Ä–∞—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞
        if article2 in self.links[article1]['incoming']:
            score += 0.5

        # –û–±—â–∏–µ –≤—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏
        incoming1 = set(self.links[article1]['incoming'])
        incoming2 = set(self.links[article2]['incoming'])
        common_incoming = len(incoming1 & incoming2)
        if common_incoming > 0:
            score += 0.3 * common_incoming

        # –û–±—â–∏–µ –∏—Å—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏
        outgoing1 = set(self.links[article1]['outgoing'])
        outgoing2 = set(self.links[article2]['outgoing'])
        common_outgoing = len(outgoing1 & outgoing2)
        if common_outgoing > 0:
            score += 0.2 * common_outgoing

        return score

    def calculate_similarity(self, article1, article2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        if article1 == article2:
            return 0.0

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        content_sim = self.calculate_tf_idf_similarity(article1, article2)
        tag_sim = self.calculate_tag_similarity(article1, article2)
        link_score = self.calculate_link_score(article1, article2)

        # –ë–æ–Ω—É—Å –∑–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        category_bonus = 0.0
        if self.articles[article1]['category'] == self.articles[article2]['category']:
            category_bonus = 0.2

        # –ë–æ–Ω—É—Å –∑–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
        subcategory_bonus = 0.0
        if (self.articles[article1]['subcategory'] and
            self.articles[article1]['subcategory'] == self.articles[article2]['subcategory']):
            subcategory_bonus = 0.3

        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        total = (
            content_sim * 0.3 +
            tag_sim * 0.4 +
            link_score * 0.2 +
            category_bonus +
            subcategory_bonus
        )

        return total

    def get_recommendations(self, article_path, limit=5):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å—Ç–∞—Ç—å–∏"""
        if article_path not in self.articles:
            return []

        similarities = []

        for other_article in self.articles:
            if other_article == article_path:
                continue

            score = self.calculate_similarity(article_path, other_article)
            similarities.append((other_article, score))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        similarities.sort(key=lambda x: -x[1])

        return similarities[:limit]

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π\n\n")
        lines.append("> –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Ç–µ–≥–æ–≤ –∏ —Å–≤—è–∑–µ–π\n\n")

        # –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ - —Ç–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        for article_path in sorted(self.articles.keys()):
            title = self.articles[article_path]['title']
            recommendations = self.get_recommendations(article_path, limit=5)

            if not recommendations:
                continue

            lines.append(f"## {title}\n\n")
            lines.append(f"`{article_path}`\n\n")
            lines.append("**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å:**\n\n")

            for i, (rec_path, score) in enumerate(recommendations, 1):
                rec_title = self.articles[rec_path]['title']
                lines.append(f"{i}. [{rec_title}]({rec_path}) ‚Äî *–æ—Ü–µ–Ω–∫–∞: {score:.2f}*\n")

                # –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—á–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º
                reasons = []
                tag_sim = self.calculate_tag_similarity(article_path, rec_path)
                if tag_sim > 0.3:
                    reasons.append(f"–ø–æ—Ö–æ–∂–∏–µ —Ç–µ–≥–∏ ({tag_sim:.0%})")

                if rec_path in self.links[article_path]['outgoing']:
                    reasons.append("–≤—ã —Å—Å—ã–ª–∞–µ—Ç–µ—Å—å –Ω–∞ —ç—Ç—É —Å—Ç–∞—Ç—å—é")

                if rec_path in self.links[article_path]['incoming']:
                    reasons.append("—ç—Ç–∞ —Å—Ç–∞—Ç—å—è —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –≤–∞—Å")

                if self.articles[article_path]['category'] == self.articles[rec_path]['category']:
                    reasons.append("—Ç–∞ –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è")

                if reasons:
                    lines.append(f"   *{', '.join(reasons)}*\n")

            lines.append("\n")

        output_file = self.root_dir / "RELATED_ARTICLES.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ JSON"""
        data = {}

        for article_path in self.articles:
            recommendations = self.get_recommendations(article_path, limit=10)

            data[article_path] = {
                'title': self.articles[article_path]['title'],
                'recommendations': [
                    {
                        'article': rec_path,
                        'title': self.articles[rec_path]['title'],
                        'score': score
                    }
                    for rec_path, score in recommendations
                ]
            }

        output_file = self.root_dir / "related_articles.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {output_file}")

    def get_popular_articles(self, limit: int = 10) -> List[Tuple[str, int]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (–ø–æ –≤—Ö–æ–¥—è—â–∏–º —Å—Å—ã–ª–∫–∞–º)"""
        popularity = []

        for article_path in self.articles:
            incoming_count = len(self.links[article_path]['incoming'])
            popularity.append((article_path, incoming_count))

        return sorted(popularity, key=lambda x: x[1], reverse=True)[:limit]

    def get_trending_articles_by_tags(self, top_n: int = 10) -> List[Tuple[str, int]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—å–∏ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏"""
        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É —Ç–µ–≥–æ–≤
        tag_counts = Counter()
        for article in self.articles.values():
            for tag in article['tags']:
                tag_counts[tag] += 1

        # –û—Ü–µ–Ω–∏—Ç—å —Å—Ç–∞—Ç—å–∏ –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∏—Ö —Ç–µ–≥–æ–≤
        article_scores = []

        for article_path, article_data in self.articles.items():
            score = sum(tag_counts[tag] for tag in article_data['tags'])
            article_scores.append((article_path, score))

        return sorted(article_scores, key=lambda x: x[1], reverse=True)[:top_n]


class CollaborativeFilteringEngine(RelatedArticlesEngine):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""

    def find_similar_users_by_category(self, category: str) -> List[str]:
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Ç–æ–π –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏–º–∏—Ç–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)"""
        similar = []

        for article_path, data in self.articles.items():
            if data['category'] == category:
                similar.append(article_path)

        return similar

    def get_category_based_recommendations(self, article_path: str, limit: int = 5) -> List[Tuple[str, float]]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if article_path not in self.articles:
            return []

        category = self.articles[article_path]['category']
        similar_articles = self.find_similar_users_by_category(category)

        recommendations = []

        for other_article in similar_articles:
            if other_article == article_path:
                continue

            # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ
            score = self.calculate_similarity(article_path, other_article)
            recommendations.append((other_article, score))

        return sorted(recommendations, key=lambda x: x[1], reverse=True)[:limit]


def main():
    parser = argparse.ArgumentParser(
        description='üéØ Related Articles - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                  # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
  %(prog)s --popular        # –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
  %(prog)s --trending       # –ü–æ–∫–∞–∑–∞—Ç—å trending —Å—Ç–∞—Ç—å–∏ (–ø–æ —Ç–µ–≥–∞–º)
  %(prog)s --for –ø—É—Ç—å       # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏
  %(prog)s --collaborative  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        """
    )

    parser.add_argument('--popular', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (–ø–æ –≤—Ö–æ–¥—è—â–∏–º —Å—Å—ã–ª–∫–∞–º)')
    parser.add_argument('--trending', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å trending —Å—Ç–∞—Ç—å–∏ (–ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–≥–æ–≤)')
    parser.add_argument('--for', dest='for_article', type=str, metavar='PATH',
                       help='–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏')
    parser.add_argument('--collaborative', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é')
    parser.add_argument('--limit', type=int, default=5,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (default: 5)')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    if args.collaborative:
        engine = CollaborativeFilteringEngine(root_dir)
    else:
        engine = RelatedArticlesEngine(root_dir)

    engine.build_index()

    # --popular
    if args.popular:
        print("üìä –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ (–ø–æ –≤—Ö–æ–¥—è—â–∏–º —Å—Å—ã–ª–∫–∞–º):\n")
        popular = engine.get_popular_articles(limit=args.limit)

        for i, (article_path, incoming_count) in enumerate(popular, 1):
            title = engine.articles[article_path]['title']
            print(f"   {i}. {title} ({incoming_count} –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫)")
            print(f"      {article_path}\n")

    # --trending
    if args.trending:
        print("üî• Trending —Å—Ç–∞—Ç—å–∏ (–ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–≥–æ–≤):\n")
        trending = engine.get_trending_articles_by_tags(top_n=args.limit)

        for i, (article_path, score) in enumerate(trending, 1):
            title = engine.articles[article_path]['title']
            tags = ', '.join(engine.articles[article_path]['tags'])
            print(f"   {i}. {title} (score: {score})")
            print(f"      –¢–µ–≥–∏: {tags}")
            print(f"      {article_path}\n")

    # --for specific article
    if args.for_article:
        article_path = args.for_article
        if article_path not in engine.articles:
            print(f"‚ö†Ô∏è  –°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {article_path}")
            return

        print(f"üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è: {engine.articles[article_path]['title']}\n")

        if args.collaborative and isinstance(engine, CollaborativeFilteringEngine):
            recommendations = engine.get_category_based_recommendations(article_path, args.limit)
        else:
            recommendations = engine.get_recommendations(article_path, args.limit)

        for i, (rec_path, score) in enumerate(recommendations, 1):
            rec_title = engine.articles[rec_path]['title']
            print(f"   {i}. {rec_title} (score: {score:.2f})")
            print(f"      {rec_path}\n")

    # Default: generate full report
    if not any([args.popular, args.trending, args.for_article]):
        engine.generate_report()
        engine.save_json()


if __name__ == "__main__":
    main()
