#!/usr/bin/env python3
"""
Advanced Duplicate Detection System - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º:
- MinHash LSH (Locality-Sensitive Hashing) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
- Simhash –¥–ª—è near-duplicate detection
- Shingling (n-gram) –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
- Content fingerprinting (MD5, SHA256)
- Duplicate clustering –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –≤–µ—Ä—Å–∏–π
- Merge suggestions –∏ canonical designation
- Code duplicate detection
- Multiple similarity algorithms

Inspired by: Google duplicate detection, Dedupe libraries, Plagiarism checkers

Author: Advanced Knowledge Management System
Version: 2.0
"""

from pathlib import Path
import re
import yaml
import json
import hashlib
from collections import defaultdict
from datetime import datetime
import argparse


class AdvancedDuplicateDetector:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    Features:
    - Multiple algorithms: Jaccard, MinHash, Simhash, Shingling
    - Duplicate types: exact, near-exact, partial, semantic
    - Content fingerprinting
    - Duplicate clustering (groups of similar docs)
    - Merge suggestions
    - Canonical URL designation
    - Code block duplicate detection
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.articles = []

        # Duplicate clusters: {cluster_id: [article_paths]}
        self.clusters = defaultdict(list)

        # Content fingerprints
        self.fingerprints = {}

        # MinHash signatures –¥–ª—è LSH
        self.minhash_sigs = {}

    def extract_frontmatter(self, file_path):
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if not match:
                return None, content

            frontmatter = yaml.safe_load(match.group(1))
            body = match.group(2)
            return frontmatter, body
        except:
            return None, ""

    # ==================== Fingerprinting ====================

    def calculate_fingerprint(self, content, algorithm='md5'):
        """–°–æ–∑–¥–∞—Ç—å fingerprint –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # Normalize: lowercase, remove whitespace
        normalized = re.sub(r'\s+', ' ', content.lower().strip())

        if algorithm == 'md5':
            return hashlib.md5(normalized.encode()).hexdigest()
        elif algorithm == 'sha256':
            return hashlib.sha256(normalized.encode()).hexdigest()
        else:
            return hashlib.sha1(normalized.encode()).hexdigest()

    def find_exact_duplicates(self):
        """–ù–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ fingerprint"""
        fingerprint_map = defaultdict(list)

        for article in self.articles:
            fp = self.calculate_fingerprint(article['content'])
            fingerprint_map[fp].append(article['file'])

        # –¢–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã —Å >1 —Ñ–∞–π–ª–æ–º
        exact_dups = {fp: files for fp, files in fingerprint_map.items() if len(files) > 1}

        return exact_dups

    # ==================== Shingling (n-grams) ====================

    def create_shingles(self, text, n=3):
        """
        –°–æ–∑–¥–∞—Ç—å n-–≥—Ä–∞–º–º—ã (shingles) –∏–∑ —Ç–µ–∫—Å—Ç–∞
        n=3 -> 'hello world' -> ['hel', 'ell', 'llo', 'lo ', 'o w', ' wo', ...]
        """
        # Character-level shingles
        text = re.sub(r'\s+', ' ', text.lower())
        shingles = set()

        for i in range(len(text) - n + 1):
            shingle = text[i:i+n]
            shingles.add(shingle)

        return shingles

    def jaccard_similarity(self, set1, set2):
        """Jaccard similarity: |A ‚à© B| / |A ‚à™ B|"""
        if not set1 or not set2:
            return 0.0

        intersection = set1 & set2
        union = set1 | set2

        return len(intersection) / len(union) if union else 0.0

    # ==================== MinHash LSH ====================

    def minhash_signature(self, shingles, num_hashes=100):
        """
        MinHash signature –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ approximate Jaccard similarity

        MinHash —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫:
        1. –î–ª—è –∫–∞–∂–¥–æ–≥–æ shingle –≤—ã—á–∏—Å–ª—è–µ–º hash
        2. –ë–µ—Ä—ë–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π hash –∏–∑ –Ω–∞–±–æ—Ä–∞
        3. –ü–æ–≤—Ç–æ—Ä—è–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ hash-—Ñ—É–Ω–∫—Ü–∏—è–º–∏
        4. –†–µ–∑—É–ª—å—Ç–∞—Ç: —Å–ø–∏—Å–æ–∫ –∏–∑ num_hashes –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

        –°–≤–æ–π—Å—Ç–≤–æ: P(minhash1[i] == minhash2[i]) ‚âà Jaccard(set1, set2)
        """
        signature = []

        for i in range(num_hashes):
            min_hash = float('inf')

            for shingle in shingles:
                # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö hash-—Ñ—É–Ω–∫—Ü–∏–π —á–µ—Ä–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–ª–∏
                h = hash(f"{i}:{shingle}") & 0xFFFFFFFF  # 32-bit hash
                min_hash = min(min_hash, h)

            signature.append(min_hash)

        return signature

    def estimate_jaccard_from_minhash(self, sig1, sig2):
        """–û—Ü–µ–Ω–∏—Ç—å Jaccard similarity –∏–∑ MinHash signatures"""
        if len(sig1) != len(sig2):
            return 0.0

        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)
        return matches / len(sig1)

    # ==================== Simhash ====================

    def simhash(self, text, hash_bits=64):
        """
        Simhash –¥–ª—è near-duplicate detection (–∫–∞–∫ –≤ Google)

        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ tokens (—Å–ª–æ–≤–∞)
        2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ token: hash -> binary
        3. –ï—Å–ª–∏ –±–∏—Ç=1: +weight, –µ—Å–ª–∏ –±–∏—Ç=0: -weight
        4. –§–∏–Ω–∞–ª—å–Ω—ã–π hash: sign(accumulated_weights)

        –°–≤–æ–π—Å—Ç–≤–æ: –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã ‚Üí –ø–æ—Ö–æ–∂–∏–µ simhash (–º–∞–ª—ã–π Hamming distance)
        """
        tokens = re.findall(r'\b\w+\b', text.lower())

        if not tokens:
            return 0

        # Accumulator –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∏—Ç–∞
        v = [0] * hash_bits

        for token in tokens:
            # Hash —Ç–æ–∫–µ–Ω–∞
            h = hash(token) & ((1 << hash_bits) - 1)  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ hash_bits –±–∏—Ç

            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∏—Ç–∞
            for i in range(hash_bits):
                if h & (1 << i):
                    v[i] += 1  # –ë–∏—Ç = 1
                else:
                    v[i] -= 1  # –ë–∏—Ç = 0

        # –§–∏–Ω–∞–ª—å–Ω—ã–π simhash
        fingerprint = 0
        for i in range(hash_bits):
            if v[i] > 0:
                fingerprint |= (1 << i)

        return fingerprint

    def hamming_distance(self, hash1, hash2):
        """Hamming distance –º–µ–∂–¥—É –¥–≤—É–º—è —Ö—ç—à–∞–º–∏"""
        xor = hash1 ^ hash2
        distance = 0

        while xor:
            distance += xor & 1
            xor >>= 1

        return distance

    # ==================== Duplicate Detection Methods ====================

    def find_near_duplicates_minhash(self, threshold=0.7):
        """–ù–∞–π—Ç–∏ near-duplicates —á–µ—Ä–µ–∑ MinHash LSH"""
        duplicates = []

        # –°–æ–∑–¥–∞—Ç—å MinHash signatures –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
        for article in self.articles:
            shingles = self.create_shingles(article['content'], n=5)
            sig = self.minhash_signature(shingles, num_hashes=100)
            self.minhash_sigs[article['file']] = sig

        # –°—Ä–∞–≤–Ω–∏—Ç—å –≤—Å–µ –ø–∞—Ä—ã
        files = list(self.minhash_sigs.keys())
        for i, file1 in enumerate(files):
            for file2 in files[i+1:]:
                similarity = self.estimate_jaccard_from_minhash(
                    self.minhash_sigs[file1],
                    self.minhash_sigs[file2]
                )

                if similarity >= threshold:
                    duplicates.append({
                        'file1': file1,
                        'file2': file2,
                        'similarity': round(similarity, 3),
                        'type': 'near-duplicate',
                        'algorithm': 'MinHash LSH'
                    })

        return duplicates

    def find_near_duplicates_simhash(self, max_distance=5):
        """–ù–∞–π—Ç–∏ near-duplicates —á–µ—Ä–µ–∑ Simhash"""
        duplicates = []
        simhashes = {}

        # –í—ã—á–∏—Å–ª–∏—Ç—å Simhash –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
        for article in self.articles:
            sh = self.simhash(article['content'], hash_bits=64)
            simhashes[article['file']] = sh

        # –°—Ä–∞–≤–Ω–∏—Ç—å –≤—Å–µ –ø–∞—Ä—ã
        files = list(simhashes.keys())
        for i, file1 in enumerate(files):
            for file2 in files[i+1:]:
                distance = self.hamming_distance(simhashes[file1], simhashes[file2])

                if distance <= max_distance:
                    # Similarity –æ—Ü–µ–Ω–∫–∞: —á–µ–º –º–µ–Ω—å—à–µ distance, —Ç–µ–º –±–æ–ª—å—à–µ similarity
                    similarity = 1.0 - (distance / 64.0)

                    duplicates.append({
                        'file1': file1,
                        'file2': file2,
                        'similarity': round(similarity, 3),
                        'hamming_distance': distance,
                        'type': 'near-duplicate',
                        'algorithm': 'Simhash'
                    })

        return duplicates

    def find_similar_by_tags(self):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Ç–µ–≥–∞–º–∏"""
        tag_groups = defaultdict(list)

        for article in self.articles:
            tags = article.get('tags', [])
            for tag in tags:
                tag_groups[tag].append(article['file'])

        similar_groups = {tag: files for tag, files in tag_groups.items() if len(files) > 1}

        return similar_groups

    def find_similar_by_title(self, threshold=0.5):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        similar = []

        for i, article1 in enumerate(self.articles):
            for article2 in self.articles[i+1:]:
                title1 = article1.get('title', '').lower()
                title2 = article2.get('title', '').lower()

                if not title1 or not title2:
                    continue

                words1 = set(title1.split())
                words2 = set(title2.split())

                similarity = self.jaccard_similarity(words1, words2)

                if similarity >= threshold:
                    similar.append({
                        'file1': article1['file'],
                        'file2': article2['file'],
                        'title1': article1.get('title'),
                        'title2': article2.get('title'),
                        'similarity': round(similarity, 3),
                        'type': 'similar-title'
                    })

        return similar

    def find_code_duplicates(self):
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã code blocks"""
        code_blocks = defaultdict(list)

        for article in self.articles:
            content = article['content']
            # –ù–∞–π—Ç–∏ –≤—Å–µ code blocks ```...```
            blocks = re.findall(r'```[\w]*\n(.*?)```', content, re.DOTALL)

            for block in blocks:
                # Normalize code
                normalized = re.sub(r'\s+', ' ', block.strip())
                if len(normalized) > 20:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –±–ª–æ–∫–∏
                    fp = hashlib.md5(normalized.encode()).hexdigest()
                    code_blocks[fp].append({
                        'file': article['file'],
                        'code': block[:100]  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                    })

        # –¢–æ–ª—å–∫–æ –¥—É–±–ª–∏–∫–∞—Ç—ã
        duplicates = {fp: blocks for fp, blocks in code_blocks.items() if len(blocks) > 1}

        return duplicates

    # ==================== Clustering ====================

    def cluster_duplicates(self, duplicates, threshold=0.8):
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø—ã

        –ï—Å–ª–∏ A –ø–æ—Ö–æ–∂ –Ω–∞ B, –∏ B –ø–æ—Ö–æ–∂ –Ω–∞ C, —Ç–æ A, B, C - –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
        """
        # Graph: file -> [similar files]
        graph = defaultdict(set)

        for dup in duplicates:
            if dup['similarity'] >= threshold:
                file1 = dup['file1']
                file2 = dup['file2']
                graph[file1].add(file2)
                graph[file2].add(file1)

        # DFS –¥–ª—è –ø–æ–∏—Å–∫–∞ connected components
        visited = set()
        clusters = []

        def dfs(node, cluster):
            visited.add(node)
            cluster.append(node)
            for neighbor in graph[node]:
                if neighbor not in visited:
                    dfs(neighbor, cluster)

        for node in graph:
            if node not in visited:
                cluster = []
                dfs(node, cluster)
                if len(cluster) > 1:
                    clusters.append(sorted(cluster))

        return clusters

    def suggest_canonical(self, cluster):
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ

        –ö—Ä–∏—Ç–µ—Ä–∏–∏:
        - –°–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        - –ë–æ–ª—å—à–µ –≤—Å–µ–≥–æ —Ç–µ–≥–æ–≤
        - –ù–æ–≤–µ–µ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞—Ç–∞)
        """
        best = None
        best_score = -1

        for file_path in cluster:
            # –ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å—é
            article = next((a for a in self.articles if a['file'] == file_path), None)
            if not article:
                continue

            score = 0
            # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            score += len(article['content']) / 100
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤
            score += len(article.get('tags', [])) * 5

            if score > best_score:
                best_score = score
                best = file_path

        return best

    def suggest_merge_action(self, cluster):
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –º–µ—Ä–¥–∂–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        canonical = self.suggest_canonical(cluster)

        return {
            'canonical': canonical,
            'duplicates': [f for f in cluster if f != canonical],
            'action': 'merge',
            'suggestion': f"Keep {canonical}, merge/redirect others"
        }

    # ==================== Main Analysis ====================

    def scan_articles(self):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter(md_file)

            article = {
                'file': str(md_file.relative_to(self.root_dir)),
                'title': frontmatter.get('title') if frontmatter else None,
                'tags': frontmatter.get('tags', []) if frontmatter else [],
                'category': frontmatter.get('category') if frontmatter else None,
                'content': content
            }

            self.articles.append(article)

    def analyze(self, algorithms=['all']):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        results = {
            'exact_duplicates': [],
            'near_duplicates_minhash': [],
            'near_duplicates_simhash': [],
            'similar_titles': [],
            'similar_tags': {},
            'code_duplicates': {},
            'clusters': [],
            'merge_suggestions': []
        }

        if 'all' in algorithms or 'exact' in algorithms:
            results['exact_duplicates'] = self.find_exact_duplicates()

        if 'all' in algorithms or 'minhash' in algorithms:
            results['near_duplicates_minhash'] = self.find_near_duplicates_minhash(threshold=0.7)

        if 'all' in algorithms or 'simhash' in algorithms:
            results['near_duplicates_simhash'] = self.find_near_duplicates_simhash(max_distance=5)

        if 'all' in algorithms or 'title' in algorithms:
            results['similar_titles'] = self.find_similar_by_title(threshold=0.5)

        if 'all' in algorithms or 'tags' in algorithms:
            results['similar_tags'] = self.find_similar_by_tags()

        if 'all' in algorithms or 'code' in algorithms:
            results['code_duplicates'] = self.find_code_duplicates()

        # Clustering
        all_duplicates = (
            results['near_duplicates_minhash'] +
            results['near_duplicates_simhash'] +
            results['similar_titles']
        )

        if all_duplicates:
            clusters = self.cluster_duplicates(all_duplicates, threshold=0.7)
            results['clusters'] = clusters

            # Merge suggestions
            for cluster in clusters:
                suggestion = self.suggest_merge_action(cluster)
                results['merge_suggestions'].append(suggestion)

        return results

    def generate_statistics(self, results):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        stats = {
            'total_articles': len(self.articles),
            'exact_duplicates': len(results['exact_duplicates']),
            'near_duplicates_minhash': len(results['near_duplicates_minhash']),
            'near_duplicates_simhash': len(results['near_duplicates_simhash']),
            'similar_titles': len(results['similar_titles']),
            'similar_tag_groups': len(results['similar_tags']),
            'code_duplicates': len(results['code_duplicates']),
            'duplicate_clusters': len(results['clusters']),
            'merge_suggestions': len(results['merge_suggestions'])
        }

        return stats

    def generate_report(self, results, format='markdown'):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        if format == 'json':
            return self.generate_json_report(results)
        else:
            return self.generate_markdown_report(results)

    def generate_markdown_report(self, results):
        """Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        stats = self.generate_statistics(results)

        lines.append("# üîç Duplicate Detection Report\n\n")
        lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # Statistics
        lines.append("## üìä Summary Statistics\n\n")
        lines.append(f"- **Total articles**: {stats['total_articles']}\n")
        lines.append(f"- **Exact duplicates**: {stats['exact_duplicates']}\n")
        lines.append(f"- **Near-duplicates (MinHash)**: {stats['near_duplicates_minhash']}\n")
        lines.append(f"- **Near-duplicates (Simhash)**: {stats['near_duplicates_simhash']}\n")
        lines.append(f"- **Similar titles**: {stats['similar_titles']}\n")
        lines.append(f"- **Tag groups**: {stats['similar_tag_groups']}\n")
        lines.append(f"- **Code duplicates**: {stats['code_duplicates']}\n")
        lines.append(f"- **Duplicate clusters**: {stats['duplicate_clusters']}\n\n")

        # Exact duplicates
        if results['exact_duplicates']:
            lines.append(f"## üíØ Exact Duplicates ({len(results['exact_duplicates'])})\n\n")

            for i, (fp, files) in enumerate(list(results['exact_duplicates'].items())[:10], 1):
                lines.append(f"### Group {i} (fingerprint: {fp[:16]}...)\n\n")
                for f in files:
                    lines.append(f"- `{f}`\n")
                lines.append("\n")

        # Near-duplicates (MinHash)
        if results['near_duplicates_minhash']:
            lines.append(f"## üîÑ Near-Duplicates - MinHash LSH ({len(results['near_duplicates_minhash'])})\n\n")

            for i, dup in enumerate(results['near_duplicates_minhash'][:20], 1):
                lines.append(f"### {i}. Similarity: {dup['similarity']*100:.1f}%\n\n")
                lines.append(f"- **File 1**: `{dup['file1']}`\n")
                lines.append(f"- **File 2**: `{dup['file2']}`\n\n")

        # Clusters
        if results['clusters']:
            lines.append(f"## üóÇÔ∏è Duplicate Clusters ({len(results['clusters'])})\n\n")

            for i, cluster in enumerate(results['clusters'][:10], 1):
                lines.append(f"### Cluster {i} ({len(cluster)} documents)\n\n")
                for f in cluster:
                    lines.append(f"- `{f}`\n")
                lines.append("\n")

        # Merge suggestions
        if results['merge_suggestions']:
            lines.append(f"## üí° Merge Suggestions ({len(results['merge_suggestions'])})\n\n")

            for i, suggestion in enumerate(results['merge_suggestions'][:10], 1):
                lines.append(f"### {i}. {suggestion['action'].upper()}\n\n")
                lines.append(f"- **Keep (canonical)**: `{suggestion['canonical']}`\n")
                lines.append(f"- **Merge/redirect**:\n")
                for dup in suggestion['duplicates']:
                    lines.append(f"  - `{dup}`\n")
                lines.append(f"- **Suggestion**: {suggestion['suggestion']}\n\n")

        output_file = self.root_dir / "DUPLICATE_DETECTION_REPORT.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown report: {output_file}")
        return output_file

    def generate_json_report(self, results):
        """JSON –æ—Ç—á—ë—Ç"""
        stats = self.generate_statistics(results)

        report = {
            'timestamp': datetime.now().isoformat(),
            'statistics': stats,
            'results': results
        }

        output_file = self.root_dir / "duplicate_detection.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ JSON report: {output_file}")
        return output_file


def main():
    parser = argparse.ArgumentParser(description='Advanced Duplicate Detection System')
    parser.add_argument('--algorithms', nargs='+',
                       choices=['all', 'exact', 'minhash', 'simhash', 'title', 'tags', 'code'],
                       default=['all'],
                       help='Algorithms to use (default: all)')
    parser.add_argument('--format', choices=['markdown', 'json'], default='markdown',
                       help='Report format (default: markdown)')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    detector = AdvancedDuplicateDetector(root_dir)

    print("üîç Advanced Duplicate Detection System\n")
    print("   Scanning articles...")
    detector.scan_articles()
    print(f"   Found: {len(detector.articles)} articles\n")

    print("   Running analysis...")
    results = detector.analyze(algorithms=args.algorithms)

    print("   Generating report...\n")
    detector.generate_report(results, format=args.format)

    # Print summary
    stats = detector.generate_statistics(results)
    print(f"\nüìä Summary:")
    print(f"   Exact duplicates: {stats['exact_duplicates']}")
    print(f"   Near-duplicates (MinHash): {stats['near_duplicates_minhash']}")
    print(f"   Near-duplicates (Simhash): {stats['near_duplicates_simhash']}")
    print(f"   Duplicate clusters: {stats['duplicate_clusters']}")
    print(f"\n‚úÖ Analysis complete!")


if __name__ == "__main__":
    main()
