#!/usr/bin/env python3
"""
Thesaurus Builder - –¢–µ–∑–∞—É—Ä—É—Å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

–°–æ–∑–¥–∞—ë—Ç —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤, –∞–Ω—Ç–æ–Ω–∏–º–æ–≤ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
"""

from pathlib import Path
import yaml
import re
import json
from collections import defaultdict


class ThesaurusBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å —Ç–µ–∑–∞—É—Ä—É—Å–∞ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.thesaurus_file = self.root_dir / "thesaurus.json"

        # –¢–µ–∑–∞—É—Ä—É—Å: term -> {synonyms, related, antonyms, broader, narrower}
        self.thesaurus = {}

        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
        self.known_synonyms = {
            'AI': ['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–ò–ò', 'artificial intelligence', 'machine intelligence'],
            'LLM': ['–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏', 'language models', 'GPT'],
            'Python': ['–ø–∏—Ç–æ–Ω', '–ø–∞–π—Ç–æ–Ω'],
            '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫': ['—Ä–µ—Ñ—Ä–∏–∂–µ—Ä–∞—Ç–æ—Ä', '–º–æ—Ä–æ–∑–∏–ª—å–Ω–∏–∫', 'fridge', 'refrigerator'],
            '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': ['coding', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'development', 'programming'],
            '–ø–∞—Ç—Ç–µ—Ä–Ω': ['—à–∞–±–ª–æ–Ω', 'pattern', 'template'],
        }

        # –ê–Ω—Ç–æ–Ω–∏–º—ã
        self.known_antonyms = {
            'hot': ['cold'],
            '–±–æ–ª—å—à–æ–π': ['–º–∞–ª–µ–Ω—å–∫–∏–π'],
            '–Ω–æ–≤—ã–π': ['—Å—Ç–∞—Ä—ã–π'],
            '–Ω–∞—á–∞–ª–æ': ['–∫–æ–Ω–µ—Ü'],
        }

        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (broader/narrower)
        self.hierarchies = {
            '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': {
                'narrower': ['Python', 'JavaScript', 'Java', 'C++'],
                'broader': ['–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –Ω–∞—É–∫–∏', 'IT']
            },
            'AI': {
                'narrower': ['LLM', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏', '–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ'],
                'broader': ['–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –Ω–∞—É–∫–∏']
            },
            '–±—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞': {
                'narrower': ['—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫', '–ø–ª–∏—Ç–∞', '—Å—Ç–∏—Ä–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞'],
                'broader': ['–¥–æ–º–∞—à–Ω–µ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ']
            }
        }

    def extract_frontmatter(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1))
        except:
            pass
        return None

    def add_term(self, term, **relations):
        """
        –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Ä–º–∏–Ω –≤ —Ç–µ–∑–∞—É—Ä—É—Å

        relations –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
        - synonyms: —Å–ø–∏—Å–æ–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        - related: —Å–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        - antonyms: —Å–ø–∏—Å–æ–∫ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤
        - broader: –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        - narrower: –±–æ–ª–µ–µ —É–∑–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        """
        term_lower = term.lower()

        if term_lower not in self.thesaurus:
            self.thesaurus[term_lower] = {
                'canonical': term,  # –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞
                'synonyms': set(),
                'related': set(),
                'antonyms': set(),
                'broader': set(),
                'narrower': set(),
                'articles': []  # –°—Ç–∞—Ç—å–∏, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
            }

        # –û–±–Ω–æ–≤–∏—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        for relation_type, terms in relations.items():
            if relation_type in self.thesaurus[term_lower]:
                if isinstance(terms, (list, set)):
                    self.thesaurus[term_lower][relation_type].update(terms)
                else:
                    self.thesaurus[term_lower][relation_type].add(terms)

    def build_from_tags(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–≥–æ–≤ —Å—Ç–∞—Ç–µ–π"""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤...\n")

        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Ç–µ–≥–∏ –∏ –∏—Ö —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ
        tag_cooccurrence = defaultdict(lambda: defaultdict(int))
        all_tags = set()

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter = self.extract_frontmatter(md_file)

            if not frontmatter:
                continue

            tags = frontmatter.get('tags', [])
            if not isinstance(tags, list):
                continue

            # –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥–∏
            for tag in tags:
                all_tags.add(tag)
                self.add_term(tag)

                # –ó–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∞—Ç—å—é
                file_path = str(md_file.relative_to(self.root_dir))
                self.thesaurus[tag.lower()]['articles'].append(file_path)

            # –¢–µ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ - –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–≤—è–∑–∞–Ω—ã
            for i, tag1 in enumerate(tags):
                for tag2 in tags[i+1:]:
                    tag_cooccurrence[tag1.lower()][tag2.lower()] += 1
                    tag_cooccurrence[tag2.lower()][tag1.lower()] += 1

        # –î–æ–±–∞–≤–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è
        for tag, related_tags in tag_cooccurrence.items():
            # –¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –≤–º–µ—Å—Ç–µ —Ç–µ–≥–æ–≤
            top_related = sorted(related_tags.items(), key=lambda x: -x[1])[:5]

            for related_tag, count in top_related:
                if count >= 2:  # –ú–∏–Ω–∏–º—É–º 2 —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è
                    self.add_term(tag, related=[related_tag])

        print(f"   –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤: {len(all_tags)}")

    def build_from_known_relations(self):
        """–î–æ–±–∞–≤–∏—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è"""
        print("üìö –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π...\n")

        # –°–∏–Ω–æ–Ω–∏–º—ã
        for term, synonyms in self.known_synonyms.items():
            self.add_term(term, synonyms=synonyms)

            # –û–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏
            for syn in synonyms:
                self.add_term(syn, synonyms=[term] + [s for s in synonyms if s != syn])

        # –ê–Ω—Ç–æ–Ω–∏–º—ã
        for term, antonyms in self.known_antonyms.items():
            self.add_term(term, antonyms=antonyms)

            for ant in antonyms:
                self.add_term(ant, antonyms=[term])

        # –ò–µ—Ä–∞—Ä—Ö–∏–∏
        for term, relations in self.hierarchies.items():
            broader = relations.get('broader', [])
            narrower = relations.get('narrower', [])

            self.add_term(term, broader=broader, narrower=narrower)

            # –û–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏
            for b in broader:
                self.add_term(b, narrower=[term])

            for n in narrower:
                self.add_term(n, broader=[term])

    def find_similar_terms(self, term1, term2):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (0.0 - 1.0)"""
        term1_lower = term1.lower()
        term2_lower = term2.lower()

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if term1_lower == term2_lower:
            return 1.0

        # –ü–æ–¥—Å—Ç—Ä–æ–∫–∞
        if term1_lower in term2_lower or term2_lower in term1_lower:
            return 0.8

        # –û–±—â–∏–µ –±—É–∫–≤—ã (–ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        common = set(term1_lower) & set(term2_lower)
        union = set(term1_lower) | set(term2_lower)

        if union:
            return len(common) / len(union) * 0.5

        return 0.0

    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∑–∞—É—Ä—É—Å"""
        print("üî§ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∑–∞—É—Ä—É—Å–∞...\n")

        self.build_from_tags()
        self.build_from_known_relations()

        print(f"\n‚úÖ –¢–µ–∑–∞—É—Ä—É—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        print(f"   –¢–µ—Ä–º–∏–Ω–æ–≤: {len(self.thesaurus)}")

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sets –≤ lists –¥–ª—è JSON
        thesaurus_json = {}

        for term, data in self.thesaurus.items():
            thesaurus_json[term] = {
                'canonical': data['canonical'],
                'synonyms': list(data['synonyms']),
                'related': list(data['related']),
                'antonyms': list(data['antonyms']),
                'broader': list(data['broader']),
                'narrower': list(data['narrower']),
                'articles': data['articles']
            }

        with open(self.thesaurus_file, 'w', encoding='utf-8') as f:
            json.dump(thesaurus_json, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –¢–µ–∑–∞—É—Ä—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {self.thesaurus_file}")

    def save_markdown(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å –≤ markdown"""
        lines = []
        lines.append("# üî§ –¢–µ–∑–∞—É—Ä—É—Å\n\n")
        lines.append("> –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏, –∞–Ω—Ç–æ–Ω–∏–º–∞–º–∏ –∏ —Å–≤—è–∑—è–º–∏\n\n")

        lines.append(f"**–í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤**: {len(self.thesaurus)}\n\n")

        # –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å
        current_letter = None

        for term in sorted(self.thesaurus.keys()):
            data = self.thesaurus[term]

            # –ù–æ–≤–∞—è –±—É–∫–≤–∞ - –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
            first_letter = term[0].upper()
            if first_letter != current_letter:
                current_letter = first_letter
                lines.append(f"\n## {current_letter}\n\n")

            # –¢–µ—Ä–º–∏–Ω
            canonical = data['canonical']
            lines.append(f"### {canonical}\n\n")

            # –°–∏–Ω–æ–Ω–∏–º—ã
            if data['synonyms']:
                syns = ', '.join(sorted(data['synonyms']))
                lines.append(f"**–°–∏–Ω–æ–Ω–∏–º—ã**: {syns}  \n")

            # –°–≤—è–∑–∞–Ω–Ω—ã–µ
            if data['related']:
                related = ', '.join(sorted(data['related']))
                lines.append(f"**–°–≤—è–∑–∞–Ω–Ω—ã–µ**: {related}  \n")

            # –ê–Ω—Ç–æ–Ω–∏–º—ã
            if data['antonyms']:
                ants = ', '.join(sorted(data['antonyms']))
                lines.append(f"**–ê–Ω—Ç–æ–Ω–∏–º—ã**: {ants}  \n")

            # –ò–µ—Ä–∞—Ä—Ö–∏—è
            if data['broader']:
                broader = ', '.join(sorted(data['broader']))
                lines.append(f"**–ë–æ–ª–µ–µ –æ–±—â–µ–µ**: {broader}  \n")

            if data['narrower']:
                narrower = ', '.join(sorted(data['narrower']))
                lines.append(f"**–ë–æ–ª–µ–µ —É–∑–∫–æ–µ**: {narrower}  \n")

            # –°—Ç–∞—Ç—å–∏
            if data['articles']:
                lines.append(f"**–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤**: {len(data['articles'])} —Å—Ç–∞—Ç—å—è—Ö  \n")

            lines.append("\n")

        output_file = self.root_dir / "THESAURUS.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown —Ç–µ–∑–∞—É—Ä—É—Å: {output_file}")

    def search(self, term):
        """–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ç–µ–∑–∞—É—Ä—É—Å–µ"""
        term_lower = term.lower()

        if term_lower in self.thesaurus:
            return self.thesaurus[term_lower]

        # –ü–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º
        for t, data in self.thesaurus.items():
            if term_lower in [s.lower() for s in data['synonyms']]:
                return data

        return None

    def expand_query(self, query_terms):
        """
        –†–∞—Å—à–∏—Ä–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏

        –ù–∞–ø—Ä–∏–º–µ—Ä: ["AI"] -> ["AI", "–ò–ò", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", ...]
        """
        expanded = set(query_terms)

        for term in query_terms:
            data = self.search(term)
            if data:
                expanded.update(data['synonyms'])
                # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏—Ç—å related —Ç–µ—Ä–º–∏–Ω—ã
                # expanded.update(data['related'])

        return list(expanded)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Thesaurus Builder - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∑–∞—É—Ä—É—Å–∞'
    )

    parser.add_argument(
        '-s', '--search',
        help='–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ç–µ–∑–∞—É—Ä—É—Å–µ'
    )

    parser.add_argument(
        '-e', '--expand',
        nargs='+',
        help='–†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = ThesaurusBuilder(root_dir)

    if args.search or args.expand:
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∑–∞—É—Ä—É—Å
        if builder.thesaurus_file.exists():
            with open(builder.thesaurus_file, 'r', encoding='utf-8') as f:
                thesaurus_json = json.load(f)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –≤ sets
            for term, data in thesaurus_json.items():
                builder.thesaurus[term] = {
                    'canonical': data['canonical'],
                    'synonyms': set(data['synonyms']),
                    'related': set(data['related']),
                    'antonyms': set(data['antonyms']),
                    'broader': set(data['broader']),
                    'narrower': set(data['narrower']),
                    'articles': data['articles']
                }

        if args.search:
            result = builder.search(args.search)

            if result:
                print(f"\nüî§ –¢–µ—Ä–º–∏–Ω: {result['canonical']}\n")

                if result['synonyms']:
                    print(f"   –°–∏–Ω–æ–Ω–∏–º—ã: {', '.join(sorted(result['synonyms']))}")

                if result['related']:
                    print(f"   –°–≤—è–∑–∞–Ω–Ω—ã–µ: {', '.join(sorted(result['related']))}")

                if result['antonyms']:
                    print(f"   –ê–Ω—Ç–æ–Ω–∏–º—ã: {', '.join(sorted(result['antonyms']))}")

                if result['broader']:
                    print(f"   –ë–æ–ª–µ–µ –æ–±—â–µ–µ: {', '.join(sorted(result['broader']))}")

                if result['narrower']:
                    print(f"   –ë–æ–ª–µ–µ —É–∑–∫–æ–µ: {', '.join(sorted(result['narrower']))}")

                if result['articles']:
                    print(f"\n   –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {len(result['articles'])} —Å—Ç–∞—Ç—å—è—Ö:")
                    for article in result['articles'][:5]:
                        print(f"      - {article}")
                    if len(result['articles']) > 5:
                        print(f"      ...–∏ –µ—â—ë {len(result['articles']) - 5}")

                print()
            else:
                print(f"‚ùå –¢–µ—Ä–º–∏–Ω '{args.search}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∑–∞—É—Ä—É—Å–µ")

        elif args.expand:
            expanded = builder.expand_query(args.expand)
            print(f"\nüîç –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å:")
            print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {', '.join(args.expand)}")
            print(f"   –†–∞—Å—à–∏—Ä–µ–Ω: {', '.join(expanded)}\n")

    else:
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å
        builder.build()
        builder.save()
        builder.save_markdown()


if __name__ == "__main__":
    main()
