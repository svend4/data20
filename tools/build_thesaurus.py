#!/usr/bin/env python3
"""
Thesaurus Builder - –¢–µ–∑–∞—É—Ä—É—Å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

–°–æ–∑–¥–∞—ë—Ç —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤, –∞–Ω—Ç–æ–Ω–∏–º–æ–≤ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
"""

from pathlib import Path
import yaml
import re
import json
from collections import defaultdict, Counter
import math


class TermExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""

    def __init__(self, root_dir):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

    def extract_ngrams(self, text, n=2, min_freq=2):
        """
        –ò–∑–≤–ª–µ—á—å n-–≥—Ä–∞–º–º—ã (–±–∏–≥—Ä–∞–º–º—ã, —Ç—Ä–∏–≥—Ä–∞–º–º—ã) –∏–∑ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: —Ç–µ–∫—Å—Ç
            n: —Ä–∞–∑–º–µ—Ä n-–≥—Ä–∞–º–º
            min_freq: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞

        Returns:
            list: n-–≥—Ä–∞–º–º—ã
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = re.findall(r'\b\w+\b', text.lower())

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è n-–≥—Ä–∞–º–º
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams.append(ngram)

        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç—ã
        ngram_freq = Counter(ngrams)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        return [(ng, freq) for ng, freq in ngram_freq.most_common() if freq >= min_freq]

    def extract_multiword_terms(self):
        """
        –ò–∑–≤–ª–µ—á—å –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π

        Returns:
            dict: —Ç–µ—Ä–º–∏–Ω—ã —Å —á–∞—Å—Ç–æ—Ç–æ–π
        """
        all_text = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # –£–¥–∞–ª–∏—Ç—å frontmatter
                content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)
                all_text.append(content)
            except:
                pass

        combined_text = '\n'.join(all_text)

        # –ë–∏–≥—Ä–∞–º–º—ã
        bigrams = self.extract_ngrams(combined_text, n=2, min_freq=3)

        # –¢—Ä–∏–≥—Ä–∞–º–º—ã
        trigrams = self.extract_ngrams(combined_text, n=3, min_freq=2)

        multiword_terms = {}
        for term, freq in bigrams + trigrams:
            multiword_terms[term] = freq

        return multiword_terms

    def calculate_tf_idf(self):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤

        Returns:
            dict: —Ç–µ—Ä–º–∏–Ω—ã —Å TF-IDF scores
        """
        # –î–æ–∫—É–º–µ–Ω—Ç—ã
        documents = []
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)
                    documents.append(content)
            except:
                pass

        if not documents:
            return {}

        # TF
        term_doc_freq = defaultdict(lambda: defaultdict(int))

        for doc_id, doc in enumerate(documents):
            words = re.findall(r'\b\w+\b', doc.lower())
            word_freq = Counter(words)

            for word, freq in word_freq.items():
                term_doc_freq[word][doc_id] = freq

        # IDF
        num_docs = len(documents)
        idf = {}

        for term, doc_freqs in term_doc_freq.items():
            docs_with_term = len(doc_freqs)
            idf[term] = math.log(num_docs / (1 + docs_with_term))

        # TF-IDF
        tf_idf_scores = {}

        for term, doc_freqs in term_doc_freq.items():
            # –°—Ä–µ–¥–Ω–∏–π TF-IDF –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            scores = []
            for doc_id, tf in doc_freqs.items():
                scores.append(tf * idf[term])

            if scores:
                tf_idf_scores[term] = sum(scores) / len(scores)

        # –¢–æ–ø —Ç–µ—Ä–º–∏–Ω—ã
        top_terms = sorted(tf_idf_scores.items(), key=lambda x: -x[1])[:100]

        return dict(top_terms)


class RelationshipMiner:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π"""

    def __init__(self, root_dir):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

    def mine_cooccurrence_relationships(self, window_size=10):
        """
        –ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ co-occurrence

        Args:
            window_size: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è

        Returns:
            dict: —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö —Å–≤—è–∑–∏
        """
        cooccurrence = defaultdict(lambda: Counter())

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)

                words = re.findall(r'\b\w+\b', content.lower())

                # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ
                for i in range(len(words)):
                    term = words[i]

                    # –û–∫–Ω–æ –≤–æ–∫—Ä—É–≥ —Ç–µ—Ä–º–∏–Ω–∞
                    start = max(0, i - window_size)
                    end = min(len(words), i + window_size + 1)

                    for j in range(start, end):
                        if i != j:
                            cooccurrence[term][words[j]] += 1
            except:
                pass

        # –¢–æ–ø —Å–≤—è–∑–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
        relationships = {}

        for term, related_counts in cooccurrence.items():
            # –¢–æ–ø-10 —Å–≤—è–∑–∞–Ω–Ω—ã—Ö
            top_related = related_counts.most_common(10)
            if top_related:
                relationships[term] = [r[0] for r in top_related]

        return relationships

    def detect_abbreviations(self, terms):
        """
        –û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã

        Args:
            terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤

        Returns:
            dict: –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ -> –ø–æ–ª–Ω–∞—è —Ñ–æ—Ä–º–∞
        """
        abbreviations = {}

        for term in terms:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Ä–º–∏–Ω –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π (–≤—Å–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ)
            if term.isupper() and len(term) >= 2:
                # –ù–∞–π—Ç–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã
                for full_term in terms:
                    if term.lower() != full_term.lower():
                        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞
                        words = full_term.split()
                        if len(words) >= len(term):
                            initials = ''.join([w[0].upper() for w in words if w])
                            if initials == term:
                                abbreviations[term] = full_term
                                break

        return abbreviations

    def find_compound_terms(self, terms):
        """
        –ù–∞–π—Ç–∏ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã

        Args:
            terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤

        Returns:
            dict: —Å–æ—Å—Ç–∞–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω -> –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        """
        compounds = {}

        for term in terms:
            if ' ' in term:  # –°–æ—Å—Ç–∞–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω
                words = term.split()
                if len(words) >= 2:
                    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ —Å–ø–∏—Å–∫–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
                    components = [w for w in words if w in terms]
                    if components:
                        compounds[term] = components

        return compounds


class ThesaurusVisualizer:
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∑–∞—É—Ä—É—Å–∞"""

    def __init__(self, thesaurus):
        self.thesaurus = thesaurus

    def generate_html_visualization(self):
        """
        –°–æ–∑–¥–∞—Ç—å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∑–∞—É—Ä—É—Å–∞

        Returns:
            str: HTML
        """
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_terms = len(self.thesaurus)
        with_synonyms = sum(1 for t in self.thesaurus.values() if t.get('synonyms'))
        with_related = sum(1 for t in self.thesaurus.values() if t.get('related'))

        # –¢–æ–ø —Ç–µ—Ä–º–∏–Ω—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–≤—è–∑–µ–π
        term_connections = []
        for term, data in self.thesaurus.items():
            connections = len(data.get('synonyms', [])) + len(data.get('related', [])) + len(data.get('broader', [])) + len(data.get('narrower', []))
            if connections > 0:
                term_connections.append((data.get('canonical', term), connections))

        top_terms = sorted(term_connections, key=lambda x: -x[1])[:20]

        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üî§ –¢–µ–∑–∞—É—Ä—É—Å</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}

        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}

        .container {{
            max-width: 1200px;
            margin: 0 auto;
        }}

        h1 {{
            color: white;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            text-shadow: 0 2px 10px rgba(0,0,0,0.3);
        }}

        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}

        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            text-align: center;
        }}

        .stat-value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #667eea;
        }}

        .stat-label {{
            color: #666;
            margin-top: 10px;
        }}

        .section {{
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            margin-bottom: 20px;
        }}

        .section-title {{
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
        }}

        .term-cloud {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            justify-content: center;
        }}

        .term-tag {{
            background: #667eea;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: calc(12px + var(--size) * 8px);
        }}

        .term-list {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
            gap: 15px;
        }}

        .term-item {{
            padding: 15px;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 4px solid #667eea;
        }}

        .term-name {{
            font-weight: bold;
            color: #333;
            margin-bottom: 5px;
        }}

        .term-connections {{
            color: #666;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üî§ –¢–µ–∑–∞—É—Ä—É—Å</h1>

        <div class="stats">
            <div class="stat-card">
                <div class="stat-value">{total_terms}</div>
                <div class="stat-label">–í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{with_synonyms}</div>
                <div class="stat-label">–° —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{with_related}</div>
                <div class="stat-label">–°–æ —Å–≤—è–∑—è–º–∏</div>
            </div>
        </div>

        <div class="section">
            <div class="section-title">‚òÅÔ∏è –¢–æ–ø —Ç–µ—Ä–º–∏–Ω—ã</div>
            <div class="term-cloud">
                {"".join(f'<span class="term-tag" style="--size: {min(connections/max(t[1] for t in top_terms), 1)}">{term} ({connections})</span>' for term, connections in top_terms[:15])}
            </div>
        </div>

        <div class="section">
            <div class="section-title">üìã –¢–µ—Ä–º–∏–Ω—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ —Å–≤—è–∑—è–º–∏</div>
            <div class="term-list">
                {"".join(f'''
                <div class="term-item">
                    <div class="term-name">{term}</div>
                    <div class="term-connections">{connections} —Å–≤—è–∑–µ–π</div>
                </div>
                ''' for term, connections in top_terms[:20])}
            </div>
        </div>
    </div>
</body>
</html>"""

        return html


class ThesaurusValidator:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∑–∞—É—Ä—É—Å–∞"""

    def __init__(self, thesaurus):
        self.thesaurus = thesaurus

    def validate_consistency(self):
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å–∞

        Returns:
            dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        issues = {
            'missing_reverse_relations': [],
            'circular_hierarchies': [],
            'orphaned_references': [],
            'duplicate_synonyms': []
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        for term, data in self.thesaurus.items():
            for syn in data.get('synonyms', []):
                syn_lower = syn.lower()
                if syn_lower in self.thesaurus:
                    if term not in self.thesaurus[syn_lower].get('synonyms', []):
                        issues['missing_reverse_relations'].append((term, syn))

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        for term, data in self.thesaurus.items():
            broader = data.get('broader', [])
            for b in broader:
                b_lower = b.lower()
                if b_lower in self.thesaurus:
                    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ —É–∫–∞–∑–∞–Ω –ª–∏ —Ç–µ–∫—É—â–∏–π —Ç–µ—Ä–º–∏–Ω –∫–∞–∫ broader –¥–ª—è b
                    if term in self.thesaurus[b_lower].get('broader', []):
                        issues['circular_hierarchies'].append((term, b))

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: orphaned references
        for term, data in self.thesaurus.items():
            all_refs = list(data.get('synonyms', [])) + list(data.get('related', [])) + list(data.get('broader', [])) + list(data.get('narrower', []))

            for ref in all_refs:
                ref_lower = ref.lower()
                if ref_lower not in self.thesaurus:
                    issues['orphaned_references'].append((term, ref))

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        for term, data in self.thesaurus.items():
            synonyms = list(data.get('synonyms', []))
            if len(synonyms) != len(set(s.lower() for s in synonyms)):
                issues['duplicate_synonyms'].append(term)

        return issues

    def calculate_quality_score(self):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∑–∞—É—Ä—É—Å–∞

        Returns:
            dict: –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        total_terms = len(self.thesaurus)

        if total_terms == 0:
            return {'quality_score': 0}

        # –ú–µ—Ç—Ä–∏–∫–∏
        terms_with_synonyms = sum(1 for t in self.thesaurus.values() if t.get('synonyms'))
        terms_with_related = sum(1 for t in self.thesaurus.values() if t.get('related'))
        terms_with_hierarchy = sum(1 for t in self.thesaurus.values() if t.get('broader') or t.get('narrower'))

        # Score (0-100)
        coverage_score = (terms_with_synonyms / total_terms) * 40
        richness_score = (terms_with_related / total_terms) * 30
        structure_score = (terms_with_hierarchy / total_terms) * 30

        quality_score = coverage_score + richness_score + structure_score

        return {
            'quality_score': round(quality_score, 1),
            'total_terms': total_terms,
            'with_synonyms': terms_with_synonyms,
            'with_related': terms_with_related,
            'with_hierarchy': terms_with_hierarchy,
            'coverage_percent': round((terms_with_synonyms / total_terms) * 100, 1),
            'richness_percent': round((terms_with_related / total_terms) * 100, 1),
            'structure_percent': round((terms_with_hierarchy / total_terms) * 100, 1)
        }


class ThesaurusBuilder:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å —Ç–µ–∑–∞—É—Ä—É—Å–∞ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.thesaurus_file = self.root_dir / "thesaurus.json"

        # –¢–µ–∑–∞—É—Ä—É—Å: term -> {synonyms, related, antonyms, broader, narrower}
        self.thesaurus = {}

        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
        self.known_synonyms = {
            'AI': ['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–ò–ò', 'artificial intelligence', 'machine intelligence'],
            'LLM': ['–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏', 'language models', 'GPT'],
            'Python': ['–ø–∏—Ç–æ–Ω', '–ø–∞–π—Ç–æ–Ω'],
            '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫': ['—Ä–µ—Ñ—Ä–∏–∂–µ—Ä–∞—Ç–æ—Ä', '–º–æ—Ä–æ–∑–∏–ª—å–Ω–∏–∫', 'fridge', 'refrigerator'],
            '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': ['coding', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'development', 'programming'],
            '–ø–∞—Ç—Ç–µ—Ä–Ω': ['—à–∞–±–ª–æ–Ω', 'pattern', 'template'],
        }

        # –ê–Ω—Ç–æ–Ω–∏–º—ã
        self.known_antonyms = {
            'hot': ['cold'],
            '–±–æ–ª—å—à–æ–π': ['–º–∞–ª–µ–Ω—å–∫–∏–π'],
            '–Ω–æ–≤—ã–π': ['—Å—Ç–∞—Ä—ã–π'],
            '–Ω–∞—á–∞–ª–æ': ['–∫–æ–Ω–µ—Ü'],
        }

        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (broader/narrower)
        self.hierarchies = {
            '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': {
                'narrower': ['Python', 'JavaScript', 'Java', 'C++'],
                'broader': ['–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –Ω–∞—É–∫–∏', 'IT']
            },
            'AI': {
                'narrower': ['LLM', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏', '–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ'],
                'broader': ['–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –Ω–∞—É–∫–∏']
            },
            '–±—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞': {
                'narrower': ['—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫', '–ø–ª–∏—Ç–∞', '—Å—Ç–∏—Ä–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞'],
                'broader': ['–¥–æ–º–∞—à–Ω–µ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ']
            }
        }

    def extract_frontmatter(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1))
        except:
            pass
        return None

    def add_term(self, term, **relations):
        """
        –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Ä–º–∏–Ω –≤ —Ç–µ–∑–∞—É—Ä—É—Å

        relations –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
        - synonyms: —Å–ø–∏—Å–æ–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        - related: —Å–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        - antonyms: —Å–ø–∏—Å–æ–∫ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤
        - broader: –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        - narrower: –±–æ–ª–µ–µ —É–∑–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        """
        term_lower = term.lower()

        if term_lower not in self.thesaurus:
            self.thesaurus[term_lower] = {
                'canonical': term,  # –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞
                'synonyms': set(),
                'related': set(),
                'antonyms': set(),
                'broader': set(),
                'narrower': set(),
                'articles': []  # –°—Ç–∞—Ç—å–∏, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
            }

        # –û–±–Ω–æ–≤–∏—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        for relation_type, terms in relations.items():
            if relation_type in self.thesaurus[term_lower]:
                if isinstance(terms, (list, set)):
                    self.thesaurus[term_lower][relation_type].update(terms)
                else:
                    self.thesaurus[term_lower][relation_type].add(terms)

    def build_from_tags(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–≥–æ–≤ —Å—Ç–∞—Ç–µ–π"""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤...\n")

        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Ç–µ–≥–∏ –∏ –∏—Ö —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ
        tag_cooccurrence = defaultdict(lambda: defaultdict(int))
        all_tags = set()

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter = self.extract_frontmatter(md_file)

            if not frontmatter:
                continue

            tags = frontmatter.get('tags', [])
            if not isinstance(tags, list):
                continue

            # –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥–∏
            for tag in tags:
                all_tags.add(tag)
                self.add_term(tag)

                # –ó–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∞—Ç—å—é
                file_path = str(md_file.relative_to(self.root_dir))
                self.thesaurus[tag.lower()]['articles'].append(file_path)

            # –¢–µ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ - –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–≤—è–∑–∞–Ω—ã
            for i, tag1 in enumerate(tags):
                for tag2 in tags[i+1:]:
                    tag_cooccurrence[tag1.lower()][tag2.lower()] += 1
                    tag_cooccurrence[tag2.lower()][tag1.lower()] += 1

        # –î–æ–±–∞–≤–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è
        for tag, related_tags in tag_cooccurrence.items():
            # –¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –≤–º–µ—Å—Ç–µ —Ç–µ–≥–æ–≤
            top_related = sorted(related_tags.items(), key=lambda x: -x[1])[:5]

            for related_tag, count in top_related:
                if count >= 2:  # –ú–∏–Ω–∏–º—É–º 2 —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è
                    self.add_term(tag, related=[related_tag])

        print(f"   –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤: {len(all_tags)}")

    def build_from_known_relations(self):
        """–î–æ–±–∞–≤–∏—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è"""
        print("üìö –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π...\n")

        # –°–∏–Ω–æ–Ω–∏–º—ã
        for term, synonyms in self.known_synonyms.items():
            self.add_term(term, synonyms=synonyms)

            # –û–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏
            for syn in synonyms:
                self.add_term(syn, synonyms=[term] + [s for s in synonyms if s != syn])

        # –ê–Ω—Ç–æ–Ω–∏–º—ã
        for term, antonyms in self.known_antonyms.items():
            self.add_term(term, antonyms=antonyms)

            for ant in antonyms:
                self.add_term(ant, antonyms=[term])

        # –ò–µ—Ä–∞—Ä—Ö–∏–∏
        for term, relations in self.hierarchies.items():
            broader = relations.get('broader', [])
            narrower = relations.get('narrower', [])

            self.add_term(term, broader=broader, narrower=narrower)

            # –û–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏
            for b in broader:
                self.add_term(b, narrower=[term])

            for n in narrower:
                self.add_term(n, broader=[term])

    def find_similar_terms(self, term1, term2):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (0.0 - 1.0)"""
        term1_lower = term1.lower()
        term2_lower = term2.lower()

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if term1_lower == term2_lower:
            return 1.0

        # –ü–æ–¥—Å—Ç—Ä–æ–∫–∞
        if term1_lower in term2_lower or term2_lower in term1_lower:
            return 0.8

        # –û–±—â–∏–µ –±—É–∫–≤—ã (–ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        common = set(term1_lower) & set(term2_lower)
        union = set(term1_lower) | set(term2_lower)

        if union:
            return len(common) / len(union) * 0.5

        return 0.0

    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∑–∞—É—Ä—É—Å"""
        print("üî§ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∑–∞—É—Ä—É—Å–∞...\n")

        self.build_from_tags()
        self.build_from_known_relations()

        print(f"\n‚úÖ –¢–µ–∑–∞—É—Ä—É—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        print(f"   –¢–µ—Ä–º–∏–Ω–æ–≤: {len(self.thesaurus)}")

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sets –≤ lists –¥–ª—è JSON
        thesaurus_json = {}

        for term, data in self.thesaurus.items():
            thesaurus_json[term] = {
                'canonical': data['canonical'],
                'synonyms': list(data['synonyms']),
                'related': list(data['related']),
                'antonyms': list(data['antonyms']),
                'broader': list(data['broader']),
                'narrower': list(data['narrower']),
                'articles': data['articles']
            }

        with open(self.thesaurus_file, 'w', encoding='utf-8') as f:
            json.dump(thesaurus_json, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –¢–µ–∑–∞—É—Ä—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {self.thesaurus_file}")

    def save_markdown(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å –≤ markdown"""
        lines = []
        lines.append("# üî§ –¢–µ–∑–∞—É—Ä—É—Å\n\n")
        lines.append("> –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏, –∞–Ω—Ç–æ–Ω–∏–º–∞–º–∏ –∏ —Å–≤—è–∑—è–º–∏\n\n")

        lines.append(f"**–í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤**: {len(self.thesaurus)}\n\n")

        # –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å
        current_letter = None

        for term in sorted(self.thesaurus.keys()):
            data = self.thesaurus[term]

            # –ù–æ–≤–∞—è –±—É–∫–≤–∞ - –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
            first_letter = term[0].upper()
            if first_letter != current_letter:
                current_letter = first_letter
                lines.append(f"\n## {current_letter}\n\n")

            # –¢–µ—Ä–º–∏–Ω
            canonical = data['canonical']
            lines.append(f"### {canonical}\n\n")

            # –°–∏–Ω–æ–Ω–∏–º—ã
            if data['synonyms']:
                syns = ', '.join(sorted(data['synonyms']))
                lines.append(f"**–°–∏–Ω–æ–Ω–∏–º—ã**: {syns}  \n")

            # –°–≤—è–∑–∞–Ω–Ω—ã–µ
            if data['related']:
                related = ', '.join(sorted(data['related']))
                lines.append(f"**–°–≤—è–∑–∞–Ω–Ω—ã–µ**: {related}  \n")

            # –ê–Ω—Ç–æ–Ω–∏–º—ã
            if data['antonyms']:
                ants = ', '.join(sorted(data['antonyms']))
                lines.append(f"**–ê–Ω—Ç–æ–Ω–∏–º—ã**: {ants}  \n")

            # –ò–µ—Ä–∞—Ä—Ö–∏—è
            if data['broader']:
                broader = ', '.join(sorted(data['broader']))
                lines.append(f"**–ë–æ–ª–µ–µ –æ–±—â–µ–µ**: {broader}  \n")

            if data['narrower']:
                narrower = ', '.join(sorted(data['narrower']))
                lines.append(f"**–ë–æ–ª–µ–µ —É–∑–∫–æ–µ**: {narrower}  \n")

            # –°—Ç–∞—Ç—å–∏
            if data['articles']:
                lines.append(f"**–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤**: {len(data['articles'])} —Å—Ç–∞—Ç—å—è—Ö  \n")

            lines.append("\n")

        output_file = self.root_dir / "THESAURUS.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown —Ç–µ–∑–∞—É—Ä—É—Å: {output_file}")

    def search(self, term):
        """–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ç–µ–∑–∞—É—Ä—É—Å–µ"""
        term_lower = term.lower()

        if term_lower in self.thesaurus:
            return self.thesaurus[term_lower]

        # –ü–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º
        for t, data in self.thesaurus.items():
            if term_lower in [s.lower() for s in data['synonyms']]:
                return data

        return None

    def expand_query(self, query_terms):
        """
        –†–∞—Å—à–∏—Ä–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏

        –ù–∞–ø—Ä–∏–º–µ—Ä: ["AI"] -> ["AI", "–ò–ò", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", ...]
        """
        expanded = set(query_terms)

        for term in query_terms:
            data = self.search(term)
            if data:
                expanded.update(data['synonyms'])
                # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏—Ç—å related —Ç–µ—Ä–º–∏–Ω—ã
                # expanded.update(data['related'])

        return list(expanded)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Thesaurus Builder - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∑–∞—É—Ä—É—Å–∞'
    )

    parser.add_argument(
        '-s', '--search',
        help='–ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ç–µ–∑–∞—É—Ä—É—Å–µ'
    )

    parser.add_argument(
        '-e', '--expand',
        nargs='+',
        help='–†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏'
    )

    parser.add_argument('--extract-terms', action='store_true',
                       help='–ò–∑–≤–ª–µ—á—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (TF-IDF, n-grams)')
    parser.add_argument('--mine-relations', action='store_true',
                       help='–ù–∞–π—Ç–∏ —Å–≤—è–∑–∏ —á–µ—Ä–µ–∑ co-occurrence')
    parser.add_argument('--html', action='store_true',
                       help='–°–æ–∑–¥–∞—Ç—å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é')
    parser.add_argument('--validate', action='store_true',
                       help='–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∑–∞—É—Ä—É—Å–∞')
    parser.add_argument('--quality', action='store_true',
                       help='–í—ã—á–∏—Å–ª–∏—Ç—å quality score')
    parser.add_argument('--all', action='store_true',
                       help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = ThesaurusBuilder(root_dir)

    if args.search or args.expand:
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∑–∞—É—Ä—É—Å
        if builder.thesaurus_file.exists():
            with open(builder.thesaurus_file, 'r', encoding='utf-8') as f:
                thesaurus_json = json.load(f)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –≤ sets
            for term, data in thesaurus_json.items():
                builder.thesaurus[term] = {
                    'canonical': data['canonical'],
                    'synonyms': set(data['synonyms']),
                    'related': set(data['related']),
                    'antonyms': set(data['antonyms']),
                    'broader': set(data['broader']),
                    'narrower': set(data['narrower']),
                    'articles': data['articles']
                }

        if args.search:
            result = builder.search(args.search)

            if result:
                print(f"\nüî§ –¢–µ—Ä–º–∏–Ω: {result['canonical']}\n")

                if result['synonyms']:
                    print(f"   –°–∏–Ω–æ–Ω–∏–º—ã: {', '.join(sorted(result['synonyms']))}")

                if result['related']:
                    print(f"   –°–≤—è–∑–∞–Ω–Ω—ã–µ: {', '.join(sorted(result['related']))}")

                if result['antonyms']:
                    print(f"   –ê–Ω—Ç–æ–Ω–∏–º—ã: {', '.join(sorted(result['antonyms']))}")

                if result['broader']:
                    print(f"   –ë–æ–ª–µ–µ –æ–±—â–µ–µ: {', '.join(sorted(result['broader']))}")

                if result['narrower']:
                    print(f"   –ë–æ–ª–µ–µ —É–∑–∫–æ–µ: {', '.join(sorted(result['narrower']))}")

                if result['articles']:
                    print(f"\n   –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {len(result['articles'])} —Å—Ç–∞—Ç—å—è—Ö:")
                    for article in result['articles'][:5]:
                        print(f"      - {article}")
                    if len(result['articles']) > 5:
                        print(f"      ...–∏ –µ—â—ë {len(result['articles']) - 5}")

                print()
            else:
                print(f"‚ùå –¢–µ—Ä–º–∏–Ω '{args.search}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∑–∞—É—Ä—É—Å–µ")

        elif args.expand:
            expanded = builder.expand_query(args.expand)
            print(f"\nüîç –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å:")
            print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {', '.join(args.expand)}")
            print(f"   –†–∞—Å—à–∏—Ä–µ–Ω: {', '.join(expanded)}\n")

    elif args.extract_terms or args.mine_relations or args.html or args.validate or args.quality or args.all:
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∑–∞—É—Ä—É—Å –µ—Å–ª–∏ –µ—Å—Ç—å
        if builder.thesaurus_file.exists():
            with open(builder.thesaurus_file, 'r', encoding='utf-8') as f:
                thesaurus_json = json.load(f)

            for term, data in thesaurus_json.items():
                builder.thesaurus[term] = {
                    'canonical': data['canonical'],
                    'synonyms': set(data['synonyms']),
                    'related': set(data['related']),
                    'antonyms': set(data['antonyms']),
                    'broader': set(data['broader']),
                    'narrower': set(data['narrower']),
                    'articles': data['articles']
                }

        # Extract terms
        if args.extract_terms or args.all:
            print("\nüìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            extractor = TermExtractor(root_dir)

            multiword = extractor.extract_multiword_terms()
            print(f"   –ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(multiword)}")
            if multiword:
                top_multi = sorted(multiword.items(), key=lambda x: -x[1])[:10]
                for term, freq in top_multi:
                    print(f"      {term}: {freq}")

            tf_idf = extractor.calculate_tf_idf()
            print(f"\n   –¢–æ–ø —Ç–µ—Ä–º–∏–Ω—ã –ø–æ TF-IDF: {len(tf_idf)}")
            for term, score in list(tf_idf.items())[:10]:
                print(f"      {term}: {score:.3f}")

        # Mine relations
        if args.mine_relations or args.all:
            print("\nüîó –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π...")
            miner = RelationshipMiner(root_dir)

            relations = miner.mine_cooccurrence_relationships()
            print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å–æ —Å–≤—è–∑—è–º–∏: {len(relations)}")

        # Validate
        if args.validate or args.all:
            print("\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∑–∞—É—Ä—É—Å–∞...")
            validator = ThesaurusValidator(builder.thesaurus)
            issues = validator.validate_consistency()

            print(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏: {len(issues['missing_reverse_relations'])}")
            print(f"   –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏: {len(issues['circular_hierarchies'])}")
            print(f"   Orphaned references: {len(issues['orphaned_references'])}")
            print(f"   –î—É–±–ª–∏–∫–∞—Ç—ã —Å–∏–Ω–æ–Ω–∏–º–æ–≤: {len(issues['duplicate_synonyms'])}")

        # Quality
        if args.quality or args.all:
            print("\nüèÜ Quality Score...")
            validator = ThesaurusValidator(builder.thesaurus)
            quality = validator.calculate_quality_score()

            print(f"   Quality Score: {quality['quality_score']}/100")
            print(f"   –¢–µ—Ä–º–∏–Ω–æ–≤ –≤—Å–µ–≥–æ: {quality['total_terms']}")
            print(f"   –° —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏: {quality['with_synonyms']} ({quality['coverage_percent']}%)")
            print(f"   –°–æ —Å–≤—è–∑—è–º–∏: {quality['with_related']} ({quality['richness_percent']}%)")
            print(f"   –° –∏–µ—Ä–∞—Ä—Ö–∏–µ–π: {quality['with_hierarchy']} ({quality['structure_percent']}%)")

        # HTML
        if args.html or args.all:
            print("\nüé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML...")
            visualizer = ThesaurusVisualizer(builder.thesaurus)
            html = visualizer.generate_html_visualization()

            html_file = root_dir / "thesaurus.html"
            html_file.write_text(html, encoding='utf-8')
            print(f"   HTML: {html_file}")

    else:
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–∑–∞—É—Ä—É—Å
        builder.build()
        builder.save()
        builder.save_markdown()


if __name__ == "__main__":
    main()
