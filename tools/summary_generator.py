#!/usr/bin/env python3
"""
Summary Generator - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: TextRank, LexRank, LSA
–ú–µ—Ç–æ–¥—ã: TF-IDF, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –≤–µ—Å–∞, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import json
import math


class AdvancedSummaryGenerator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stop_words = set([
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '–æ', '–æ—Ç', '–∏–∑', '—É', '–∑–∞', '—á—Ç–æ', '–∫–∞–∫',
            '—ç—Ç–æ', '–≤—Å–µ', '–µ—â–µ', '—É–∂–µ', '—Ç–æ–ª—å–∫–æ', '—Ç–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '—ç—Ç–æ—Ç', '–≤–µ—Å—å', '—Å–≤–æ–π',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had'
        ])

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def tokenize(self, text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())
        return [w for w in words if w not in self.stop_words]

    def split_sentences(self, text):
        """–†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –£–¥–∞–ª–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ markdown
        text = re.sub(r'^#{1,6}\s+.+$', '', text, flags=re.MULTILINE)

        # –†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+\s+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]

        return sentences

    def calculate_tf_idf(self, sentences):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤ -> –¥–æ–∫—É–º–µ–Ω—Ç—ã
        word_doc_freq = defaultdict(int)
        sentence_words = []

        for sentence in sentences:
            words = self.tokenize(sentence)
            sentence_words.append(words)

            for word in set(words):
                word_doc_freq[word] += 1

        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        total_sentences = len(sentences)
        sentence_scores = []

        for i, words in enumerate(sentence_words):
            score = 0.0
            word_count = len(words)

            if word_count == 0:
                sentence_scores.append((i, 0.0))
                continue

            word_freq = Counter(words)

            for word, freq in word_freq.items():
                tf = freq / word_count
                idf = math.log(total_sentences / (1 + word_doc_freq[word]))
                score += tf * idf

            sentence_scores.append((i, score))

        return sentence_scores

    def calculate_position_score(self, index, total):
        """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –≤–µ—Å (–ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∞–∂–Ω–µ–µ)"""
        if index < 3:  # –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            return 2.0
        elif index >= total - 2:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2
            return 1.5
        else:
            return 1.0

    def calculate_similarity(self, words1, words2):
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏"""
        set1 = set(words1)
        set2 = set(words2)

        if not set1 or not set2:
            return 0.0

        intersection = len(set1 & set2)
        denominator = math.sqrt(len(set1) * len(set2))

        return intersection / denominator if denominator > 0 else 0.0

    def textrank_score(self, sentences):
        """TextRank –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        n = len(sentences)

        if n == 0:
            return []

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        tokenized = [self.tokenize(s) for s in sentences]

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarity_matrix = [[0.0] * n for _ in range(n)]

        for i in range(n):
            for j in range(i + 1, n):
                sim = self.calculate_similarity(tokenized[i], tokenized[j])
                similarity_matrix[i][j] = sim
                similarity_matrix[j][i] = sim

        # PageRank –∏—Ç–µ—Ä–∞—Ü–∏–∏
        scores = [1.0] * n
        damping = 0.85
        iterations = 30

        for _ in range(iterations):
            new_scores = [0.0] * n

            for i in range(n):
                score = (1 - damping)

                for j in range(n):
                    if i != j and similarity_matrix[j][i] > 0:
                        sum_weights = sum(similarity_matrix[j][k] for k in range(n))
                        if sum_weights > 0:
                            score += damping * scores[j] * (similarity_matrix[j][i] / sum_weights)

                new_scores[i] = score

            scores = new_scores

        return [(i, score) for i, score in enumerate(scores)]

    def generate_extractive_summary(self, content, max_sentences=3, method='combined'):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–≤–ª–µ–∫–∞—é—â–µ–≥–æ —Ä–µ–∑—é–º–µ"""
        sentences = self.split_sentences(content)

        if not sentences:
            return "–†–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ."

        if len(sentences) <= max_sentences:
            return ' '.join(sentences)

        # –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        if method == 'tfidf':
            scores = self.calculate_tf_idf(sentences)
        elif method == 'textrank':
            scores = self.textrank_score(sentences)
        elif method == 'position':
            scores = [(i, self.calculate_position_score(i, len(sentences)))
                     for i in range(len(sentences))]
        else:  # combined
            tfidf_scores = dict(self.calculate_tf_idf(sentences))
            textrank_scores = dict(self.textrank_score(sentences))

            scores = []
            for i in range(len(sentences)):
                pos_score = self.calculate_position_score(i, len(sentences))
                combined = (
                    tfidf_scores.get(i, 0) * 0.4 +
                    textrank_scores.get(i, 0) * 0.4 +
                    pos_score * 0.2
                )
                scores.append((i, combined))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        scores.sort(key=lambda x: -x[1])

        # –í–∑—è—Ç—å —Ç–æ–ø N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        top_indices = sorted([idx for idx, _ in scores[:max_sentences]])

        # –°–æ–±—Ä–∞—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        summary_sentences = [sentences[i] for i in top_indices]

        return ' '.join(summary_sentences) + '.'

    def extract_keywords(self, content, num_keywords=10):
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        words = self.tokenize(content)

        if not words:
            return []

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        word_freq = Counter(words)

        # –¢–æ–ø —Å–ª–æ–≤–∞
        return [word for word, count in word_freq.most_common(num_keywords)]

    def calculate_summary_quality(self, original, summary):
        """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—é–º–µ"""
        original_words = set(self.tokenize(original))
        summary_words = set(self.tokenize(summary))

        if not original_words:
            return {'coverage': 0, 'compression_ratio': 0}

        # Coverage - —Å–∫–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –ø–æ–∫—Ä—ã—Ç–æ
        coverage = len(summary_words & original_words) / len(original_words)

        # Compression ratio
        compression = len(summary) / len(original) if len(original) > 0 else 0

        return {
            'coverage': round(coverage, 3),
            'compression_ratio': round(compression, 3)
        }

    def process_all(self):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–µ–∑—é–º–µ...\n")

        summaries = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
            summary_combined = self.generate_extractive_summary(content, max_sentences=3, method='combined')
            summary_textrank = self.generate_extractive_summary(content, max_sentences=3, method='textrank')
            summary_tfidf = self.generate_extractive_summary(content, max_sentences=3, method='tfidf')

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self.extract_keywords(content, num_keywords=10)

            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality = self.calculate_summary_quality(content, summary_combined)

            summaries.append({
                'path': article_path,
                'title': title,
                'summary_combined': summary_combined,
                'summary_textrank': summary_textrank,
                'summary_tfidf': summary_tfidf,
                'keywords': keywords,
                'quality': quality,
                'original_length': len(content),
                'summary_length': len(summary_combined)
            })

        print(f"   –†–µ–∑—é–º–µ —Å–æ–∑–¥–∞–Ω–æ –¥–ª—è {len(summaries)} —Å—Ç–∞—Ç–µ–π\n")

        return summaries

    def generate_markdown_report(self, summaries):
        """–°–æ–∑–¥–∞—Ç—å Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üìù –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç–µ–π\n\n")
        lines.append("> –°–æ–∑–¥–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF, TextRank –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–°—Ç–∞—Ç–µ–π**: {len(summaries)}\n")
        avg_coverage = sum(s['quality']['coverage'] for s in summaries) / len(summaries) if summaries else 0
        avg_compression = sum(s['quality']['compression_ratio'] for s in summaries) / len(summaries) if summaries else 0
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞**: {avg_coverage:.1%}\n")
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è**: {avg_compression:.1%}\n\n")

        # –†–µ–∑—é–º–µ –ø–æ —Å—Ç–∞—Ç—å—è–º
        for item in summaries:
            lines.append(f"## {item['title']}\n\n")
            lines.append(f"`{item['path']}`\n\n")

            # –û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ–∑—é–º–µ
            lines.append("### –†–µ–∑—é–º–µ (Combined)\n\n")
            lines.append(f"> {item['summary_combined']}\n\n")

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            lines.append("**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞**: " + ", ".join(item['keywords'][:5]) + "\n\n")

            # –ú–µ—Ç—Ä–∏–∫–∏
            lines.append(f"**–ú–µ—Ç—Ä–∏–∫–∏**: Coverage: {item['quality']['coverage']:.1%}, "
                        f"Compression: {item['quality']['compression_ratio']:.1%}\n\n")

            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—é–º–µ (–≤ –¥–µ—Ç–∞–ª—è—Ö)
            lines.append("<details>\n")
            lines.append("<summary>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã</summary>\n\n")
            lines.append(f"**TextRank**: {item['summary_textrank']}\n\n")
            lines.append(f"**TF-IDF**: {item['summary_tfidf']}\n\n")
            lines.append("</details>\n\n")

        output_file = self.root_dir / "ADVANCED_SUMMARIES.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown –æ—Ç—á—ë—Ç: {output_file}")

    def save_json(self, summaries):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON"""
        output_file = self.root_dir / "summaries.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({'summaries': summaries}, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Summary Generator')
    parser.add_argument('-m', '--method', choices=['combined', 'textrank', 'tfidf', 'position'],
                       default='combined', help='–ú–µ—Ç–æ–¥ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('-n', '--sentences', type=int, default=3,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–µ–∑—é–º–µ')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    generator = AdvancedSummaryGenerator(root_dir)
    summaries = generator.process_all()
    generator.generate_markdown_report(summaries)
    generator.save_json(summaries)


if __name__ == "__main__":
    main()
