#!/usr/bin/env python3
"""
Summary Generator - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: TextRank, LexRank, LSA
–ú–µ—Ç–æ–¥—ã: TF-IDF, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –≤–µ—Å–∞, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import json
import math
import argparse
from typing import Dict, List, Tuple, Set
import hashlib


class SentenceImportanceAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
    """

    def __init__(self, stop_words: Set[str]):
        self.stop_words = stop_words

    def analyze_sentence_features(self, sentence: str, position: int, total_sentences: int) -> Dict[str, float]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        –ü—Ä–∏–∑–Ω–∞–∫–∏:
        - –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        - –ü–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
        - –ù–∞–ª–∏—á–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        - –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–≤–∞–∂–Ω—ã—Ö –ø–æ–Ω—è—Ç–∏–π)
        - –ù–∞–ª–∏—á–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã)
        """
        words = sentence.split()

        # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è)
        length_score = len(words) / 50.0  # —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ ~25 —Å–ª–æ–≤
        length_score = min(1.0, length_score)

        # –ü–æ–∑–∏—Ü–∏—è (–ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∞–∂–Ω–µ–µ)
        if position < 3:
            position_score = 1.0
        elif position >= total_sentences - 2:
            position_score = 0.8
        else:
            position_score = 0.5

        # –ß–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ü–∏—Ñ—Ä—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –¥–∞—Ç—ã)
        has_numbers = bool(re.search(r'\d+', sentence))
        numbers_score = 0.7 if has_numbers else 0.0

        # –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã)
        capitalized = re.findall(r'\b[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+\b', sentence)
        entities_score = min(1.0, len(capitalized) / 3.0)

        # –ö–ª—é—á–µ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        key_markers = ['–≤–∞–∂–Ω–æ', '–≥–ª–∞–≤–Ω–æ–µ', '–æ—Å–Ω–æ–≤–Ω–æ–π', '–∫–ª—é—á–µ–≤–æ–π', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Å–ª–µ–¥—É–µ—Ç',
                      '–≤–∞–∂–Ω—ã–π', '–≥–ª–∞–≤–Ω—ã–π', 'central', 'important', 'key', 'essential', 'main']
        has_markers = any(marker in sentence.lower() for marker in key_markers)
        markers_score = 0.8 if has_markers else 0.0

        # –¶–∏—Ç–∞—Ç—ã –∏ –∫–∞–≤—ã—á–∫–∏
        has_quotes = bool(re.search(r'[¬´¬ª""]', sentence))
        quotes_score = 0.6 if has_quotes else 0.0

        return {
            'length': length_score,
            'position': position_score,
            'numbers': numbers_score,
            'entities': entities_score,
            'markers': markers_score,
            'quotes': quotes_score
        }

    def calculate_importance_score(self, features: Dict[str, float], weights: Dict[str, float] = None) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–µ—Å–∞:
        - position: 0.25
        - entities: 0.20
        - markers: 0.20
        - length: 0.15
        - numbers: 0.10
        - quotes: 0.10
        """
        if weights is None:
            weights = {
                'position': 0.25,
                'entities': 0.20,
                'markers': 0.20,
                'length': 0.15,
                'numbers': 0.10,
                'quotes': 0.10
            }

        score = sum(features.get(key, 0) * weight for key, weight in weights.items())
        return score


class SummaryDiversityScorer:
    """
    –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—é–º–µ
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–µ–∑—é–º–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
    """

    def __init__(self):
        pass

    def calculate_diversity_metrics(self, selected_sentences: List[str], all_sentences: List[str]) -> Dict[str, any]:
        """
        –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—é–º–µ
        """
        if not selected_sentences or not all_sentences:
            return {'diversity_score': 0, 'coverage': 0}

        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (–ø–æ–∫—Ä—ã—Ç–∏–µ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞)
        positions = []
        for sel_sent in selected_sentences:
            if sel_sent in all_sentences:
                positions.append(all_sentences.index(sel_sent))

        if not positions:
            return {'diversity_score': 0, 'coverage': 0}

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
        positions_sorted = sorted(positions)
        distances = []
        for i in range(len(positions_sorted) - 1):
            distances.append(positions_sorted[i + 1] - positions_sorted[i])

        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        if distances:
            avg_distance = sum(distances) / len(distances)
            # –ò–¥–µ–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            ideal_distance = len(all_sentences) / len(selected_sentences)
            uniformity = 1.0 - abs(avg_distance - ideal_distance) / ideal_distance
            uniformity = max(0, min(1, uniformity))
        else:
            uniformity = 1.0

        # –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞)
        range_coverage = (max(positions) - min(positions)) / len(all_sentences) if len(all_sentences) > 1 else 0

        # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞)
        all_words = []
        for sent in selected_sentences:
            words = re.findall(r'\b[–∞-—è—ëa-z]+\b', sent.lower())
            all_words.extend(words)

        lexical_diversity = len(set(all_words)) / len(all_words) if all_words else 0

        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        diversity_score = (uniformity * 0.4 + range_coverage * 0.3 + lexical_diversity * 0.3)

        return {
            'diversity_score': round(diversity_score, 3),
            'uniformity': round(uniformity, 3),
            'range_coverage': round(range_coverage, 3),
            'lexical_diversity': round(lexical_diversity, 3),
            'positions': positions
        }

    def calculate_redundancy(self, sentences: List[str]) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å (–ø–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å) –≤ —Ä–µ–∑—é–º–µ

        –ù–∏–∑–∫–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å = —Ö–æ—Ä–æ—à–æ
        """
        if len(sentences) < 2:
            return 0.0

        # –°—Ä–∞–≤–Ω–∏—Ç—å –≤—Å–µ –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        total_similarity = 0.0
        pairs = 0

        for i in range(len(sentences)):
            words_i = set(re.findall(r'\b[–∞-—è—ëa-z]+\b', sentences[i].lower()))

            for j in range(i + 1, len(sentences)):
                words_j = set(re.findall(r'\b[–∞-—è—ëa-z]+\b', sentences[j].lower()))

                if words_i and words_j:
                    intersection = len(words_i & words_j)
                    union = len(words_i | words_j)
                    similarity = intersection / union if union > 0 else 0
                    total_similarity += similarity
                    pairs += 1

        avg_similarity = total_similarity / pairs if pairs > 0 else 0
        return round(avg_similarity, 3)


class TopicModelingSummarizer:
    """
    –†–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–¥–µ–ª–µ–Ω–∏—é —Ç–µ–º
    """

    def __init__(self, stop_words: Set[str]):
        self.stop_words = stop_words

    def extract_topics(self, sentences: List[str], num_topics: int = 3) -> Dict[int, List[str]]:
        """
        –ò–∑–≤–ª–µ—á—å —Ç–µ–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞

        –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –æ–±—â–∏–º —Å–ª–æ–≤–∞–º
        """
        if not sentences:
            return {}

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        tokenized = []
        for sentence in sentences:
            words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', sentence.lower())
            words = [w for w in words if w not in self.stop_words]
            tokenized.append(words)

        # –ù–∞–π—Ç–∏ —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã)
        all_words = []
        for words in tokenized:
            all_words.extend(words)

        word_freq = Counter(all_words)
        topic_words = [word for word, _ in word_freq.most_common(num_topics * 3)]

        # –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ç–µ–º–∞–º
        topics = defaultdict(list)

        for i, words in enumerate(tokenized):
            # –ù–∞–π—Ç–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —Ç–µ–º—É –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            topic_scores = defaultdict(int)

            for word in words:
                if word in topic_words:
                    # –ö–∞–∫–∞—è —Ç–µ–º–∞?
                    topic_id = topic_words.index(word) % num_topics
                    topic_scores[topic_id] += 1

            if topic_scores:
                dominant_topic = max(topic_scores, key=topic_scores.get)
                topics[dominant_topic].append(sentences[i])

        return dict(topics)

    def summarize_by_topics(self, sentences: List[str], max_sentences: int = 3) -> Tuple[str, Dict]:
        """
        –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—é–º–µ, –≤—ã–±–∏—Ä–∞—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–º
        """
        num_topics = min(3, len(sentences) // 2)
        topics = self.extract_topics(sentences, num_topics=num_topics)

        if not topics:
            return sentences[0] if sentences else "", {}

        # –í—ã–±—Ä–∞—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é –∏–∑ –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
        selected = []
        sentences_per_topic = max(1, max_sentences // len(topics))

        for topic_id, topic_sentences in sorted(topics.items()):
            # –í–∑—è—Ç—å –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–∑ —Ç–µ–º—ã (–æ–±—ã—á–Ω–æ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–æ–µ)
            selected.extend(topic_sentences[:sentences_per_topic])

        # –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç, –¥–æ–±–∞–≤–∏—Ç—å –µ—â–µ
        if len(selected) < max_sentences:
            remaining = max_sentences - len(selected)
            for topic_id, topic_sentences in sorted(topics.items()):
                if remaining <= 0:
                    break
                additional = topic_sentences[sentences_per_topic:sentences_per_topic + remaining]
                selected.extend(additional)
                remaining -= len(additional)

        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫
        selected_ordered = []
        for sent in sentences:
            if sent in selected and sent not in selected_ordered:
                selected_ordered.append(sent)
                if len(selected_ordered) >= max_sentences:
                    break

        summary = ' '.join(selected_ordered[:max_sentences]) + '.'

        return summary, {
            'num_topics': len(topics),
            'topics': {k: len(v) for k, v in topics.items()}
        }


class AbstractiveSummarizer:
    """
    –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤
    """

    def __init__(self, stop_words: Set[str]):
        self.stop_words = stop_words

    def extract_key_phrases(self, text: str, num_phrases: int = 5) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã (–±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã)
        """
        words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())
        words = [w for w in words if w not in self.stop_words]

        # –ë–∏–≥—Ä–∞–º–º—ã
        bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words) - 1)]

        # –¢—Ä–∏–≥—Ä–∞–º–º—ã
        trigrams = [f"{words[i]} {words[i+1]} {words[i+2]}" for i in range(len(words) - 2)]

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        phrase_freq = Counter(bigrams + trigrams)

        return [phrase for phrase, _ in phrase_freq.most_common(num_phrases)]

    def generate_template_summary(self, content: str, keywords: List[str], key_phrases: List[str]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤

        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã
        """
        if not keywords:
            return "–†–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ."

        # –®–∞–±–ª–æ–Ω—ã
        templates = [
            "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç {topic}, –≤–∫–ª—é—á–∞—è {aspects}.",
            "–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã: {topics}. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è {details}.",
            "–î–æ–∫—É–º–µ–Ω—Ç –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç {main_topic} –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã: {related}.",
        ]

        # –ó–∞–ø–æ–ª–Ω–∏—Ç—å —à–∞–±–ª–æ–Ω
        if key_phrases:
            topic = key_phrases[0] if len(key_phrases) > 0 else keywords[0]
            aspects = ", ".join(key_phrases[1:3]) if len(key_phrases) > 1 else ", ".join(keywords[1:3])
            topics = ", ".join(keywords[:3])
            details = ", ".join(key_phrases[:2]) if key_phrases else ", ".join(keywords[:2])
            main_topic = keywords[0]
            related = ", ".join(keywords[1:4])
        else:
            topic = keywords[0] if keywords else "—Ç–µ–º–∞"
            aspects = ", ".join(keywords[1:3]) if len(keywords) > 1 else ""
            topics = ", ".join(keywords[:3])
            details = ", ".join(keywords[:2])
            main_topic = keywords[0] if keywords else "—Ç–µ–º–∞"
            related = ", ".join(keywords[1:4]) if len(keywords) > 1 else ""

        # –í—ã–±—Ä–∞—Ç—å —à–∞–±–ª–æ–Ω
        template = templates[0]

        summary = template.format(topic=topic, aspects=aspects, topics=topics,
                                 details=details, main_topic=main_topic, related=related)

        return summary

    def create_bullet_summary(self, sentences: List[str], max_points: int = 5) -> List[str]:
        """
        –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—é–º–µ –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤
        """
        if not sentences:
            return []

        # –í—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        # –ö—Ä–∏—Ç–µ—Ä–∏–π: –¥–ª–∏–Ω–∞ –æ—Ç 10 –¥–æ 30 —Å–ª–æ–≤
        candidates = []

        for sent in sentences:
            words = sent.split()
            if 10 <= len(words) <= 30:
                candidates.append(sent)

        if not candidates:
            candidates = sentences

        # –í–∑—è—Ç—å –ø–µ—Ä–≤—ã–µ max_points
        return candidates[:max_points]


class AdvancedSummaryGenerator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stop_words = set([
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '–æ', '–æ—Ç', '–∏–∑', '—É', '–∑–∞', '—á—Ç–æ', '–∫–∞–∫',
            '—ç—Ç–æ', '–≤—Å–µ', '–µ—â–µ', '—É–∂–µ', '—Ç–æ–ª—å–∫–æ', '—Ç–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '—ç—Ç–æ—Ç', '–≤–µ—Å—å', '—Å–≤–æ–π',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had'
        ])

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        self.importance_analyzer = SentenceImportanceAnalyzer(self.stop_words)
        self.diversity_scorer = SummaryDiversityScorer()
        self.topic_summarizer = TopicModelingSummarizer(self.stop_words)
        self.abstractive_summarizer = AbstractiveSummarizer(self.stop_words)

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def tokenize(self, text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())
        return [w for w in words if w not in self.stop_words]

    def split_sentences(self, text):
        """–†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –£–¥–∞–ª–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ markdown
        text = re.sub(r'^#{1,6}\s+.+$', '', text, flags=re.MULTILINE)

        # –†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+\s+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]

        return sentences

    def calculate_tf_idf(self, sentences):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤ -> –¥–æ–∫—É–º–µ–Ω—Ç—ã
        word_doc_freq = defaultdict(int)
        sentence_words = []

        for sentence in sentences:
            words = self.tokenize(sentence)
            sentence_words.append(words)

            for word in set(words):
                word_doc_freq[word] += 1

        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        total_sentences = len(sentences)
        sentence_scores = []

        for i, words in enumerate(sentence_words):
            score = 0.0
            word_count = len(words)

            if word_count == 0:
                sentence_scores.append((i, 0.0))
                continue

            word_freq = Counter(words)

            for word, freq in word_freq.items():
                tf = freq / word_count
                idf = math.log(total_sentences / (1 + word_doc_freq[word]))
                score += tf * idf

            sentence_scores.append((i, score))

        return sentence_scores

    def calculate_position_score(self, index, total):
        """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –≤–µ—Å (–ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∞–∂–Ω–µ–µ)"""
        if index < 3:  # –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            return 2.0
        elif index >= total - 2:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2
            return 1.5
        else:
            return 1.0

    def calculate_similarity(self, words1, words2):
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏"""
        set1 = set(words1)
        set2 = set(words2)

        if not set1 or not set2:
            return 0.0

        intersection = len(set1 & set2)
        denominator = math.sqrt(len(set1) * len(set2))

        return intersection / denominator if denominator > 0 else 0.0

    def textrank_score(self, sentences):
        """TextRank –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        n = len(sentences)

        if n == 0:
            return []

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        tokenized = [self.tokenize(s) for s in sentences]

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarity_matrix = [[0.0] * n for _ in range(n)]

        for i in range(n):
            for j in range(i + 1, n):
                sim = self.calculate_similarity(tokenized[i], tokenized[j])
                similarity_matrix[i][j] = sim
                similarity_matrix[j][i] = sim

        # PageRank –∏—Ç–µ—Ä–∞—Ü–∏–∏
        scores = [1.0] * n
        damping = 0.85
        iterations = 30

        for _ in range(iterations):
            new_scores = [0.0] * n

            for i in range(n):
                score = (1 - damping)

                for j in range(n):
                    if i != j and similarity_matrix[j][i] > 0:
                        sum_weights = sum(similarity_matrix[j][k] for k in range(n))
                        if sum_weights > 0:
                            score += damping * scores[j] * (similarity_matrix[j][i] / sum_weights)

                new_scores[i] = score

            scores = new_scores

        return [(i, score) for i, score in enumerate(scores)]

    def generate_extractive_summary(self, content, max_sentences=3, method='combined'):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–≤–ª–µ–∫–∞—é—â–µ–≥–æ —Ä–µ–∑—é–º–µ"""
        sentences = self.split_sentences(content)

        if not sentences:
            return "–†–µ–∑—é–º–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ."

        if len(sentences) <= max_sentences:
            return ' '.join(sentences)

        # –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        if method == 'tfidf':
            scores = self.calculate_tf_idf(sentences)
        elif method == 'textrank':
            scores = self.textrank_score(sentences)
        elif method == 'position':
            scores = [(i, self.calculate_position_score(i, len(sentences)))
                     for i in range(len(sentences))]
        else:  # combined
            tfidf_scores = dict(self.calculate_tf_idf(sentences))
            textrank_scores = dict(self.textrank_score(sentences))

            scores = []
            for i in range(len(sentences)):
                pos_score = self.calculate_position_score(i, len(sentences))
                combined = (
                    tfidf_scores.get(i, 0) * 0.4 +
                    textrank_scores.get(i, 0) * 0.4 +
                    pos_score * 0.2
                )
                scores.append((i, combined))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        scores.sort(key=lambda x: -x[1])

        # –í–∑—è—Ç—å —Ç–æ–ø N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        top_indices = sorted([idx for idx, _ in scores[:max_sentences]])

        # –°–æ–±—Ä–∞—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        summary_sentences = [sentences[i] for i in top_indices]

        return ' '.join(summary_sentences) + '.'

    def extract_keywords(self, content, num_keywords=10):
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        words = self.tokenize(content)

        if not words:
            return []

        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        word_freq = Counter(words)

        # –¢–æ–ø —Å–ª–æ–≤–∞
        return [word for word, count in word_freq.most_common(num_keywords)]

    def calculate_summary_quality(self, original, summary):
        """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—é–º–µ"""
        original_words = set(self.tokenize(original))
        summary_words = set(self.tokenize(summary))

        if not original_words:
            return {'coverage': 0, 'compression_ratio': 0}

        # Coverage - —Å–∫–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –ø–æ–∫—Ä—ã—Ç–æ
        coverage = len(summary_words & original_words) / len(original_words)

        # Compression ratio
        compression = len(summary) / len(original) if len(original) > 0 else 0

        return {
            'coverage': round(coverage, 3),
            'compression_ratio': round(compression, 3)
        }

    def process_all(self):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–µ–∑—é–º–µ...\n")

        summaries = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
            summary_combined = self.generate_extractive_summary(content, max_sentences=3, method='combined')
            summary_textrank = self.generate_extractive_summary(content, max_sentences=3, method='textrank')
            summary_tfidf = self.generate_extractive_summary(content, max_sentences=3, method='tfidf')

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self.extract_keywords(content, num_keywords=10)

            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality = self.calculate_summary_quality(content, summary_combined)

            summaries.append({
                'path': article_path,
                'title': title,
                'summary_combined': summary_combined,
                'summary_textrank': summary_textrank,
                'summary_tfidf': summary_tfidf,
                'keywords': keywords,
                'quality': quality,
                'original_length': len(content),
                'summary_length': len(summary_combined)
            })

        print(f"   –†–µ–∑—é–º–µ —Å–æ–∑–¥–∞–Ω–æ –¥–ª—è {len(summaries)} —Å—Ç–∞—Ç–µ–π\n")

        return summaries

    def generate_markdown_report(self, summaries):
        """–°–æ–∑–¥–∞—Ç—å Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üìù –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç–µ–π\n\n")
        lines.append("> –°–æ–∑–¥–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF, TextRank –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–°—Ç–∞—Ç–µ–π**: {len(summaries)}\n")
        avg_coverage = sum(s['quality']['coverage'] for s in summaries) / len(summaries) if summaries else 0
        avg_compression = sum(s['quality']['compression_ratio'] for s in summaries) / len(summaries) if summaries else 0
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞**: {avg_coverage:.1%}\n")
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è**: {avg_compression:.1%}\n\n")

        # –†–µ–∑—é–º–µ –ø–æ —Å—Ç–∞—Ç—å—è–º
        for item in summaries:
            lines.append(f"## {item['title']}\n\n")
            lines.append(f"`{item['path']}`\n\n")

            # –û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ–∑—é–º–µ
            lines.append("### –†–µ–∑—é–º–µ (Combined)\n\n")
            lines.append(f"> {item['summary_combined']}\n\n")

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            lines.append("**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞**: " + ", ".join(item['keywords'][:5]) + "\n\n")

            # –ú–µ—Ç—Ä–∏–∫–∏
            lines.append(f"**–ú–µ—Ç—Ä–∏–∫–∏**: Coverage: {item['quality']['coverage']:.1%}, "
                        f"Compression: {item['quality']['compression_ratio']:.1%}\n\n")

            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—é–º–µ (–≤ –¥–µ—Ç–∞–ª—è—Ö)
            lines.append("<details>\n")
            lines.append("<summary>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã</summary>\n\n")
            lines.append(f"**TextRank**: {item['summary_textrank']}\n\n")
            lines.append(f"**TF-IDF**: {item['summary_tfidf']}\n\n")
            lines.append("</details>\n\n")

        output_file = self.root_dir / "ADVANCED_SUMMARIES.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown –æ—Ç—á—ë—Ç: {output_file}")

    def save_json(self, summaries):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON"""
        output_file = self.root_dir / "summaries.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({'summaries': summaries}, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")

    def comprehensive_analysis(self, content: str, max_sentences: int = 3) -> Dict[str, any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–µ–∑—é–º–µ –∏ –º–µ—Ç—Ä–∏–∫–∏
        """
        sentences = self.split_sentences(content)

        if not sentences:
            return {'error': 'No sentences found'}

        # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self.extract_keywords(content, num_keywords=10)

        # –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—é–º–µ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        summary_combined = self.generate_extractive_summary(content, max_sentences, 'combined')
        summary_textrank = self.generate_extractive_summary(content, max_sentences, 'textrank')
        summary_tfidf = self.generate_extractive_summary(content, max_sentences, 'tfidf')
        summary_position = self.generate_extractive_summary(content, max_sentences, 'position')

        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ
        summary_topics, topics_info = self.topic_summarizer.summarize_by_topics(sentences, max_sentences)

        # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ —Ä–µ–∑—é–º–µ
        key_phrases = self.abstractive_summarizer.extract_key_phrases(content, num_phrases=5)
        summary_abstractive = self.abstractive_summarizer.generate_template_summary(content, keywords, key_phrases)

        # Bullet points
        bullet_points = self.abstractive_summarizer.create_bullet_summary(sentences, max_points=5)

        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è combined
        quality = self.calculate_summary_quality(content, summary_combined)

        # Diversity –º–µ—Ç—Ä–∏–∫–∏
        summary_sents = self.split_sentences(summary_combined)
        diversity = self.diversity_scorer.calculate_diversity_metrics(summary_sents, sentences)
        redundancy = self.diversity_scorer.calculate_redundancy(summary_sents)

        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Ç–æ–ø-5)
        sentence_importance = []
        for i, sent in enumerate(sentences[:10]):  # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 10
            features = self.importance_analyzer.analyze_sentence_features(sent, i, len(sentences))
            importance_score = self.importance_analyzer.calculate_importance_score(features)
            sentence_importance.append({
                'sentence': sent[:100] + '...' if len(sent) > 100 else sent,
                'importance_score': round(importance_score, 3),
                'features': features
            })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sentence_importance.sort(key=lambda x: -x['importance_score'])

        return {
            'summaries': {
                'combined': summary_combined,
                'textrank': summary_textrank,
                'tfidf': summary_tfidf,
                'position': summary_position,
                'topics': summary_topics,
                'abstractive': summary_abstractive
            },
            'bullet_points': bullet_points,
            'keywords': keywords,
            'key_phrases': key_phrases,
            'quality_metrics': quality,
            'diversity_metrics': diversity,
            'redundancy': redundancy,
            'topics_info': topics_info,
            'top_sentences': sentence_importance[:5],
            'statistics': {
                'total_sentences': len(sentences),
                'original_length': len(content),
                'summary_length': len(summary_combined)
            }
        }

    def analyze_all_with_metrics(self) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        results = []

        print("\nüìä –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            analysis = self.comprehensive_analysis(content, max_sentences=3)

            results.append({
                'path': article_path,
                'title': title,
                **analysis
            })

        print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(results)}\n")

        return results

    def export_html_summaries(self, summaries: List[Dict], output_file: str):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—é–º–µ –≤ HTML —Å –∫—Ä–∞—Å–∏–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º"""
        html = []
        html.append('<!DOCTYPE html>\n<html lang="ru">\n<head>\n')
        html.append('<meta charset="UTF-8">\n')
        html.append('<meta name="viewport" content="width=device-width, initial-scale=1.0">\n')
        html.append('<title>–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç–µ–π</title>\n')
        html.append('<style>\n')
        html.append('body { font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif; ')
        html.append('max-width: 1400px; margin: 0 auto; padding: 20px; background: #f8f9fa; }\n')
        html.append('h1 { color: #2c3e50; border-bottom: 4px solid #3498db; padding-bottom: 15px; }\n')
        html.append('h2 { color: #34495e; margin-top: 30px; }\n')
        html.append('.article { background: white; border-radius: 10px; padding: 25px; ')
        html.append('margin-bottom: 25px; box-shadow: 0 3px 6px rgba(0,0,0,0.1); }\n')
        html.append('.title { font-size: 1.6em; font-weight: bold; color: #2c3e50; margin-bottom: 10px; }\n')
        html.append('.summary-box { background: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; ')
        html.append('margin: 15px 0; border-radius: 5px; }\n')
        html.append('.summary-label { font-weight: bold; color: #7f8c8d; font-size: 0.9em; ')
        html.append('text-transform: uppercase; margin-bottom: 8px; }\n')
        html.append('.summary-text { color: #2c3e50; line-height: 1.6; }\n')
        html.append('.keywords { margin: 15px 0; }\n')
        html.append('.keyword-tag { display: inline-block; background: #3498db; color: white; ')
        html.append('padding: 5px 12px; border-radius: 15px; margin: 3px; font-size: 0.85em; }\n')
        html.append('.metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); ')
        html.append('gap: 10px; margin: 15px 0; }\n')
        html.append('.metric { background: #fff; border: 1px solid #ddd; padding: 10px; ')
        html.append('border-radius: 5px; text-align: center; }\n')
        html.append('.metric-value { font-size: 1.4em; font-weight: bold; color: #3498db; }\n')
        html.append('.metric-label { font-size: 0.8em; color: #7f8c8d; margin-top: 5px; }\n')
        html.append('.bullet-list { list-style: none; padding-left: 0; }\n')
        html.append('.bullet-list li { padding: 8px 0; padding-left: 25px; ')
        html.append('border-left: 3px solid #3498db; margin: 5px 0; }\n')
        html.append('.tabs { display: flex; gap: 10px; margin: 15px 0; }\n')
        html.append('.tab { padding: 10px 20px; background: #ecf0f1; border-radius: 5px 5px 0 0; ')
        html.append('cursor: pointer; font-weight: bold; color: #7f8c8d; }\n')
        html.append('.tab.active { background: #3498db; color: white; }\n')
        html.append('</style>\n</head>\n<body>\n')

        html.append('<h1>üìù –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç–µ–π</h1>\n')
        html.append('<p style="color: #7f8c8d;">–°–æ–∑–¥–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF, TextRank, —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è</p>\n')

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        html.append('<div class="article">\n')
        html.append('<h2>–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h2>\n')
        total_articles = len(summaries)
        avg_coverage = sum(s.get('quality_metrics', {}).get('coverage', 0) for s in summaries) / total_articles if total_articles > 0 else 0
        avg_diversity = sum(s.get('diversity_metrics', {}).get('diversity_score', 0) for s in summaries) / total_articles if total_articles > 0 else 0

        html.append('<div class="metrics">\n')
        html.append(f'<div class="metric"><div class="metric-value">{total_articles}</div>')
        html.append('<div class="metric-label">–°—Ç–∞—Ç–µ–π</div></div>\n')
        html.append(f'<div class="metric"><div class="metric-value">{avg_coverage:.1%}</div>')
        html.append('<div class="metric-label">–°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞</div></div>\n')
        html.append(f'<div class="metric"><div class="metric-value">{avg_diversity:.2f}</div>')
        html.append('<div class="metric-label">–°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç—å</div></div>\n')
        html.append('</div>\n')
        html.append('</div>\n')

        # –ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è
        for item in summaries:
            html.append('<div class="article">\n')
            html.append(f'<div class="title">{item["title"]}</div>\n')
            html.append(f'<div style="color: #7f8c8d; font-size: 0.9em; margin-bottom: 15px;">{item["path"]}</div>\n')

            # –û—Å–Ω–æ–≤–Ω–æ–µ —Ä–µ–∑—é–º–µ
            summary = item.get('summaries', {}).get('combined', '')
            html.append('<div class="summary-box">\n')
            html.append('<div class="summary-label">üìå –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ</div>\n')
            html.append(f'<div class="summary-text">{summary}</div>\n')
            html.append('</div>\n')

            # Bullet points
            bullets = item.get('bullet_points', [])
            if bullets:
                html.append('<div style="margin: 15px 0;">\n')
                html.append('<div class="summary-label">üî∏ –ö–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã:</div>\n')
                html.append('<ul class="bullet-list">\n')
                for bullet in bullets[:5]:
                    html.append(f'<li>{bullet}</li>\n')
                html.append('</ul>\n')
                html.append('</div>\n')

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = item.get('keywords', [])[:8]
            if keywords:
                html.append('<div class="keywords">\n')
                html.append('<div class="summary-label">üè∑Ô∏è –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:</div>\n')
                for kw in keywords:
                    html.append(f'<span class="keyword-tag">{kw}</span>\n')
                html.append('</div>\n')

            # –ú–µ—Ç—Ä–∏–∫–∏
            quality = item.get('quality_metrics', {})
            diversity = item.get('diversity_metrics', {})
            redundancy = item.get('redundancy', 0)

            html.append('<div class="metrics">\n')
            html.append(f'<div class="metric"><div class="metric-value">{quality.get("coverage", 0):.1%}</div>')
            html.append('<div class="metric-label">–ü–æ–ª–Ω–æ—Ç–∞</div></div>\n')
            html.append(f'<div class="metric"><div class="metric-value">{quality.get("compression_ratio", 0):.1%}</div>')
            html.append('<div class="metric-label">–ö–æ–º–ø—Ä–µ—Å—Å–∏—è</div></div>\n')
            html.append(f'<div class="metric"><div class="metric-value">{diversity.get("diversity_score", 0):.2f}</div>')
            html.append('<div class="metric-label">–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ</div></div>\n')
            html.append(f'<div class="metric"><div class="metric-value">{redundancy:.2f}</div>')
            html.append('<div class="metric-label">–ò–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å</div></div>\n')
            html.append('</div>\n')

            html.append('</div>\n')

        html.append('</body>\n</html>')

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(''.join(html))

        print(f"‚úÖ HTML —ç–∫—Å–ø–æ—Ä—Ç: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='üìù Advanced Summary Generator - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                                          # –ë–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
  %(prog)s --analyze                                # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
  %(prog)s --method textrank                        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ TextRank
  %(prog)s --topics                                 # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ
  %(prog)s --abstractive                            # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ
  %(prog)s --json output.json                       # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
  %(prog)s --html output.html                       # –≠–∫—Å–ø–æ—Ä—Ç –≤ HTML
  %(prog)s --all                                    # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ + –≤—Å–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
        """
    )

    # –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã
    parser.add_argument(
        '-m', '--method',
        choices=['combined', 'textrank', 'tfidf', 'position'],
        default='combined',
        help='–ú–µ—Ç–æ–¥ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: combined)'
    )

    parser.add_argument(
        '-n', '--sentences',
        type=int,
        default=3,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–µ–∑—é–º–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)'
    )

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –æ–ø—Ü–∏–∏
    parser.add_argument(
        '--analyze',
        action='store_true',
        help='–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è'
    )

    parser.add_argument(
        '--topics',
        action='store_true',
        help='–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º)'
    )

    parser.add_argument(
        '--abstractive',
        action='store_true',
        help='–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)'
    )

    parser.add_argument(
        '--diversity',
        action='store_true',
        help='–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—é–º–µ'
    )

    parser.add_argument(
        '--importance',
        action='store_true',
        help='–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π'
    )

    # –≠–∫—Å–ø–æ—Ä—Ç
    parser.add_argument(
        '--json',
        metavar='FILE',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON'
    )

    parser.add_argument(
        '--html',
        metavar='FILE',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ HTML —Å –∫—Ä–∞—Å–∏–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º'
    )

    parser.add_argument(
        '--markdown',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å Markdown –æ—Ç—á—ë—Ç'
    )

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
    parser.add_argument(
        '--all',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –≤–∏–¥—ã –∞–Ω–∞–ª–∏–∑–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    generator = AdvancedSummaryGenerator(root_dir)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ --all
    if args.all:
        args.analyze = True
        args.topics = True
        args.abstractive = True
        args.diversity = True
        args.importance = True
        args.markdown = True
        if not args.json:
            args.json = str(root_dir / "summaries_comprehensive.json")
        if not args.html:
            args.html = str(root_dir / "summaries_comprehensive.html")

    # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    if args.analyze or args.topics or args.abstractive or args.diversity or args.importance:
        print("üîç –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ...\n")
        results = generator.analyze_all_with_metrics()

        if not results:
            print("‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(results)}")

        avg_coverage = sum(r.get('quality_metrics', {}).get('coverage', 0) for r in results) / len(results) if results else 0
        avg_compression = sum(r.get('quality_metrics', {}).get('compression_ratio', 0) for r in results) / len(results) if results else 0
        avg_diversity = sum(r.get('diversity_metrics', {}).get('diversity_score', 0) for r in results) / len(results) if results else 0
        avg_redundancy = sum(r.get('redundancy', 0) for r in results) / len(results) if results else 0

        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞ —Ä–µ–∑—é–º–µ: {avg_coverage:.1%}")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è: {avg_compression:.1%}")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç—å: {avg_diversity:.2f}")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å: {avg_redundancy:.2f}")

        # –¢–æ–ø –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        print(f"\nüèÜ –¢–æ–ø-5 –ª—É—á—à–∏—Ö —Ä–µ–∑—é–º–µ (–ø–æ –ø–æ–ª–Ω–æ—Ç–µ):")
        sorted_by_coverage = sorted(results, key=lambda x: x.get('quality_metrics', {}).get('coverage', 0), reverse=True)
        for i, r in enumerate(sorted_by_coverage[:5], 1):
            title = r['title']
            coverage = r.get('quality_metrics', {}).get('coverage', 0)
            diversity = r.get('diversity_metrics', {}).get('diversity_score', 0)
            print(f"   {i}. {title}: {coverage:.1%} (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {diversity:.2f})")

        # –≠–∫—Å–ø–æ—Ä—Ç—ã
        if args.json:
            json_path = root_dir / args.json if not Path(args.json).is_absolute() else Path(args.json)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({'summaries': results}, f, ensure_ascii=False, indent=2)
            print(f"\n‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç: {json_path}")

        if args.html:
            html_path = root_dir / args.html if not Path(args.html).is_absolute() else Path(args.html)
            generator.export_html_summaries(results, str(html_path))

        print()

    # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    elif args.markdown or (not args.json and not args.html):
        summaries = generator.process_all()

        if args.markdown:
            generator.generate_markdown_report(summaries)

        if args.json:
            json_path = root_dir / args.json if not Path(args.json).is_absolute() else Path(args.json)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({'summaries': summaries}, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç: {json_path}")
        else:
            generator.save_json(summaries)

    # –¢–æ–ª—å–∫–æ —ç–∫—Å–ø–æ—Ä—Ç
    elif args.json or args.html:
        summaries = generator.process_all()

        if args.json:
            json_path = root_dir / args.json if not Path(args.json).is_absolute() else Path(args.json)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({'summaries': summaries}, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ JSON —ç–∫—Å–ø–æ—Ä—Ç: {json_path}")

        if args.html:
            # –î–ª—è HTML –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            results = generator.analyze_all_with_metrics()
            html_path = root_dir / args.html if not Path(args.html).is_absolute() else Path(args.html)
            generator.export_html_summaries(results, str(html_path))


if __name__ == "__main__":
    main()
