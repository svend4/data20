#!/usr/bin/env python3
"""
External Links Tracker - –¢—Ä–µ–∫–∏–Ω–≥ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –∫–∞—Ç–∞–ª–æ–≥–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Web archives, Internet Archive
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict
from urllib.parse import urlparse
import json


class ExternalLinksTracker:
    """–¢—Ä–µ–∫–µ—Ä –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Å—ã–ª–æ–∫
        self.links = defaultdict(lambda: {
            'url': '',
            'domain': '',
            'count': 0,
            'articles': []
        })

        self.articles_with_links = {}

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def extract_external_links(self, content):
        """–ò–∑–≤–ª–µ—á—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏"""
        links = []

        # Markdown —Å—Å—ã–ª–∫–∏
        markdown_links = re.findall(r'\[([^\]]+)\]\((https?://[^)]+)\)', content)

        for text, url in markdown_links:
            # –û—á–∏—Å—Ç–∏—Ç—å URL –æ—Ç —è–∫–æ—Ä–µ–π –∏ query params (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            clean_url = url.split('#')[0]

            links.append({
                'url': url,
                'clean_url': clean_url,
                'text': text
            })

        # –ì–æ–ª—ã–µ URL
        bare_urls = re.findall(r'(?<!\()(https?://[^\s<>)\]]+)', content)

        for url in bare_urls:
            clean_url = url.split('#')[0]
            links.append({
                'url': url,
                'clean_url': clean_url,
                'text': ''
            })

        return links

    def analyze_all(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üîó –ê–Ω–∞–ª–∏–∑ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            # –ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏
            external_links = self.extract_external_links(content)

            if external_links:
                self.articles_with_links[article_path] = {
                    'title': title,
                    'links': external_links
                }

                # –ö–∞—Ç–∞–ª–æ–≥–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫–∏
                for link in external_links:
                    url = link['clean_url']

                    # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–º–µ–Ω–∞
                    try:
                        parsed = urlparse(url)
                        domain = parsed.netloc
                    except:
                        domain = 'unknown'

                    self.links[url]['url'] = url
                    self.links[url]['domain'] = domain
                    self.links[url]['count'] += 1
                    self.links[url]['articles'].append({
                        'path': article_path,
                        'title': title,
                        'context': link['text']
                    })

        print(f"   –°—Ç–∞—Ç–µ–π —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏: {len(self.articles_with_links)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {len(self.links)}\n")

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîó –í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏\n\n")
        lines.append("> –ö–∞—Ç–∞–ª–æ–≥ –≤—Å–µ—Ö –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_links = sum(link['count'] for link in self.links.values())

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
        by_domain = defaultdict(list)
        for url, data in self.links.items():
            by_domain[data['domain']].append((url, data))

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫**: {len(self.links)}\n")
        lines.append(f"- **–í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π**: {total_links}\n")
        lines.append(f"- **–î–æ–º–µ–Ω–æ–≤**: {len(by_domain)}\n\n")

        # –¢–æ–ø –¥–æ–º–µ–Ω–æ–≤
        lines.append("## –¢–æ–ø-10 –¥–æ–º–µ–Ω–æ–≤\n\n")

        domain_stats = [
            (domain, sum(link['count'] for _, link in links))
            for domain, links in by_domain.items()
        ]
        domain_stats.sort(key=lambda x: -x[1])

        for i, (domain, count) in enumerate(domain_stats[:10], 1):
            lines.append(f"{i}. **{domain}** ‚Äî {count} —Å—Å—ã–ª–æ–∫\n")

        lines.append("\n")

        # –ü–æ –¥–æ–º–µ–Ω–∞–º
        lines.append("## –ü–æ –¥–æ–º–µ–Ω–∞–º\n\n")

        for domain in sorted(by_domain.keys()):
            links = by_domain[domain]

            lines.append(f"### {domain} ({len(links)} —Å—Å—ã–ª–æ–∫)\n\n")

            for url, data in sorted(links, key=lambda x: -x[1]['count'])[:10]:
                lines.append(f"#### [{url}]({url})\n\n")
                lines.append(f"**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π**: {data['count']}\n\n")
                lines.append("**–°—Ç–∞—Ç—å–∏:**\n")

                for article in data['articles']:
                    lines.append(f"- [{article['title']}]({article['path']})")
                    if article['context']:
                        lines.append(f" ‚Äî *\"{article['context']}\"*")
                    lines.append("\n")

                lines.append("\n")

            if len(links) > 10:
                lines.append(f"*...–∏ –µ—â—ë {len(links) - 10} —Å—Å—ã–ª–æ–∫*\n\n")

        output_file = self.root_dir / "EXTERNAL_LINKS.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ JSON"""
        data = {
            'total_unique_links': len(self.links),
            'total_uses': sum(link['count'] for link in self.links.values()),
            'links': {
                url: {
                    'domain': link['domain'],
                    'count': link['count'],
                    'articles': link['articles']
                }
                for url, link in self.links.items()
            }
        }

        output_file = self.root_dir / "external_links.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")


def main():
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    tracker = ExternalLinksTracker(root_dir)
    tracker.analyze_all()
    tracker.generate_report()
    tracker.save_json()


if __name__ == "__main__":
    main()
