#!/usr/bin/env python3
"""
Advanced Link Health Monitor - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–¥–æ—Ä–æ–≤—å—è —Å—Å—ã–ª–æ–∫

–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (—Ñ–∞–π–ª—ã, —è–∫–æ—Ä—è)
- –í–Ω–µ—à–Ω–∏–µ HTTP/HTTPS —Å—Å—ã–ª–∫–∏ —Å –ø–æ–ª–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
- SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
- –¶–µ–ø–æ—á–∫–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Å—ã–ª–æ–∫
- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π —Ç—Ä–µ–∫–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–º–µ–Ω—ã
- Health scoring (0-100)

Inspired by: LinkChecker, W3C Link Checker, Broken Link Checker

Author: Advanced Knowledge Management System
Version: 2.0
"""

from pathlib import Path
import re
import yaml
import json
import hashlib
import time
from datetime import datetime, timedelta
from collections import defaultdict
from urllib.parse import urlparse, urljoin
import argparse

# Optional dependencies –¥–ª—è HTTP checking
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    import ssl
    import socket
    from OpenSSL import crypto
    SSL_AVAILABLE = True
except ImportError:
    SSL_AVAILABLE = False


class LinkHealthMonitor:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä –∑–¥–æ—Ä–æ–≤—å—è —Å—Å—ã–ª–æ–∫

    Features:
    - Internal link validation (files, anchors)
    - External link checking (HTTP/HTTPS)
    - SSL certificate validation
    - Redirect chain detection (–ê‚ÜíB‚ÜíC‚ÜíD)
    - Link performance metrics (response time)
    - Historical status tracking
    - Health scoring (0-100)
    - Broken link suggestions
    - Link freshness tracking
    """

    def __init__(self, root_dir=".", cache_file=".link_health_cache.json"):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.cache_file = self.root_dir / cache_file

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.links = []  # –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        self.broken_links = []

        # Historical tracking
        self.history = self.load_history()

        # Performance tracking
        self.performance_stats = defaultdict(list)

        # External link cache (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –æ–¥–∏–Ω URL –º–Ω–æ–≥–æ —Ä–∞–∑)
        self.external_cache = {}

        # HTTP session —Å retry logic
        if REQUESTS_AVAILABLE:
            self.session = self.create_http_session()
        else:
            self.session = None

    def create_http_session(self, retries=3, backoff_factor=0.3):
        """–°–æ–∑–¥–∞—Ç—å HTTP session —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏"""
        session = requests.Session()

        retry_strategy = Retry(
            total=retries,
            backoff_factor=backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET"]
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # User-Agent —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–∏
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; LinkHealthMonitor/2.0; +https://github.com)'
        })

        return session

    def load_history(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–≤–µ—Ä–æ–∫"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_history(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–≤–µ—Ä–æ–∫"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, indent=2, ensure_ascii=False)

    def get_link_hash(self, url):
        """–•—ç—à —Å—Å—ã–ª–∫–∏ –¥–ª—è tracking"""
        return hashlib.md5(url.encode()).hexdigest()[:16]

    def extract_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ markdown —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n.*?\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return match.group(1)
        except:
            pass
        return None

    def extract_headings(self, content):
        """–ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —è–∫–æ—Ä–µ–π"""
        headings = []
        for line in content.split('\n'):
            match = re.match(r'^#{1,6}\s+(.+)$', line)
            if match:
                text = match.group(1).strip()
                # –°–æ–∑–¥–∞—Ç—å —è–∫–æ—Ä—å (GitHub-style)
                anchor = text.lower()
                anchor = re.sub(r'[^\w\s-]', '', anchor)
                anchor = re.sub(r'\s+', '-', anchor)
                anchor = re.sub(r'-+', '-', anchor)
                anchor = anchor.strip('-')
                headings.append(anchor)
        return headings

    def check_internal_link(self, file_path, link, link_text):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å—Å—ã–ª–∫—É (—Ñ–∞–π–ª/—è–∫–æ—Ä—å)"""
        result = {
            'url': link,
            'type': 'internal',
            'link_text': link_text,
            'source': str(file_path.relative_to(self.root_dir)),
            'status': 'ok',
            'health_score': 100.0,
            'issues': []
        }

        # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ —Ñ–∞–π–ª –∏ —è–∫–æ—Ä—å
        if '#' in link:
            file_part, anchor_part = link.split('#', 1)
        else:
            file_part = link
            anchor_part = None

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª
        if file_part:
            target = (file_path.parent / file_part).resolve()

            if not target.exists():
                result['status'] = 'broken_file'
                result['health_score'] = 0.0
                result['issues'].append(f"File not found: {file_part}")
                result['target'] = str(target.relative_to(self.root_dir)) if target.is_relative_to(self.root_dir) else str(target)
                self.broken_links.append(result)
                return result

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —è–∫–æ—Ä—å
        if anchor_part:
            if file_part:
                # –Ø–∫–æ—Ä—å –≤ –¥—Ä—É–≥–æ–º —Ñ–∞–π–ª–µ
                target = (file_path.parent / file_part).resolve()
                if target.exists():
                    target_content = self.extract_content(target)
                    if target_content:
                        target_anchors = set(self.extract_headings(target_content))
                        if anchor_part not in target_anchors:
                            result['status'] = 'broken_anchor'
                            result['health_score'] = 30.0  # –§–∞–π–ª –µ—Å—Ç—å, —è–∫–æ—Ä—è –Ω–µ—Ç
                            result['issues'].append(f"Anchor not found: #{anchor_part}")
                            result['anchor'] = anchor_part
                            result['available_anchors'] = list(target_anchors)[:5]  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø–æ–¥—Å–∫–∞–∑–∫–∏
                            self.broken_links.append(result)
            else:
                # –Ø–∫–æ—Ä—å –≤ —Ç–µ–∫—É—â–µ–º —Ñ–∞–π–ª–µ
                content = self.extract_content(file_path)
                if content:
                    local_anchors = set(self.extract_headings(content))
                    if anchor_part not in local_anchors:
                        result['status'] = 'broken_anchor'
                        result['health_score'] = 50.0
                        result['issues'].append(f"Local anchor not found: #{anchor_part}")
                        result['anchor'] = anchor_part
                        self.broken_links.append(result)

        return result

    def check_ssl_certificate(self, hostname, port=443):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç"""
        if not SSL_AVAILABLE:
            return {'valid': None, 'days_remaining': None, 'issuer': None}

        try:
            context = ssl.create_default_context()
            with socket.create_connection((hostname, port), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert_bin = ssock.getpeercert(binary_form=True)
                    cert = crypto.load_certificate(crypto.FILETYPE_ASN1, cert_bin)

                    # Expiry date
                    expiry_date = datetime.strptime(cert.get_notAfter().decode(), '%Y%m%d%H%M%SZ')
                    days_remaining = (expiry_date - datetime.now()).days

                    # Issuer
                    issuer = dict(cert.get_issuer().get_components())
                    issuer_cn = issuer.get(b'CN', b'Unknown').decode()

                    return {
                        'valid': True,
                        'days_remaining': days_remaining,
                        'issuer': issuer_cn,
                        'expiry_date': expiry_date.isoformat()
                    }
        except Exception as e:
            return {'valid': False, 'error': str(e)}

    def check_redirect_chain(self, url, max_redirects=5):
        """–û—Ç—Å–ª–µ–¥–∏—Ç—å —Ü–µ–ø–æ—á–∫—É —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤: A‚ÜíB‚ÜíC‚ÜíD"""
        chain = []
        current_url = url

        for i in range(max_redirects):
            try:
                response = self.session.get(current_url, allow_redirects=False, timeout=10)

                chain.append({
                    'url': current_url,
                    'status': response.status_code,
                    'step': i + 1
                })

                # –ï—Å–ª–∏ –Ω–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç - –≤—ã—Ö–æ–¥–∏–º
                if response.status_code not in [301, 302, 303, 307, 308]:
                    break

                # –°–ª–µ–¥—É—é—â–∏–π URL –≤ —Ü–µ–ø–æ—á–∫–µ
                if 'Location' in response.headers:
                    next_url = response.headers['Location']
                    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL -> –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
                    if not next_url.startswith('http'):
                        next_url = urljoin(current_url, next_url)
                    current_url = next_url
                else:
                    break

            except Exception as e:
                chain.append({'url': current_url, 'error': str(e), 'step': i + 1})
                break

        return chain

    def check_external_link(self, url, link_text, source_file):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–Ω–µ—à–Ω—é—é HTTP/HTTPS —Å—Å—ã–ª–∫—É"""
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
        url_hash = self.get_link_hash(url)
        if url in self.external_cache:
            cached = self.external_cache[url].copy()
            cached['cached'] = True
            return cached

        result = {
            'url': url,
            'type': 'external',
            'link_text': link_text,
            'source': str(source_file.relative_to(self.root_dir)),
            'status': 'unknown',
            'health_score': 0.0,
            'issues': [],
            'timestamp': datetime.now().isoformat()
        }

        if not REQUESTS_AVAILABLE:
            result['issues'].append("requests library not available")
            return result

        start_time = time.time()

        try:
            # –°–Ω–∞—á–∞–ª–∞ HEAD –∑–∞–ø—Ä–æ—Å (–±—ã—Å—Ç—Ä–µ–µ)
            response = self.session.head(url, timeout=10, allow_redirects=True)

            # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∞–π—Ç—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç HEAD - –ø—Ä–æ–±—É–µ–º GET
            if response.status_code == 405:
                response = self.session.get(url, timeout=10, stream=True)
                response.close()  # –ù–µ —Å–∫–∞—á–∏–≤–∞–µ–º —Ç–µ–ª–æ

            elapsed_ms = (time.time() - start_time) * 1000

            # –°—Ç–∞—Ç—É—Å –∫–æ–¥
            status_code = response.status_code
            result['status_code'] = status_code
            result['response_time_ms'] = round(elapsed_ms, 2)

            # Health score –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç—É—Å–∞
            if status_code == 200:
                result['status'] = 'ok'
                result['health_score'] = 100.0
            elif status_code in [301, 302, 303, 307, 308]:
                result['status'] = 'redirect'
                result['health_score'] = 80.0
                result['issues'].append(f"Redirect: {status_code}")
                result['final_url'] = response.url
            elif status_code == 404:
                result['status'] = 'not_found'
                result['health_score'] = 0.0
                result['issues'].append("404 Not Found")
                self.broken_links.append(result)
            elif status_code == 403:
                result['status'] = 'forbidden'
                result['health_score'] = 50.0
                result['issues'].append("403 Forbidden")
            elif status_code >= 500:
                result['status'] = 'server_error'
                result['health_score'] = 20.0
                result['issues'].append(f"Server error: {status_code}")
            else:
                result['status'] = 'other'
                result['health_score'] = 60.0

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
            if len(response.history) > 0:
                result['redirect_count'] = len(response.history)
                if len(response.history) > 3:
                    result['issues'].append(f"Too many redirects: {len(response.history)}")
                    result['health_score'] -= 10

            # SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –¥–ª—è HTTPS
            if url.startswith('https://'):
                parsed = urlparse(url)
                ssl_info = self.check_ssl_certificate(parsed.hostname)
                result['ssl'] = ssl_info

                if ssl_info.get('valid') is False:
                    result['issues'].append(f"SSL error: {ssl_info.get('error')}")
                    result['health_score'] -= 20
                elif ssl_info.get('days_remaining') is not None:
                    days = ssl_info['days_remaining']
                    if days < 30:
                        result['issues'].append(f"SSL expires soon: {days} days")
                        result['health_score'] -= 10

            # Performance warning
            if elapsed_ms > 5000:
                result['issues'].append(f"Slow response: {elapsed_ms:.0f}ms")
                result['health_score'] -= 5

            # –ó–∞–ø–æ–º–Ω–∏—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏–∏
            if url_hash not in self.history:
                self.history[url_hash] = {'url': url, 'checks': []}

            self.history[url_hash]['checks'].append({
                'timestamp': result['timestamp'],
                'status_code': status_code,
                'response_time_ms': elapsed_ms,
                'health_score': result['health_score']
            })

            # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
            self.history[url_hash]['checks'] = self.history[url_hash]['checks'][-10:]

        except requests.exceptions.Timeout:
            result['status'] = 'timeout'
            result['health_score'] = 10.0
            result['issues'].append("Request timeout (>10s)")
            self.broken_links.append(result)
        except requests.exceptions.SSLError as e:
            result['status'] = 'ssl_error'
            result['health_score'] = 0.0
            result['issues'].append(f"SSL error: {str(e)[:100]}")
            self.broken_links.append(result)
        except requests.exceptions.ConnectionError as e:
            result['status'] = 'connection_error'
            result['health_score'] = 0.0
            result['issues'].append(f"Connection error: {str(e)[:100]}")
            self.broken_links.append(result)
        except Exception as e:
            result['status'] = 'error'
            result['health_score'] = 0.0
            result['issues'].append(f"Error: {str(e)[:100]}")

        # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.external_cache[url] = result

        return result

    def check_file(self, file_path, check_external=True):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª–µ"""
        content = self.extract_content(file_path)
        if not content:
            return

        # –ù–∞–π—Ç–∏ –≤—Å–µ markdown —Å—Å—ã–ª–∫–∏ [text](url)
        links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)

        for text, link in links:
            # –í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            if link.startswith('http://') or link.startswith('https://'):
                if check_external:
                    result = self.check_external_link(link, text, file_path)
                    self.links.append(result)
            # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            else:
                result = self.check_internal_link(file_path, link, text)
                self.links.append(result)

    def check_all(self, check_external=True):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã"""
        print("üîó Link Health Monitor - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫...\n")

        md_files = list(self.knowledge_dir.rglob("*.md"))
        print(f"   –§–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(md_files)}")

        for i, md_file in enumerate(md_files, 1):
            if md_file.name == "INDEX.md":
                continue

            if i % 10 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(md_files)}")

            self.check_file(md_file, check_external=check_external)

        print(f"\n   –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {len(self.links)}")
        print(f"   –ë–∏—Ç—ã—Ö/–ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö: {len(self.broken_links)}\n")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é
        if check_external:
            self.save_history()

    def calculate_link_freshness(self, url_hash):
        """–°–≤–µ–∂–µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ (–∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –ø—Ä–æ–≤–µ—Ä—è–ª–∞—Å—å)"""
        if url_hash not in self.history:
            return None

        checks = self.history[url_hash].get('checks', [])
        if not checks:
            return None

        last_check = datetime.fromisoformat(checks[-1]['timestamp'])
        age_hours = (datetime.now() - last_check).total_seconds() / 3600

        return {
            'last_check': last_check.isoformat(),
            'age_hours': round(age_hours, 2),
            'freshness': 'fresh' if age_hours < 24 else 'stale' if age_hours < 168 else 'old'
        }

    def suggest_replacement(self, broken_link):
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∑–∞–º–µ–Ω—É –¥–ª—è –±–∏—Ç–æ–π —Å—Å—ã–ª–∫–∏"""
        suggestions = []

        # –î–ª—è –±–∏—Ç—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ - –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Ñ–∞–π–ª—ã
        if broken_link['type'] == 'internal' and broken_link['status'] == 'broken_file':
            target = broken_link.get('target', '')
            # –ù–∞–π—Ç–∏ —Ñ–∞–π–ª—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∏–º–µ–Ω–∞–º–∏
            target_name = Path(target).stem.lower()

            for md_file in self.knowledge_dir.rglob("*.md"):
                file_name = md_file.stem.lower()
                # –ü—Ä–æ—Å—Ç–∞—è Levenshtein distance
                if self.levenshtein_distance(target_name, file_name) <= 3:
                    suggestions.append(str(md_file.relative_to(self.root_dir)))

        # –î–ª—è –±–∏—Ç—ã—Ö —è–∫–æ—Ä–µ–π - –ø–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–∫–æ—Ä—è
        elif broken_link['status'] == 'broken_anchor':
            available = broken_link.get('available_anchors', [])
            if available:
                suggestions.extend([f"#{anchor}" for anchor in available])

        return suggestions[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

    def levenshtein_distance(self, s1, s2):
        """–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def generate_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Å—ã–ª–∫–∞–º"""
        stats = {
            'total_links': len(self.links),
            'internal_links': sum(1 for l in self.links if l['type'] == 'internal'),
            'external_links': sum(1 for l in self.links if l['type'] == 'external'),
            'broken_links': len(self.broken_links),
            'health_distribution': defaultdict(int),
            'status_distribution': defaultdict(int),
            'avg_response_time': 0.0,
        }

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ health score
        for link in self.links:
            score = link['health_score']
            if score >= 90:
                stats['health_distribution']['excellent'] += 1
            elif score >= 70:
                stats['health_distribution']['good'] += 1
            elif score >= 50:
                stats['health_distribution']['fair'] += 1
            else:
                stats['health_distribution']['poor'] += 1

            stats['status_distribution'][link['status']] += 1

        # –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        response_times = [l['response_time_ms'] for l in self.links if 'response_time_ms' in l]
        if response_times:
            stats['avg_response_time'] = round(sum(response_times) / len(response_times), 2)

        return stats

    def generate_report(self, format='markdown'):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        stats = self.generate_statistics()

        if format == 'json':
            return self.generate_json_report(stats)
        elif format == 'html':
            return self.generate_html_report(stats)
        else:
            return self.generate_markdown_report(stats)

    def generate_markdown_report(self, stats):
        """Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîó Link Health Report\n\n")
        lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("## üìä Summary Statistics\n\n")
        lines.append(f"- **Total links**: {stats['total_links']}\n")
        lines.append(f"- **Internal links**: {stats['internal_links']}\n")
        lines.append(f"- **External links**: {stats['external_links']}\n")
        lines.append(f"- **Broken/Issues**: {stats['broken_links']}\n")
        if stats['avg_response_time'] > 0:
            lines.append(f"- **Avg response time**: {stats['avg_response_time']:.0f}ms\n")
        lines.append("\n")

        # Health distribution
        lines.append("### Health Distribution\n\n")
        lines.append(f"- üü¢ **Excellent (90-100)**: {stats['health_distribution']['excellent']}\n")
        lines.append(f"- üü° **Good (70-89)**: {stats['health_distribution']['good']}\n")
        lines.append(f"- üü† **Fair (50-69)**: {stats['health_distribution']['fair']}\n")
        lines.append(f"- üî¥ **Poor (0-49)**: {stats['health_distribution']['poor']}\n\n")

        # Status distribution
        lines.append("### Status Distribution\n\n")
        for status, count in sorted(stats['status_distribution'].items(), key=lambda x: -x[1]):
            lines.append(f"- **{status}**: {count}\n")
        lines.append("\n")

        # –ë–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏
        if self.broken_links:
            lines.append(f"## ‚ùå Broken/Problematic Links ({len(self.broken_links)})\n\n")

            for i, link in enumerate(self.broken_links[:50], 1):  # –ü–µ—Ä–≤—ã–µ 50
                lines.append(f"### {i}. {link['status'].upper()}\n\n")
                lines.append(f"- **URL**: `{link['url']}`\n")
                lines.append(f"- **Source**: {link['source']}\n")
                lines.append(f"- **Health score**: {link['health_score']:.1f}/100\n")

                if link.get('issues'):
                    lines.append(f"- **Issues**: {', '.join(link['issues'])}\n")

                if link.get('status_code'):
                    lines.append(f"- **Status code**: {link['status_code']}\n")

                if link.get('response_time_ms'):
                    lines.append(f"- **Response time**: {link['response_time_ms']:.0f}ms\n")

                # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–º–µ–Ω—ã
                suggestions = self.suggest_replacement(link)
                if suggestions:
                    lines.append(f"- **Suggestions**: {', '.join(f'`{s}`' for s in suggestions)}\n")

                lines.append("\n")
        else:
            lines.append("## ‚úÖ All Links Healthy!\n\n")

        output_file = self.root_dir / "LINK_HEALTH_REPORT.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown report: {output_file}")
        return output_file

    def generate_json_report(self, stats):
        """JSON –æ—Ç—á—ë—Ç"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'statistics': stats,
            'all_links': self.links,
            'broken_links': self.broken_links
        }

        output_file = self.root_dir / "link_health.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ JSON report: {output_file}")
        return output_file


def main():
    parser = argparse.ArgumentParser(description='Advanced Link Health Monitor')
    parser.add_argument('--no-external', action='store_true',
                       help='Skip external link checking (only internal)')
    parser.add_argument('--format', choices=['markdown', 'json'], default='markdown',
                       help='Report format (default: markdown)')
    parser.add_argument('--cache', default='.link_health_cache.json',
                       help='Cache file for historical data')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    monitor = LinkHealthMonitor(root_dir, cache_file=args.cache)
    monitor.check_all(check_external=not args.no_external)
    monitor.generate_report(format=args.format)


if __name__ == "__main__":
    main()
