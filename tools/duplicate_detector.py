#!/usr/bin/env python3
"""
Duplicate Detector - –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è –∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Google duplicate content detection, Copyscape
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import hashlib
import json
import argparse
import math
from typing import Dict, List, Tuple, Set


class DuplicateDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""

    def __init__(self, root_dir=".", similarity_threshold=0.8):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.similarity_threshold = similarity_threshold

        # –î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–µ–π
        self.articles = {}

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.duplicates = {
            'exact': [],
            'near_duplicate': [],
            'similar_titles': []
        }

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def normalize_text(self, text):
        """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        # –£–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        # –£–¥–∞–ª–∏—Ç—å markdown —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
        text = re.sub(r'[#*`\[\]()]', '', text)
        # Lowercase
        text = text.lower().strip()
        return text

    def calculate_hash(self, text):
        """–í—ã—á–∏—Å–ª–∏—Ç—å MD5 —Ö–µ—à —Ç–µ–∫—Å—Ç–∞"""
        normalized = self.normalize_text(text)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()

    def calculate_similarity(self, text1, text2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ (Jaccard similarity)"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words1 = set(re.findall(r'\b\w+\b', text1.lower()))
        words2 = set(re.findall(r'\b\w+\b', text2.lower()))

        if not words1 or not words2:
            return 0.0

        # Jaccard similarity
        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def levenshtein_distance(self, s1, s2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)

        for i, c1 in enumerate(s1):
            current_row = [i + 1]

            for j, c2 in enumerate(s2):
                # –°—Ç–æ–∏–º–æ—Å—Ç—å –≤—Å—Ç–∞–≤–∫–∏, —É–¥–∞–ª–µ–Ω–∏—è –∏–ª–∏ –∑–∞–º–µ–Ω—ã
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)

                current_row.append(min(insertions, deletions, substitutions))

            previous_row = current_row

        return previous_row[-1]

    def title_similarity(self, title1, title2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        t1 = self.normalize_text(title1)
        t2 = self.normalize_text(title2)

        if t1 == t2:
            return 1.0

        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
        max_len = max(len(t1), len(t2))
        if max_len == 0:
            return 0.0

        distance = self.levenshtein_distance(t1, t2)
        similarity = 1.0 - (distance / max_len)

        return similarity

    def collect_articles(self):
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üìö –°–±–æ—Ä —Å—Ç–∞—Ç–µ–π...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            self.articles[article_path] = {
                'title': title,
                'content': content,
                'hash': self.calculate_hash(content),
                'word_count': len(re.findall(r'\b\w+\b', content))
            }

        print(f"   –°—Ç–∞—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ: {len(self.articles)}\n")

    def find_exact_duplicates(self):
        """–ù–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ —Ö–µ—à—É)"""
        print("üîç –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ö–µ—à—É
        by_hash = defaultdict(list)

        for article_path, data in self.articles.items():
            by_hash[data['hash']].append(article_path)

        # –ù–∞–π—Ç–∏ –≥—Ä—É–ø–ø—ã —Å –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å—ë–π
        for hash_value, articles in by_hash.items():
            if len(articles) > 1:
                self.duplicates['exact'].append({
                    'type': 'exact',
                    'articles': [
                        {
                            'path': article,
                            'title': self.articles[article]['title']
                        }
                        for article in articles
                    ],
                    'similarity': 1.0
                })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(self.duplicates['exact'])}\n")

    def find_near_duplicates(self):
        """–ù–∞–π—Ç–∏ –ø–æ—á—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã (–≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)"""
        print("üîé –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...\n")

        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self.calculate_similarity(data1['content'], data2['content'])

                if similarity >= self.similarity_threshold:
                    self.duplicates['near_duplicate'].append({
                        'type': 'near_duplicate',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π: {len(self.duplicates['near_duplicate'])}\n")

    def find_similar_titles(self):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        print("üìù –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤...\n")

        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                similarity = self.title_similarity(data1['title'], data2['title'])

                if similarity >= 0.7:  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    self.duplicates['similar_titles'].append({
                        'type': 'similar_title',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏: {len(self.duplicates['similar_titles'])}\n")

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîç –û—Ç—á—ë—Ç: –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n\n")
        lines.append("> –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ—Å—è –∏ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_issues = (
            len(self.duplicates['exact']) +
            len(self.duplicates['near_duplicate']) +
            len(self.duplicates['similar_titles'])
        )

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ**: {len(self.articles)}\n")
        lines.append(f"- **–¢–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤**: {len(self.duplicates['exact'])}\n")
        lines.append(f"- **–ü–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π**: {len(self.duplicates['near_duplicate'])}\n")
        lines.append(f"- **–ü–æ—Ö–æ–∂–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {len(self.duplicates['similar_titles'])}\n")
        lines.append(f"- **–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º**: {total_issues}\n\n")

        if total_issues == 0:
            lines.append("‚úÖ **–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!**\n\n")
        else:
            lines.append("‚ö†Ô∏è  **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã**\n\n")

        # –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if self.duplicates['exact']:
            lines.append("## ‚õî –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã\n\n")
            lines.append("> –ò–¥–µ–Ω—Ç–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (100% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)\n\n")

            for i, dup in enumerate(self.duplicates['exact'], 1):
                lines.append(f"### –ì—Ä—É–ø–ø–∞ {i}\n\n")

                for article in dup['articles']:
                    lines.append(f"- **{article['title']}**\n")
                    lines.append(f"  - `{article['path']}`\n")

                lines.append("\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é\n\n")

        # –ü–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏
        if self.duplicates['near_duplicate']:
            lines.append("## ‚ö†Ô∏è  –ü–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç\n\n")
            lines.append(f"> –°—Ö–æ–¥—Å—Ç–≤–æ >= {self.similarity_threshold * 100:.0f}%\n\n")

            for i, dup in enumerate(self.duplicates['near_duplicate'], 1):
                lines.append(f"### –ü–∞—Ä–∞ {i} ‚Äî –°—Ö–æ–¥—Å—Ç–≤–æ: {dup['similarity'] * 100:.1f}%\n\n")

                for article in dup['articles']:
                    lines.append(f"- **{article['title']}**\n")
                    lines.append(f"  - [{article['path']}]({article['path']})\n")

                lines.append("\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–ª–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å\n\n")

        # –ü–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if self.duplicates['similar_titles']:
            lines.append("## üìù –ü–æ—Ö–æ–∂–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏\n\n")
            lines.append("> –ú–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Ç–∞–Ω–∏—Ü–µ–π –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª–µ–π\n\n")

            for i, dup in enumerate(self.duplicates['similar_titles'], 1):
                lines.append(f"### –ü–∞—Ä–∞ {i} ‚Äî –°—Ö–æ–¥—Å—Ç–≤–æ: {dup['similarity'] * 100:.1f}%\n\n")

                for article in dup['articles']:
                    lines.append(f"- **{article['title']}**\n")
                    lines.append(f"  - [{article['path']}]({article['path']})\n")

                lines.append("\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –°–¥–µ–ª–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±–æ–ª–µ–µ —Ä–∞–∑–ª–∏—á–∏–º—ã–º–∏\n\n")

        output_file = self.root_dir / "DUPLICATES_REPORT.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON"""
        output_file = self.root_dir / "duplicates.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.duplicates, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")

    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å cosine similarity —Å TF-IDF

        cos(Œ∏) = (A¬∑B) / (||A|| √ó ||B||)
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words1 = re.findall(r'\b\w+\b', text1.lower())
        words2 = re.findall(r'\b\w+\b', text2.lower())

        # TF (Term Frequency)
        tf1 = Counter(words1)
        tf2 = Counter(words2)

        # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        all_words = set(tf1.keys()) | set(tf2.keys())

        # –í–µ–∫—Ç–æ—Ä—ã
        vec1 = [tf1.get(word, 0) for word in all_words]
        vec2 = [tf2.get(word, 0) for word in all_words]

        # Dot product
        dot_product = sum(a * b for a, b in zip(vec1, vec2))

        # Magnitudes
        mag1 = math.sqrt(sum(a * a for a in vec1))
        mag2 = math.sqrt(sum(b * b for b in vec2))

        if mag1 == 0 or mag2 == 0:
            return 0.0

        return dot_product / (mag1 * mag2)

    def get_shingles(self, text: str, k: int = 3) -> Set[str]:
        """
        –°–æ–∑–¥–∞—Ç—å k-shingles (n-grams) –¥–ª—è —Ç–µ–∫—Å—Ç–∞

        Example: "hello world" with k=3 ‚Üí {"hel", "ell", "llo", ...}
        """
        normalized = self.normalize_text(text)
        shingles = set()

        for i in range(len(normalized) - k + 1):
            shingle = normalized[i:i + k]
            shingles.add(shingle)

        return shingles

    def shingle_similarity(self, text1: str, text2: str, k: int = 3) -> float:
        """Jaccard similarity –Ω–∞ –æ—Å–Ω–æ–≤–µ shingles"""
        shingles1 = self.get_shingles(text1, k)
        shingles2 = self.get_shingles(text2, k)

        if not shingles1 or not shingles2:
            return 0.0

        intersection = len(shingles1 & shingles2)
        union = len(shingles1 | shingles2)

        return intersection / union if union > 0 else 0.0


class AdvancedDuplicateDetector(DuplicateDetector):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏"""

    def find_duplicates_by_cosine(self, threshold: float = 0.8) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è cosine similarity"""
        print(f"üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (cosine similarity >= {threshold})...\n")

        duplicates = []
        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                similarity = self.calculate_cosine_similarity(data1['content'], data2['content'])

                if similarity >= threshold:
                    duplicates.append({
                        'type': 'cosine_duplicate',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity,
                        'method': 'cosine'
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä (cosine): {len(duplicates)}\n")
        return duplicates

    def find_duplicates_by_shingles(self, threshold: float = 0.7, k: int = 3) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è shingle similarity"""
        print(f"üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (shingles k={k}, threshold={threshold})...\n")

        duplicates = []
        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                similarity = self.shingle_similarity(data1['content'], data2['content'], k)

                if similarity >= threshold:
                    duplicates.append({
                        'type': 'shingle_duplicate',
                        'articles': [
                            {'path': path1, 'title': data1['title']},
                            {'path': path2, 'title': data2['title']}
                        ],
                        'similarity': similarity,
                        'method': f'shingles-{k}'
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä (shingles): {len(duplicates)}\n")
        return duplicates

    def analyze_similarity_distribution(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏"""
        print("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞...\n")

        similarities = []
        articles_list = list(self.articles.items())

        for i, (path1, data1) in enumerate(articles_list):
            for path2, data2 in articles_list[i + 1:]:
                jaccard = self.calculate_similarity(data1['content'], data2['content'])
                cosine = self.calculate_cosine_similarity(data1['content'], data2['content'])

                similarities.append({
                    'pair': (data1['title'], data2['title']),
                    'jaccard': jaccard,
                    'cosine': cosine
                })

        if not similarities:
            return {}

        # Statistics
        jaccard_scores = [s['jaccard'] for s in similarities]
        cosine_scores = [s['cosine'] for s in similarities]

        stats = {
            'total_pairs': len(similarities),
            'jaccard': {
                'mean': sum(jaccard_scores) / len(jaccard_scores),
                'max': max(jaccard_scores),
                'min': min(jaccard_scores)
            },
            'cosine': {
                'mean': sum(cosine_scores) / len(cosine_scores),
                'max': max(cosine_scores),
                'min': min(cosine_scores)
            },
            'top_similar': sorted(similarities, key=lambda x: x['cosine'], reverse=True)[:5]
        }

        print(f"   –í—Å–µ–≥–æ –ø–∞—Ä –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {stats['total_pairs']}")
        print(f"   Jaccard: mean={stats['jaccard']['mean']:.3f}, max={stats['jaccard']['max']:.3f}")
        print(f"   Cosine: mean={stats['cosine']['mean']:.3f}, max={stats['cosine']['max']:.3f}\n")

        return stats


def main():
    parser = argparse.ArgumentParser(
        description='üîç Duplicate Detector - –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
  %(prog)s --advanced             # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ (cosine, shingles)
  %(prog)s --analyze              # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
  %(prog)s --threshold 0.9        # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
  %(prog)s --method cosine        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ cosine similarity
        """
    )

    parser.add_argument('-t', '--threshold', type=float, default=0.8,
                       help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (0.0-1.0, default: 0.8)')
    parser.add_argument('--advanced', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã (cosine, shingles)')
    parser.add_argument('--analyze', action='store_true',
                       help='–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞')
    parser.add_argument('--method', type=str, choices=['jaccard', 'cosine', 'shingles', 'all'],
                       default='all', help='–ú–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    if args.advanced or args.method in ['cosine', 'shingles', 'all']:
        detector = AdvancedDuplicateDetector(root_dir, similarity_threshold=args.threshold)
    else:
        detector = DuplicateDetector(root_dir, similarity_threshold=args.threshold)

    detector.collect_articles()

    # Standard methods
    if args.method in ['jaccard', 'all']:
        detector.find_exact_duplicates()
        detector.find_near_duplicates()
        detector.find_similar_titles()

    # Advanced methods
    if args.advanced or args.method in ['cosine', 'shingles', 'all']:
        if isinstance(detector, AdvancedDuplicateDetector):
            if args.method in ['cosine', 'all']:
                detector.find_duplicates_by_cosine(args.threshold)
            if args.method in ['shingles', 'all']:
                detector.find_duplicates_by_shingles(threshold=0.7, k=3)

    # Analyze
    if args.analyze and isinstance(detector, AdvancedDuplicateDetector):
        stats = detector.analyze_similarity_distribution()

        print("üìä –¢–æ–ø-5 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä:\n")
        for i, sim in enumerate(stats.get('top_similar', []), 1):
            print(f"   {i}. {sim['pair'][0]} ‚Üî {sim['pair'][1]}")
            print(f"      Jaccard: {sim['jaccard']:.3f}, Cosine: {sim['cosine']:.3f}\n")

    # Generate reports
    detector.generate_report()
    detector.save_json()


if __name__ == "__main__":
    main()
