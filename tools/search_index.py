#!/usr/bin/env python3
"""
Search Index - –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Elasticsearch, Lucene
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict
import json
import math


class SearchIndexBuilder:
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        self.index = defaultdict(lambda: defaultdict(int))

        # –î–æ–∫—É–º–µ–Ω—Ç—ã
        self.documents = {}

        # IDF (inverse document frequency)
        self.idf = {}

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def tokenize(self, text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–∏—Ç—å markdown
        text = re.sub(r'[#*`\[\]()]', ' ', text)

        # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–π + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        words = re.findall(r'\b[–∞-—è—ëa-z]{2,}\b', text.lower())

        return words

    def build_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å"""
        print("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem
            tags = frontmatter.get('tags', []) if frontmatter else []

            # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–º–µ–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å
            full_text = f"{title} {title} {title} " + content
            full_text += " " + " ".join(tags)

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            words = self.tokenize(full_text)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
            self.documents[article_path] = {
                'title': title,
                'tags': tags,
                'word_count': len(words)
            }

            # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            word_freq = defaultdict(int)
            for word in words:
                word_freq[word] += 1

            for word, freq in word_freq.items():
                self.index[word][article_path] = freq

        # –í—ã—á–∏—Å–ª–∏—Ç—å IDF
        total_docs = len(self.documents)

        for word, docs in self.index.items():
            self.idf[word] = math.log(total_docs / len(docs))

        print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(self.documents)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.index)}\n")

    def search(self, query, limit=10):
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query_words = self.tokenize(query)

        if not query_words:
            return []

        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        scores = defaultdict(float)

        for word in query_words:
            if word not in self.index:
                continue

            idf = self.idf[word]

            for doc_path, tf in self.index[word].items():
                # TF-IDF
                tf_normalized = tf / self.documents[doc_path]['word_count']
                scores[doc_path] += tf_normalized * idf

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results = [
            {
                'path': doc_path,
                'title': self.documents[doc_path]['title'],
                'score': score
            }
            for doc_path, score in scores.items()
        ]

        results.sort(key=lambda x: -x['score'])

        return results[:limit]

    def save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å –≤ JSON"""
        data = {
            'documents': self.documents,
            'index': {
                word: dict(docs)
                for word, docs in self.index.items()
            },
            'idf': self.idf
        }

        output_file = self.root_dir / "search_index.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Search Index')
    parser.add_argument('-q', '--query', help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å')
    parser.add_argument('-n', '--limit', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = SearchIndexBuilder(root_dir)
    builder.build_index()
    builder.save_index()

    if args.query:
        print(f"\nüîç –ü–æ–∏—Å–∫: '{args.query}'\n")
        results = builder.search(args.query, limit=args.limit)

        if results:
            for i, result in enumerate(results, 1):
                print(f"{i}. {result['title']} (score: {result['score']:.2f})")
                print(f"   {result['path']}\n")
        else:
            print("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")


if __name__ == "__main__":
    main()
