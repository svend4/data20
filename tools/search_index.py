#!/usr/bin/env python3
"""
Advanced Search Index - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞
–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å BM25, fuzzy search, boolean queries

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- BM25 scoring (–ª—É—á—à–µ —á–µ–º TF-IDF –¥–ª—è –ø–æ–∏—Å–∫–∞)
- Phrase search ("exact phrase" –≤ –∫–∞–≤—ã—á–∫–∞—Ö)
- Proximity search (—Å–ª–æ–≤–∞ —Ä—è–¥–æ–º –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º)
- Field boosting (title x3, headers x2, body x1)
- Fuzzy search (typo tolerance, Levenshtein distance)
- Boolean queries (AND, OR, NOT)
- Search suggestions ("did you mean?")
- Search analytics (–ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, no-result queries)
- Incremental indexing
- Multiple export formats

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Elasticsearch, Apache Lucene, Solr
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import json
import math
import argparse
from datetime import datetime


class AdvancedSearchIndex:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å BM25"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: term -> {doc_id: {tf, positions}}
        self.index = defaultdict(lambda: defaultdict(lambda: {'tf': 0, 'positions': []}))

        # –î–æ–∫—É–º–µ–Ω—Ç—ã: doc_id -> metadata
        self.documents = {}

        # Field-specific indexes (–¥–ª—è field boosting)
        self.title_index = defaultdict(lambda: defaultdict(int))
        self.header_index = defaultdict(lambda: defaultdict(int))

        # IDF
        self.idf = {}

        # BM25 parameters
        self.k1 = 1.5  # term frequency saturation
        self.b = 0.75  # length normalization

        # Avg document length
        self.avg_doc_length = 0

        # Search analytics
        self.search_history = []
        self.no_result_queries = []

        # Stop words (frequent words to ignore)
        self.stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–∫', '–ø–æ', '–¥–ª—è', '–∏–∑', '—á—Ç–æ', '—ç—Ç–æ',
            '–∫–∞–∫', '–∏–ª–∏', '–Ω–æ', '–∞', '–æ', '–æ—Ç', '–¥–æ', '–∑–∞', '–ø—Ä–∏', '–Ω–µ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
            'for', 'of', 'as', 'by', 'with', 'from', 'is', 'are', 'was', 'were',
        }

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def tokenize(self, text, remove_stop_words=True):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º stop words"""
        # –£–¥–∞–ª–∏—Ç—å markdown
        text = re.sub(r'[#*`\[\]()]', ' ', text)

        # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–π + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        words = re.findall(r'\b[–∞-—è—ëa-z]{2,}\b', text.lower())

        if remove_stop_words:
            words = [w for w in words if w not in self.stop_words]

        return words

    def extract_headers(self, content):
        """–ò–∑–≤–ª–µ—á—å –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ markdown"""
        headers = re.findall(r'^#{1,6}\s+(.+)$', content, re.MULTILINE)
        return ' '.join(headers)

    def levenshtein_distance(self, s1, s2):
        """–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –¥–ª—è fuzzy search"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def build_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–ª–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        print("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...\n")

        total_words = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            doc_id = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem
            tags = frontmatter.get('tags', []) if frontmatter else []

            # –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–∫–∏
            headers = self.extract_headers(content)

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø–æ–ª—è–º (–¥–ª—è field boosting)
            title_words = self.tokenize(title)
            header_words = self.tokenize(headers)
            body_words = self.tokenize(content)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ field-specific indexes
            for word in title_words:
                self.title_index[word][doc_id] += 1

            for word in header_words:
                self.header_index[word][doc_id] += 1

            # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ —Å–ª–æ–≤–∞ —Å –ø–æ–∑–∏—Ü–∏—è–º–∏
            all_words = title_words + header_words + body_words

            # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ (–¥–ª—è proximity search)
            for position, word in enumerate(all_words):
                self.index[word][doc_id]['tf'] += 1
                self.index[word][doc_id]['positions'].append(position)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            word_count = len(all_words)
            total_words += word_count

            self.documents[doc_id] = {
                'title': title,
                'tags': tags,
                'word_count': word_count,
                'title_words': len(title_words),
                'header_words': len(header_words),
            }

        # –í—ã—á–∏—Å–ª–∏—Ç—å avg document length
        if self.documents:
            self.avg_doc_length = total_words / len(self.documents)

        # –í—ã—á–∏—Å–ª–∏—Ç—å IDF
        total_docs = len(self.documents)
        for word, docs in self.index.items():
            df = len(docs)
            self.idf[word] = math.log((total_docs - df + 0.5) / (df + 0.5) + 1)

        print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.index)}")
        print(f"   Avg doc length: {self.avg_doc_length:.1f}\n")

    def calculate_bm25_score(self, query_words, doc_id):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å BM25 score
        BM25 = sum(IDF(qi) * (tf(qi, D) * (k1 + 1)) / (tf(qi, D) + k1 * (1 - b + b * |D| / avgdl)))
        """
        score = 0.0
        doc_length = self.documents[doc_id]['word_count']

        for word in query_words:
            if word not in self.index or doc_id not in self.index[word]:
                continue

            tf = self.index[word][doc_id]['tf']
            idf = self.idf.get(word, 0)

            # BM25 formula
            numerator = tf * (self.k1 + 1)
            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))

            score += idf * (numerator / denominator)

        return score

    def apply_field_boosting(self, query_words, doc_id, base_score):
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å field boosting
        title: x3, headers: x2, body: x1
        """
        boost = 0.0

        for word in query_words:
            # Title boost (x3)
            if word in self.title_index and doc_id in self.title_index[word]:
                boost += self.title_index[word][doc_id] * 3.0

            # Header boost (x2)
            if word in self.header_index and doc_id in self.header_index[word]:
                boost += self.header_index[word][doc_id] * 2.0

        return base_score + boost * 0.1

    def phrase_search(self, phrase):
        """
        –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–π —Ñ—Ä–∞–∑—ã
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ —Å–ª–æ–≤–∞ –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥
        """
        words = self.tokenize(phrase, remove_stop_words=False)
        if not words:
            return []

        # –ù–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –í–°–ï —Å–ª–æ–≤–∞
        candidate_docs = set(self.index[words[0]].keys())
        for word in words[1:]:
            if word in self.index:
                candidate_docs &= set(self.index[word].keys())
            else:
                return []

        results = []

        for doc_id in candidate_docs:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Å–ª–æ–≤–∞ –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥
            positions = [self.index[word][doc_id]['positions'] for word in words]

            # –ù–∞–π—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            phrase_found = False
            for start_pos in positions[0]:
                match = True
                for i, word_positions in enumerate(positions[1:], 1):
                    if start_pos + i not in word_positions:
                        match = False
                        break
                if match:
                    phrase_found = True
                    break

            if phrase_found:
                results.append({
                    'path': doc_id,
                    'title': self.documents[doc_id]['title'],
                    'score': 100.0,  # Exact match
                })

        return results

    def proximity_search(self, word1, word2, max_distance=5):
        """
        Proximity search: –Ω–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≥–¥–µ word1 –∏ word2 –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É
        """
        if word1 not in self.index or word2 not in self.index:
            return []

        results = []
        candidate_docs = set(self.index[word1].keys()) & set(self.index[word2].keys())

        for doc_id in candidate_docs:
            positions1 = self.index[word1][doc_id]['positions']
            positions2 = self.index[word2][doc_id]['positions']

            # –ù–∞–π—Ç–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            min_dist = float('inf')
            for p1 in positions1:
                for p2 in positions2:
                    dist = abs(p1 - p2)
                    if dist < min_dist:
                        min_dist = dist

            if min_dist <= max_distance:
                # Score –Ω–∞ –æ—Å–Ω–æ–≤–µ proximity (–±–ª–∏–∂–µ = –ª—É—á—à–µ)
                proximity_score = 100 / min_dist if min_dist > 0 else 100
                results.append({
                    'path': doc_id,
                    'title': self.documents[doc_id]['title'],
                    'score': proximity_score,
                    'distance': min_dist,
                })

        results.sort(key=lambda x: -x['score'])
        return results

    def fuzzy_search(self, query_word, max_distance=2):
        """
        Fuzzy search —Å Levenshtein distance
        –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ (–¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫)
        """
        similar_words = []

        for indexed_word in self.index.keys():
            distance = self.levenshtein_distance(query_word, indexed_word)
            if distance <= max_distance:
                similar_words.append((indexed_word, distance))

        return sorted(similar_words, key=lambda x: x[1])

    def search_with_boolean(self, query):
        """
        –ü–æ–∏—Å–∫ —Å boolean –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏ (AND, OR, NOT)
        –ü—Ä–∏–º–µ—Ä—ã:
        - "python AND programming"
        - "machine learning OR deep learning"
        - "python NOT java"
        """
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ boolean –∑–∞–ø—Ä–æ—Å–æ–≤
        if ' AND ' in query:
            parts = query.split(' AND ')
            # –í—Å–µ —á–∞—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
            results_sets = [set(r['path'] for r in self.search(part.strip())) for part in parts]
            common_docs = set.intersection(*results_sets) if results_sets else set()
            return [{'path': doc, 'title': self.documents[doc]['title'], 'score': 50.0}
                    for doc in common_docs]

        elif ' OR ' in query:
            parts = query.split(' OR ')
            # –•–æ—Ç—è –±—ã –æ–¥–Ω–∞ —á–∞—Å—Ç—å
            all_results = []
            for part in parts:
                all_results.extend(self.search(part.strip()))
            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            seen = set()
            unique_results = []
            for r in all_results:
                if r['path'] not in seen:
                    seen.add(r['path'])
                    unique_results.append(r)
            return unique_results

        elif ' NOT ' in query:
            parts = query.split(' NOT ')
            if len(parts) == 2:
                include_results = self.search(parts[0].strip())
                exclude_docs = set(r['path'] for r in self.search(parts[1].strip()))
                return [r for r in include_results if r['path'] not in exclude_docs]

        # Fallback to regular search
        return self.search(query)

    def search(self, query, limit=10):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ —Å BM25"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ phrase search
        if query.startswith('"') and query.endswith('"'):
            phrase = query.strip('"')
            return self.phrase_search(phrase)[:limit]

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query_words = self.tokenize(query)

        if not query_words:
            self.no_result_queries.append(query)
            return []

        # –í—ã—á–∏—Å–ª–∏—Ç—å BM25 scores
        scores = {}

        for doc_id in self.documents.keys():
            bm25_score = self.calculate_bm25_score(query_words, doc_id)
            if bm25_score > 0:
                # –ü—Ä–∏–º–µ–Ω–∏—Ç—å field boosting
                final_score = self.apply_field_boosting(query_words, doc_id, bm25_score)
                scores[doc_id] = final_score

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        results = [
            {
                'path': doc_id,
                'title': self.documents[doc_id]['title'],
                'score': score
            }
            for doc_id, score in scores.items()
        ]

        results.sort(key=lambda x: -x['score'])

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ analytics
        self.search_history.append({
            'query': query,
            'results_count': len(results),
            'timestamp': datetime.now().isoformat(),
        })

        if not results:
            self.no_result_queries.append(query)

        return results[:limit]

    def suggest_query(self, query):
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å ("did you mean?")
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç fuzzy search –¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫
        """
        query_words = self.tokenize(query)
        suggestions = []

        for word in query_words:
            if word in self.index:
                continue  # –°–ª–æ–≤–æ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ –Ω—É–∂–Ω–æ

            # Fuzzy search
            similar = self.fuzzy_search(word, max_distance=2)
            if similar:
                # –í–∑—è—Ç—å —Å–∞–º–æ–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ –ø–æ—Ö–æ–∂–µ–µ —Å–ª–æ–≤–æ
                best_match = max(similar, key=lambda x: len(self.index[x[0]]))
                suggestions.append((word, best_match[0]))

        if suggestions:
            corrected = query
            for original, suggested in suggestions:
                corrected = corrected.replace(original, suggested)
            return corrected

        return None

    def get_search_analytics(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞"""
        if not self.search_history:
            return {}

        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        all_queries = [h['query'] for h in self.search_history]
        popular_queries = Counter(all_queries).most_common(10)

        # No-result queries
        no_result_counts = Counter(self.no_result_queries).most_common(10)

        # Avg results per query
        avg_results = sum(h['results_count'] for h in self.search_history) / len(self.search_history)

        return {
            'total_searches': len(self.search_history),
            'popular_queries': popular_queries,
            'no_result_queries': no_result_counts,
            'avg_results_per_query': avg_results,
        }

    def export_to_json(self, output_file='search_index.json'):
        """–ü–æ–ª–Ω—ã–π JSON —ç–∫—Å–ø–æ—Ä—Ç"""
        data = {
            'generated_at': datetime.now().isoformat(),
            'statistics': {
                'total_documents': len(self.documents),
                'unique_words': len(self.index),
                'avg_doc_length': self.avg_doc_length,
            },
            'documents': self.documents,
            'analytics': self.get_search_analytics(),
        }

        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å: {output_path}")

    def generate_analytics_report(self, output_file='SEARCH_ANALYTICS.md'):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ –ø–æ–∏—Å–∫–∞"""
        analytics = self.get_search_analytics()

        lines = []
        lines.append("# üîç Search Analytics\n\n")
        lines.append(f"> –°–æ–∑–¥–∞–Ω {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")

        if analytics:
            lines.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
            lines.append(f"- **–í—Å–µ–≥–æ –ø–æ–∏—Å–∫–æ–≤**: {analytics['total_searches']}\n")
            lines.append(f"- **Avg —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**: {analytics['avg_results_per_query']:.1f}\n\n")

            # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            if analytics['popular_queries']:
                lines.append("## üî• –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã\n\n")
                for query, count in analytics['popular_queries']:
                    lines.append(f"- **{query}** ({count} —Ä–∞–∑)\n")
                lines.append("\n")

            # No-result queries
            if analytics['no_result_queries']:
                lines.append("## ‚ùå –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n\n")
                for query, count in analytics['no_result_queries']:
                    lines.append(f"- **{query}** ({count} —Ä–∞–∑)\n")
                lines.append("\n")

        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Analytics: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description='–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å BM25'
    )
    parser.add_argument(
        '-q', '--query',
        help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ "—Ñ—Ä–∞–∑–∞" –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)'
    )
    parser.add_argument(
        '-n', '--limit',
        type=int,
        default=10,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'
    )
    parser.add_argument(
        '--fuzzy',
        action='store_true',
        help='–í–∫–ª—é—á–∏—Ç—å fuzzy search'
    )
    parser.add_argument(
        '--analytics',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å search analytics'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    index = AdvancedSearchIndex(root_dir)
    index.build_index()
    index.export_to_json()

    if args.query:
        print(f"\nüîç –ü–æ–∏—Å–∫: '{args.query}'\n")

        # Boolean search
        results = index.search_with_boolean(args.query) if ' AND ' in args.query or ' OR ' in args.query or ' NOT ' in args.query else index.search(args.query, limit=args.limit)

        if results:
            for i, result in enumerate(results, 1):
                print(f"{i}. {result['title']} (score: {result['score']:.2f})")
                print(f"   {result['path']}\n")
        else:
            print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

            # Suggest correction
            suggestion = index.suggest_query(args.query)
            if suggestion:
                print(f"\nüí° –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: '{suggestion}'")

    if args.analytics:
        index.generate_analytics_report()

    print()


if __name__ == "__main__":
    main()
