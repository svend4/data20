#!/usr/bin/env python3
"""
Bibliography Generator - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Å–æ–∑–¥–∞—ë—Ç –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Å–ø–∏—Å–∫–∏

–§—É–Ω–∫—Ü–∏–∏:
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
- –ü–∞—Ä—Å–∏–Ω–≥ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª—è—Ö (APA, MLA, Chicago)
- –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–π –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict
from datetime import datetime


class BibliographyGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°–æ–±—Ä–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        self.sources = []

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def extract_urls(self, content):
        """–ò–∑–≤–ª–µ—á—å –≤—Å–µ URL –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        urls = []

        # Markdown —Å—Å—ã–ª–∫–∏: [text](url)
        markdown_links = re.findall(r'\[([^\]]+)\]\((https?://[^\)]+)\)', content)
        for text, url in markdown_links:
            urls.append({'text': text, 'url': url, 'type': 'link'})

        # –ì–æ–ª—ã–µ URL
        bare_urls = re.findall(r'(?<!\()(https?://[^\s\)]+)', content)
        for url in bare_urls:
            if not any(u['url'] == url for u in urls):
                urls.append({'text': '', 'url': url, 'type': 'bare'})

        return urls

    def parse_url_metadata(self, url):
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ URL (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)"""
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if 'github.com' in url:
            source_type = 'GitHub Repository'
        elif 'arxiv.org' in url:
            source_type = 'arXiv Paper'
        elif 'wikipedia.org' in url:
            source_type = 'Wikipedia'
        elif 'stackoverflow.com' in url:
            source_type = 'Stack Overflow'
        elif 'youtube.com' in url or 'youtu.be' in url:
            source_type = 'YouTube Video'
        elif any(ext in url for ext in ['.pdf', '.doc', '.docx']):
            source_type = 'Document'
        else:
            source_type = 'Web Page'

        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω
        domain_match = re.search(r'https?://([^/]+)', url)
        domain = domain_match.group(1) if domain_match else ''

        return {
            'type': source_type,
            'domain': domain
        }

    def collect_sources(self):
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("üìö –°–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not frontmatter:
                continue

            article_file = str(md_file.relative_to(self.root_dir))
            article_title = frontmatter.get('title', md_file.stem)

            # –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            source = frontmatter.get('source')
            if source:
                self.sources.append({
                    'title': article_title,
                    'source': source,
                    'author': frontmatter.get('author', ''),
                    'date': frontmatter.get('date', ''),
                    'url': frontmatter.get('source_url', ''),
                    'article': article_file,
                    'type': 'metadata'
                })

            # URL –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            if content:
                urls = self.extract_urls(content)

                for url_data in urls:
                    url_meta = self.parse_url_metadata(url_data['url'])

                    self.sources.append({
                        'title': url_data['text'] or url_meta['domain'],
                        'url': url_data['url'],
                        'domain': url_meta['domain'],
                        'source_type': url_meta['type'],
                        'article': article_file,
                        'article_title': article_title,
                        'type': 'url'
                    })

        print(f"   –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.sources)}")

    def format_source_apa(self, source):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Å—Ç–∏–ª–µ APA"""
        if source['type'] == 'metadata':
            author = source.get('author', 'Unknown')
            date = source.get('date', 'n.d.')
            if isinstance(date, datetime):
                date = date.year
            elif isinstance(date, str) and '-' in date:
                date = date.split('-')[0]

            title = source['title']

            result = f"{author}. ({date}). *{title}*."

            if source.get('url'):
                result += f" Retrieved from {source['url']}"

            return result

        elif source['type'] == 'url':
            title = source['title'] or source['domain']
            url = source['url']
            source_type = source.get('source_type', 'Web page')

            return f"*{title}*. {source_type}. {url}"

        return str(source)

    def format_source_mla(self, source):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Å—Ç–∏–ª–µ MLA"""
        if source['type'] == 'metadata':
            author = source.get('author', 'Unknown')
            title = source['title']
            date = source.get('date', 'n.d.')

            return f"{author}. \"{title}.\" {date}."

        elif source['type'] == 'url':
            title = source['title'] or source['domain']
            url = source['url']

            return f"\"{title}.\" Web. {url}"

        return str(source)

    def generate_bibliography_by_article(self):
        """–°–æ–∑–¥–∞—Ç—å –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—é —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ —Å—Ç–∞—Ç—å—è–º"""
        lines = []
        lines.append("# üìö –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è –ø–æ —Å—Ç–∞—Ç—å—è–º\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Å—Ç–∞—Ç—å—è–º
        by_article = defaultdict(list)

        for source in self.sources:
            article = source.get('article', 'Unknown')
            by_article[article].append(source)

        # –í—ã–≤–µ—Å—Ç–∏
        for article in sorted(by_article.keys()):
            sources = by_article[article]

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
            article_title = sources[0].get('article_title', article)
            lines.append(f"## [{article_title}]({article})\n\n")

            # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
            for i, source in enumerate(sources, 1):
                formatted = self.format_source_apa(source)
                lines.append(f"{i}. {formatted}\n")

            lines.append("\n")

        output_file = self.root_dir / "BIBLIOGRAPHY_BY_ARTICLE.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è –ø–æ —Å—Ç–∞—Ç—å—è–º: {output_file}")

    def generate_master_bibliography(self):
        """–°–æ–∑–¥–∞—Ç—å –æ–±—â—É—é –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—é –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        lines = []
        lines.append("# üìö –û–±—â–∞—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è\n\n")
        lines.append("> –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π\n\n")

        lines.append(f"**–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤**: {len(self.sources)}\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø—É
        by_type = defaultdict(list)

        for source in self.sources:
            if source['type'] == 'url':
                source_type = source.get('source_type', 'Web Page')
            else:
                source_type = 'Article Source'

            by_type[source_type].append(source)

        # –í—ã–≤–µ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º
        for source_type in sorted(by_type.keys()):
            sources = by_type[source_type]
            lines.append(f"## {source_type} ({len(sources)})\n\n")

            # –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            seen_urls = set()
            unique_sources = []

            for source in sources:
                url = source.get('url', source.get('source', ''))
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_sources.append(source)
                elif not url:
                    unique_sources.append(source)

            # –í—ã–≤–µ—Å—Ç–∏
            for i, source in enumerate(unique_sources, 1):
                formatted = self.format_source_apa(source)
                lines.append(f"{i}. {formatted}\n")

            lines.append("\n")

        output_file = self.root_dir / "MASTER_BIBLIOGRAPHY.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û–±—â–∞—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è: {output_file}")

    def generate_sources_by_domain(self):
        """–°–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –¥–æ–º–µ–Ω–∞–º"""
        lines = []
        lines.append("# üåê –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º\n\n")

        # –°–æ–±—Ä–∞—Ç—å URL –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        url_sources = [s for s in self.sources if s['type'] == 'url']

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–æ–º–µ–Ω–∞–º
        by_domain = defaultdict(list)

        for source in url_sources:
            domain = source.get('domain', 'Unknown')
            by_domain[domain].append(source)

        lines.append(f"**–í—Å–µ–≥–æ –¥–æ–º–µ–Ω–æ–≤**: {len(by_domain)}\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("## –¢–æ–ø-10 –¥–æ–º–µ–Ω–æ–≤\n\n")

        domain_counts = [(domain, len(sources)) for domain, sources in by_domain.items()]
        domain_counts.sort(key=lambda x: -x[1])

        for i, (domain, count) in enumerate(domain_counts[:10], 1):
            lines.append(f"{i}. **{domain}** ‚Äî {count} —Å—Å—ã–ª–æ–∫\n")

        lines.append("\n## –ü–æ –¥–æ–º–µ–Ω–∞–º\n\n")

        # –î–µ—Ç–∞–ª–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
        for domain in sorted(by_domain.keys()):
            sources = by_domain[domain]
            lines.append(f"### {domain} ({len(sources)})\n\n")

            # –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen_urls = set()
            for source in sources[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10 –Ω–∞ –¥–æ–º–µ–Ω
                url = source['url']
                if url not in seen_urls:
                    seen_urls.add(url)
                    title = source['title'] or 'Untitled'
                    lines.append(f"- [{title}]({url})\n")

            if len(sources) > 10:
                lines.append(f"\n...–∏ –µ—â—ë {len(sources) - 10}\n")

            lines.append("\n")

        output_file = self.root_dir / "SOURCES_BY_DOMAIN.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Bibliography Generator - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏'
    )

    parser.add_argument(
        '-s', '--style',
        choices=['apa', 'mla', 'chicago'],
        default='apa',
        help='–°—Ç–∏–ª—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: APA)'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    generator = BibliographyGenerator(root_dir)

    # –°–æ–±—Ä–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    generator.collect_sources()

    print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–π...\n")

    # –°–æ–∑–¥–∞—Ç—å –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
    generator.generate_bibliography_by_article()
    generator.generate_master_bibliography()
    generator.generate_sources_by_domain()

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
