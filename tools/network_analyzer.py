#!/usr/bin/env python3
"""
Network Analyzer - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Å–µ—Ç–∏
–í—ã—á–∏—Å–ª—è–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Å–µ—Ç–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫:
- Centrality (degree, betweenness, closeness, eigenvector, PageRank)
- Clustering coefficient
- Community detection
- Graph properties (density, diameter, components)
- Path analysis

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: NetworkX, Gephi, igraph, Neo4j
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, deque
import json
import math


class AdvancedNetworkAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–µ—Ç–∏"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.graph = defaultdict(set)  # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ
        self.undirected_graph = defaultdict(set)  # –ù–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–µ—Ç—Ä–∏–∫
        self.articles = set()
        self.article_titles = {}

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def build_network(self):
        print("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–µ—Ç–∏...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            source = str(md_file.relative_to(self.root_dir))
            self.articles.add(source)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem
            self.article_titles[source] = title

            # –ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
            for text, link in links:
                if not link.startswith('http'):
                    try:
                        target = (md_file.parent / link.split('#')[0]).resolve()
                        if target.exists() and target.is_relative_to(self.root_dir):
                            target_path = str(target.relative_to(self.root_dir))
                            self.graph[source].add(target_path)
                            self.undirected_graph[source].add(target_path)
                            self.undirected_graph[target_path].add(source)
                            self.articles.add(target_path)
                    except:
                        pass

        print(f"   Nodes: {len(self.articles)}")
        print(f"   Edges: {sum(len(v) for v in self.graph.values())}\n")

    def calculate_degree_centrality(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å degree centrality"""
        centrality = {}

        for article in self.articles:
            out_degree = len(self.graph[article])
            in_degree = sum(1 for neighbors in self.graph.values() if article in neighbors)

            centrality[article] = {
                'out_degree': out_degree,
                'in_degree': in_degree,
                'total_degree': out_degree + in_degree
            }

        return centrality

    def bfs_shortest_paths(self, start):
        """BFS –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫—Ä–∞—Ç—á–∞–π—à–∏—Ö –ø—É—Ç–µ–π"""
        distances = {start: 0}
        paths_through = defaultdict(int)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—Ä–∞—Ç—á–∞–π—à–∏—Ö –ø—É—Ç–µ–π —á–µ—Ä–µ–∑ —É–∑–µ–ª
        queue = deque([start])
        predecessors = defaultdict(list)

        while queue:
            node = queue.popleft()

            for neighbor in self.undirected_graph[node]:
                if neighbor not in distances:
                    distances[neighbor] = distances[node] + 1
                    queue.append(neighbor)
                    predecessors[neighbor].append(node)
                elif distances[neighbor] == distances[node] + 1:
                    predecessors[neighbor].append(node)

        return distances, predecessors

    def calculate_betweenness_centrality(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å betweenness centrality (–∞–ª–≥–æ—Ä–∏—Ç–º Brandes)"""
        betweenness = {article: 0.0 for article in self.articles}

        for source in self.articles:
            # BFS –æ—Ç source
            distances = {source: 0}
            paths_count = {source: 1}
            stack = []
            predecessors = defaultdict(list)
            queue = deque([source])

            while queue:
                node = queue.popleft()
                stack.append(node)

                for neighbor in self.undirected_graph[node]:
                    # –ü–µ—Ä–≤–æ–µ –ø–æ—Å–µ—â–µ–Ω–∏–µ neighbor
                    if neighbor not in distances:
                        distances[neighbor] = distances[node] + 1
                        queue.append(neighbor)

                    # –ö—Ä–∞—Ç—á–∞–π—à–∏–π –ø—É—Ç—å –∫ neighbor —á–µ—Ä–µ–∑ node
                    if distances[neighbor] == distances[node] + 1:
                        paths_count[neighbor] = paths_count.get(neighbor, 0) + paths_count[node]
                        predecessors[neighbor].append(node)

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            dependency = {article: 0.0 for article in self.articles}

            while stack:
                node = stack.pop()

                for pred in predecessors[node]:
                    dependency[pred] += (paths_count[pred] / paths_count[node]) * (1 + dependency[node])

                if node != source:
                    betweenness[node] += dependency[node]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        n = len(self.articles)
        if n > 2:
            normalization = 1.0 / ((n - 1) * (n - 2))
            betweenness = {k: v * normalization for k, v in betweenness.items()}

        return betweenness

    def calculate_closeness_centrality(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å closeness centrality"""
        closeness = {}

        for article in self.articles:
            distances, _ = self.bfs_shortest_paths(article)

            # –°—É–º–º–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –≤—Å–µ—Ö –¥–æ—Å—Ç–∏–∂–∏–º—ã—Ö —É–∑–ª–æ–≤
            total_distance = sum(distances.values())

            # Closeness = (n-1) / sum(distances)
            if total_distance > 0:
                closeness[article] = (len(distances) - 1) / total_distance
            else:
                closeness[article] = 0.0

        return closeness

    def calculate_pagerank(self, damping=0.85, max_iterations=100, tolerance=1e-6):
        """–í—ã—á–∏—Å–ª–∏—Ç—å PageRank"""
        n = len(self.articles)
        pagerank = {article: 1.0 / n for article in self.articles}

        for iteration in range(max_iterations):
            new_pagerank = {}
            max_diff = 0

            for article in self.articles:
                # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                rank = (1 - damping) / n

                # –í–∫–ª–∞–¥ –æ—Ç –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
                for source in self.articles:
                    if article in self.graph[source]:
                        out_degree = len(self.graph[source])
                        if out_degree > 0:
                            rank += damping * pagerank[source] / out_degree

                new_pagerank[article] = rank
                max_diff = max(max_diff, abs(new_pagerank[article] - pagerank[article]))

            pagerank = new_pagerank

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if max_diff < tolerance:
                break

        return pagerank

    def calculate_clustering_coefficient(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å clustering coefficient (–ª–æ–∫–∞–ª—å–Ω—ã–π –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–π)"""
        clustering = {}

        for article in self.articles:
            neighbors = self.undirected_graph[article]
            k = len(neighbors)

            if k < 2:
                clustering[article] = 0.0
                continue

            # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–æ—Å–µ–¥—è–º–∏
            links_between_neighbors = 0

            for n1 in neighbors:
                for n2 in neighbors:
                    if n1 != n2 and n2 in self.undirected_graph[n1]:
                        links_between_neighbors += 1

            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            max_links = k * (k - 1)
            clustering[article] = links_between_neighbors / max_links if max_links > 0 else 0.0

        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
        global_clustering = sum(clustering.values()) / len(clustering) if clustering else 0.0

        return clustering, global_clustering

    def detect_communities_simple(self):
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ—Å—Ç–≤ (connected components)"""
        visited = set()
        communities = []

        def dfs(node, community):
            visited.add(node)
            community.add(node)

            for neighbor in self.undirected_graph[node]:
                if neighbor not in visited:
                    dfs(neighbor, community)

        for article in self.articles:
            if article not in visited:
                community = set()
                dfs(article, community)
                communities.append(community)

        return communities

    def calculate_graph_properties(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ –≥—Ä–∞—Ñ–∞"""
        n = len(self.articles)
        m = sum(len(v) for v in self.graph.values())

        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å
        max_edges = n * (n - 1)
        density = m / max_edges if max_edges > 0 else 0.0

        # –î–∏–∞–º–µ—Ç—Ä –∏ —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—É—Ç–∏
        all_distances = []

        for article in self.articles:
            distances, _ = self.bfs_shortest_paths(article)
            all_distances.extend(distances.values())

        diameter = max(all_distances) if all_distances else 0
        avg_path_length = sum(all_distances) / len(all_distances) if all_distances else 0

        # Connected components
        communities = self.detect_communities_simple()

        return {
            'nodes': n,
            'edges': m,
            'density': density,
            'diameter': diameter,
            'avg_path_length': avg_path_length,
            'connected_components': len(communities),
            'largest_component': max(len(c) for c in communities) if communities else 0
        }

    def generate_report(self, all_metrics):
        """–°–æ–∑–¥–∞—Ç—å Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üìä –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑\n\n")
        lines.append("> –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π\n\n")

        # –°–≤–æ–π—Å—Ç–≤–∞ –≥—Ä–∞—Ñ–∞
        props = all_metrics['graph_properties']

        lines.append("## –°–≤–æ–π—Å—Ç–≤–∞ –≥—Ä–∞—Ñ–∞\n\n")
        lines.append(f"- **–£–∑–ª–æ–≤ (nodes)**: {props['nodes']}\n")
        lines.append(f"- **–†—ë–±–µ—Ä (edges)**: {props['edges']}\n")
        lines.append(f"- **–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (density)**: {props['density']:.4f}\n")
        lines.append(f"- **–î–∏–∞–º–µ—Ç—Ä (diameter)**: {props['diameter']}\n")
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—É—Ç–∏**: {props['avg_path_length']:.2f}\n")
        lines.append(f"- **Connected components**: {props['connected_components']}\n")
        lines.append(f"- **Largest component**: {props['largest_component']}\n\n")

        # –¢–æ–ø –ø–æ PageRank
        lines.append("## –¢–æ–ø-10 –ø–æ PageRank\n\n")
        sorted_pr = sorted(all_metrics['pagerank'].items(), key=lambda x: -x[1])

        for article, score in sorted_pr[:10]:
            title = self.article_titles.get(article, Path(article).stem)
            lines.append(f"1. **{title}**: {score:.6f}\n")

        # –¢–æ–ø –ø–æ Betweenness
        lines.append("\n## –¢–æ–ø-10 –ø–æ Betweenness Centrality\n\n")
        lines.append("> –°—Ç–∞—Ç—å–∏, –Ω–∞—Ö–æ–¥—è—â–∏–µ—Å—è –Ω–∞ –∫—Ä–∞—Ç—á–∞–π—à–∏—Ö –ø—É—Ç—è—Ö –º–µ–∂–¥—É –¥—Ä—É–≥–∏–º–∏\n\n")

        sorted_bt = sorted(all_metrics['betweenness'].items(), key=lambda x: -x[1])

        for article, score in sorted_bt[:10]:
            title = self.article_titles.get(article, Path(article).stem)
            lines.append(f"1. **{title}**: {score:.6f}\n")

        # –¢–æ–ø –ø–æ Closeness
        lines.append("\n## –¢–æ–ø-10 –ø–æ Closeness Centrality\n\n")
        lines.append("> –°—Ç–∞—Ç—å–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –¥–æ –¥—Ä—É–≥–∏—Ö\n\n")

        sorted_cl = sorted(all_metrics['closeness'].items(), key=lambda x: -x[1])

        for article, score in sorted_cl[:10]:
            title = self.article_titles.get(article, Path(article).stem)
            lines.append(f"1. **{title}**: {score:.6f}\n")

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        lines.append("\n## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n\n")
        lines.append(f"- **–ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**: {all_metrics['global_clustering']:.4f}\n\n")

        lines.append("### –¢–æ–ø-10 –ø–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n\n")
        sorted_clust = sorted(all_metrics['clustering'].items(), key=lambda x: -x[1])

        for article, score in sorted_clust[:10]:
            title = self.article_titles.get(article, Path(article).stem)
            lines.append(f"1. **{title}**: {score:.4f}\n")

        # Degree centrality
        lines.append("\n## –¢–æ–ø-10 –ø–æ Degree Centrality\n\n")
        sorted_deg = sorted(all_metrics['degree'].items(), key=lambda x: -x[1]['total_degree'])

        for article, metrics in sorted_deg[:10]:
            title = self.article_titles.get(article, Path(article).stem)
            lines.append(f"1. **{title}**: {metrics['total_degree']} ")
            lines.append(f"(in: {metrics['in_degree']}, out: {metrics['out_degree']})\n")

        output_file = self.root_dir / "ADVANCED_NETWORK_ANALYSIS.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –°–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑: {output_file}")

    def save_json(self, all_metrics):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON"""
        data = {
            'graph_properties': all_metrics['graph_properties'],
            'nodes': {}
        }

        # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞
        for article in self.articles:
            title = self.article_titles.get(article, Path(article).stem)

            data['nodes'][article] = {
                'title': title,
                'pagerank': all_metrics['pagerank'].get(article, 0),
                'betweenness': all_metrics['betweenness'].get(article, 0),
                'closeness': all_metrics['closeness'].get(article, 0),
                'clustering': all_metrics['clustering'].get(article, 0),
                'degree': all_metrics['degree'].get(article, {})
            }

        output_file = self.root_dir / "network_metrics.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –º–µ—Ç—Ä–∏–∫–∏: {output_file}")


def main():
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    analyzer = AdvancedNetworkAnalyzer(root_dir)
    analyzer.build_network()

    print("üìà –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...\n")

    # –í—ã—á–∏—Å–ª–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
    degree = analyzer.calculate_degree_centrality()
    print("   ‚úì Degree centrality")

    pagerank = analyzer.calculate_pagerank()
    print("   ‚úì PageRank")

    betweenness = analyzer.calculate_betweenness_centrality()
    print("   ‚úì Betweenness centrality")

    closeness = analyzer.calculate_closeness_centrality()
    print("   ‚úì Closeness centrality")

    clustering, global_clustering = analyzer.calculate_clustering_coefficient()
    print("   ‚úì Clustering coefficient")

    graph_properties = analyzer.calculate_graph_properties()
    print("   ‚úì Graph properties\n")

    # –°–æ–±—Ä–∞—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
    all_metrics = {
        'degree': degree,
        'pagerank': pagerank,
        'betweenness': betweenness,
        'closeness': closeness,
        'clustering': clustering,
        'global_clustering': global_clustering,
        'graph_properties': graph_properties
    }

    # –°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç—ã
    analyzer.generate_report(all_metrics)
    analyzer.save_json(all_metrics)


if __name__ == "__main__":
    main()
