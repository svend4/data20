#!/usr/bin/env python3
"""
Knowledge Graph Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
–°–æ–∑–¥–∞—ë—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏, –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ –∏ —Ç–∏–ø–∞–º–∏

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: DBpedia, Wikidata, Google Knowledge Graph
–§–æ—Ä–º–∞—Ç—ã: RDF, JSON-LD, Neo4j Cypher
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import json
import argparse
from typing import Dict, List, Tuple, Set
import math


class EntityLinker:
    """–õ–∏–Ω–∫–æ–≤–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""

    def __init__(self, entities: Dict):
        self.entities = entities

    def levenshtein_distance(self, s1: str, s2: str) -> int:
        """–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def find_similar_entities(self, threshold: float = 0.8) -> List[Tuple[str, str, float]]:
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ)"""
        similar_pairs = []
        entity_names = list(self.entities.keys())

        for i, name1 in enumerate(entity_names):
            for name2 in entity_names[i+1:]:
                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å
                norm1 = name1.lower().strip()
                norm2 = name2.lower().strip()

                if norm1 == norm2:
                    continue

                # –†–∞—Å—á—ë—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏
                distance = self.levenshtein_distance(norm1, norm2)
                max_len = max(len(norm1), len(norm2))
                similarity = 1 - (distance / max_len)

                if similarity >= threshold:
                    similar_pairs.append((name1, name2, similarity))

        return sorted(similar_pairs, key=lambda x: -x[2])

    def merge_entities(self, entity1: str, entity2: str, target_name: str = None):
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥–≤–µ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if entity1 not in self.entities or entity2 not in self.entities:
            return

        target = target_name or entity1

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
        self.entities[target]['mentions'].extend(self.entities[entity2]['mentions'])

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∞–ª–∏–∞—Å—ã
        self.entities[target]['aliases'].add(entity2)
        self.entities[target]['aliases'].update(self.entities[entity2].get('aliases', set()))

        # –£–¥–∞–ª–∏—Ç—å –≤—Ç–æ—Ä—É—é —Å—É—â–Ω–æ—Å—Ç—å
        del self.entities[entity2]


class GraphAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""

    def __init__(self, entities: Dict, relations: List[Dict]):
        self.entities = entities
        self.relations = relations

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å adjacency list
        self.adjacency = defaultdict(set)
        for rel in relations:
            self.adjacency[rel['subject']].add(rel['object'])

    def calculate_degree_centrality(self) -> Dict[str, int]:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–µ–ø–µ–Ω—å –≤–µ—Ä—à–∏–Ω (degree centrality)"""
        degree = Counter()

        for rel in self.relations:
            degree[rel['subject']] += 1  # –ò—Å—Ö–æ–¥—è—â–∞—è —Å—Ç–µ–ø–µ–Ω—å
            degree[rel['object']] += 1   # –í—Ö–æ–¥—è—â–∞—è —Å—Ç–µ–ø–µ–Ω—å

        return dict(degree)

    def calculate_betweenness_centrality_approx(self) -> Dict[str, float]:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è betweenness centrality (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å—É—â–Ω–æ—Å—Ç—å –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö
        betweenness = Counter()

        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—É—â–Ω–æ—Å—Ç–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π = –≤—ã—Å–æ–∫–∏–π betweenness
        for entity in self.entities:
            incoming = sum(1 for r in self.relations if r['object'] == entity)
            outgoing = sum(1 for r in self.relations if r['subject'] == entity)

            # Betweenness approximation
            betweenness[entity] = incoming * outgoing

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        max_val = max(betweenness.values()) if betweenness else 1
        return {k: v / max_val for k, v in betweenness.items()}

    def find_hubs(self, top_n: int = 10) -> List[Tuple[str, int]]:
        """–ù–∞–π—Ç–∏ —Ö–∞–±—ã (—Å—É—â–Ω–æ—Å—Ç–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π)"""
        degree = self.calculate_degree_centrality()
        return sorted(degree.items(), key=lambda x: -x[1])[:top_n]

    def calculate_clustering_coefficient(self, entity: str) -> float:
        """–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–∏"""
        neighbors = list(self.adjacency.get(entity, set()))

        if len(neighbors) < 2:
            return 0.0

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–æ—Å–µ–¥—è–º–∏
        neighbor_connections = 0
        for i, n1 in enumerate(neighbors):
            for n2 in neighbors[i+1:]:
                if n2 in self.adjacency.get(n1, set()):
                    neighbor_connections += 1

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π
        max_connections = len(neighbors) * (len(neighbors) - 1) / 2

        return neighbor_connections / max_connections if max_connections > 0 else 0.0

    def detect_communities_simple(self) -> Dict[str, Set[str]]:
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–æ–æ–±—â–µ—Å—Ç–≤ (–ø–æ —Ç–∏–ø–∞–º —Å—É—â–Ω–æ—Å—Ç–µ–π)"""
        communities = defaultdict(set)

        for entity_name, entity_data in self.entities.items():
            entity_type = entity_data.get('type', 'Unknown')
            communities[entity_type].add(entity_name)

        return dict(communities)


class Neo4jExporter:
    """–≠–∫—Å–ø–æ—Ä—Ç –≤ Neo4j Cypher queries"""

    def __init__(self, entities: Dict, relations: List[Dict]):
        self.entities = entities
        self.relations = relations

    def generate_cypher_queries(self) -> List[str]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Cypher –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è Neo4j"""
        queries = []

        # CREATE –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        queries.append("// –°–æ–∑–¥–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π")
        queries.append("")

        for entity_name, entity_data in self.entities.items():
            safe_name = re.sub(r'[^\w]', '_', entity_name)
            entity_type = entity_data.get('type', 'Unknown')
            importance = entity_data.get('importance', 0)

            query = f"CREATE (n_{safe_name}:{entity_type} {{"
            query += f"name: \"{entity_name}\", "
            query += f"importance: {importance}"
            query += "})"

            queries.append(query)

        queries.append("")
        queries.append("// –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π")
        queries.append("")

        # CREATE –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        for i, rel in enumerate(self.relations):
            subj_safe = re.sub(r'[^\w]', '_', rel['subject'])
            obj_safe = re.sub(r'[^\w]', '_', rel['object'])
            pred = rel['predicate'].upper()

            query = f"MATCH (a {{name: \"{rel['subject']}\"}}), (b {{name: \"{rel['object']}\"}})"
            query2 = f"CREATE (a)-[:{pred}]->(b)"

            queries.append(query)
            queries.append(query2)

            if i > 50:  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤
                queries.append(f"\n// ... –∏ –µ—â—ë {len(self.relations) - i} –æ—Ç–Ω–æ—à–µ–Ω–∏–π\n")
                break

        return queries

    def save_cypher_file(self, output_path: Path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å Cypher –∑–∞–ø—Ä–æ—Å—ã –≤ —Ñ–∞–π–ª"""
        queries = self.generate_cypher_queries()

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("// Neo4j Cypher Import Script\n")
            f.write("// –ò–º–ø–æ—Ä—Ç –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –≤ Neo4j\n\n")
            f.write('\n'.join(queries))

        print(f"‚úÖ Neo4j Cypher: {output_path}")


class SPARQLQueryGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤"""

    @staticmethod
    def generate_sample_queries() -> List[Tuple[str, str]]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤"""
        queries = []

        # 1. –í—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞
        queries.append((
            "–í—Å–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
            """
PREFIX kg: <http://example.org/kg#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

SELECT ?entity ?label
WHERE {
    ?entity rdf:type kg:Technology .
    ?entity rdfs:label ?label .
}
ORDER BY ?label
            """.strip()
        ))

        # 2. –¢–æ–ø —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        queries.append((
            "–¢–æ–ø-10 –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏",
            """
PREFIX kg: <http://example.org/kg#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?entity ?label ?importance
WHERE {
    ?entity rdfs:label ?label .
    ?entity kg:importance ?importance .
}
ORDER BY DESC(?importance)
LIMIT 10
            """.strip()
        ))

        # 3. –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        queries.append((
            "–í—Å–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è 'uses'",
            """
PREFIX kg: <http://example.org/kg#>

SELECT ?subject ?object
WHERE {
    ?subject kg:uses ?object .
}
            """.strip()
        ))

        return queries

    @staticmethod
    def save_sparql_queries(output_path: Path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤"""
        queries = SPARQLQueryGenerator.generate_sample_queries()

        lines = []
        lines.append("# SPARQL Query Examples\n\n")
        lines.append("–ü—Ä–∏–º–µ—Ä—ã SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.\n\n")

        for title, query in queries:
            lines.append(f"## {title}\n\n")
            lines.append("```sparql\n")
            lines.append(query)
            lines.append("\n```\n\n")

        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ SPARQL –ø—Ä–∏–º–µ—Ä—ã: {output_path}")


class AdvancedKnowledgeGraphBuilder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—É—â–Ω–æ—Å—Ç–∏ —Å —Ç–∏–ø–∞–º–∏
        self.entities = defaultdict(lambda: {
            'type': 'Unknown',
            'mentions': [],
            'properties': {},
            'aliases': set()
        })

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è (—Ç—Ä–æ–π–∫–∏: subject-predicate-object)
        self.relations = []

        # –°–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_type_patterns = {
            'Technology': r'\b(Python|Docker|Kubernetes|LLM|API|Git|Linux|JavaScript|React)\b',
            'Concept': r'\*\*([–ê-–ØA-Z][–∞-—èa-z]{3,30}?)\*\*',
            'Organization': r'\b(Google|Microsoft|Apple|Amazon|Facebook|Meta)\b',
            'Product': r'\b(ChatGPT|GPT-4|Claude|Gemini)\b',
            'Method': r'\b([–ê-–Ø][–∞-—è]+(?:–∞—Ü–∏—è|–Ω–∏–µ|—Ç–æ—Ä|–∫–∞))\b'  # –†—É—Å—Å–∫–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        }

        # –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.entity_linker = None
        self.graph_analyzer = None
        self.neo4j_exporter = None

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def detect_entity_type(self, entity_name):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏"""
        for entity_type, pattern in self.entity_type_patterns.items():
            if re.search(pattern, entity_name, re.IGNORECASE):
                return entity_type

        # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        if entity_name[0].isupper():
            return 'Concept'

        return 'Unknown'

    def extract_entities(self, content, article_path, article_title):
        """–ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        found_entities = []

        # 1. –í—ã–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (**—Ç–µ—Ä–º–∏–Ω**)
        bold_terms = re.findall(r'\*\*([–ê-–ØA-Z][^\*]{2,40}?)\*\*', content)

        for term in bold_terms:
            clean_term = term.strip()
            entity_type = self.detect_entity_type(clean_term)

            self.entities[clean_term]['type'] = entity_type
            self.entities[clean_term]['mentions'].append({
                'article': article_path,
                'article_title': article_title
            })

            found_entities.append(clean_term)

        # 2. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ (–ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º)
        tech_pattern = self.entity_type_patterns['Technology']
        technologies = re.findall(tech_pattern, content)

        for tech in set(technologies):
            self.entities[tech]['type'] = 'Technology'
            if article_path not in [m['article'] for m in self.entities[tech]['mentions']]:
                self.entities[tech]['mentions'].append({
                    'article': article_path,
                    'article_title': article_title
                })

            found_entities.append(tech)

        # 3. –ò–∑–≤–ª–µ—á—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headings = re.findall(r'^#{2,6}\s+(.+)$', content, re.MULTILINE)

        for heading in headings:
            clean_heading = re.sub(r'[#*`\[\]()]', '', heading).strip()

            if len(clean_heading) > 3 and clean_heading not in self.entities:
                entity_type = self.detect_entity_type(clean_heading)
                self.entities[clean_heading]['type'] = entity_type
                self.entities[clean_heading]['mentions'].append({
                    'article': article_path,
                    'article_title': article_title
                })

                found_entities.append(clean_heading)

        return found_entities

    def extract_relations(self, content, entities, article_path):
        """–ò–∑–≤–ª–µ—á—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        relation_patterns = [
            (r'(\w+)\s+—è–≤–ª—è–µ—Ç—Å—è\s+(\w+)', 'is_a'),
            (r'(\w+)\s+—á–∞—Å—Ç—å\s+(\w+)', 'part_of'),
            (r'(\w+)\s+–∏—Å–ø–æ–ª—å–∑—É–µ—Ç\s+(\w+)', 'uses'),
            (r'(\w+)\s+—Ç—Ä–µ–±—É–µ—Ç\s+(\w+)', 'requires'),
            (r'(\w+)\s+–æ—Å–Ω–æ–≤–∞–Ω\s+–Ω–∞\s+(\w+)', 'based_on'),
            (r'(\w+)\s+‚Üí\s+(\w+)', 'leads_to')
        ]

        for pattern, relation_type in relation_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)

            for subject, obj in matches:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –æ–±–∞ - –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
                if subject in entities and obj in entities:
                    self.relations.append({
                        'subject': subject,
                        'predicate': relation_type,
                        'object': obj,
                        'source': article_path
                    })

        # –ê–Ω–∞–ª–∏–∑ co-occurrence (—Å—É—â–Ω–æ—Å—Ç–∏ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏)
        sentences = re.split(r'[.!?]+', content)

        for sentence in sentences:
            # –ù–∞–π—Ç–∏ –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
            sentence_entities = [e for e in entities if e in sentence]

            # –°–æ–∑–¥–∞—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è co-occurrence
            if len(sentence_entities) >= 2:
                for i, ent1 in enumerate(sentence_entities):
                    for ent2 in sentence_entities[i+1:]:
                        self.relations.append({
                            'subject': ent1,
                            'predicate': 'co_occurs_with',
                            'object': ent2,
                            'source': article_path
                        })

    def build_graph(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π"""
        print("üï∏Ô∏è  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...\n")

        all_entities_by_article = {}

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ - –∏–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            entities = self.extract_entities(content, article_path, title)
            all_entities_by_article[article_path] = entities

        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥ - –∏–∑–≤–ª–µ—á—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            _, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            entities = all_entities_by_article.get(article_path, [])

            self.extract_relations(content, entities, article_path)

        print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {len(self.entities)}")
        print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏–π: {len(self.relations)}\n")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.entity_linker = EntityLinker(self.entities)
        self.graph_analyzer = GraphAnalyzer(self.entities, self.relations)
        self.neo4j_exporter = Neo4jExporter(self.entities, self.relations)

    def calculate_entity_importance(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        for entity_name, entity_data in self.entities.items():
            # –í–∞–∂–Ω–æ—Å—Ç—å = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π + –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
            mentions_count = len(entity_data['mentions'])

            # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
            relations_count = sum(
                1 for r in self.relations
                if r['subject'] == entity_name or r['object'] == entity_name
            )

            entity_data['importance'] = mentions_count + relations_count * 2

    def run_entity_linking(self, threshold: float = 0.8):
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –ª–∏–Ω–∫–æ–≤–∫—É –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        if not self.entity_linker:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ (build_graph)")
            return

        print(f"\nüîó –õ–∏–Ω–∫–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–ø–æ—Ä–æ–≥: {threshold})\n")

        similar = self.entity_linker.find_similar_entities(threshold)

        if not similar:
            print("   –ü–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return

        print(f"–ù–∞–π–¥–µ–Ω–æ {len(similar)} –ø–∞—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π:")
        for entity1, entity2, similarity in similar[:10]:
            print(f"   ‚Ä¢ {entity1} ‚âà {entity2} ({similarity:.2%})")

        if len(similar) > 10:
            print(f"   ... –∏ –µ—â—ë {len(similar) - 10}")

    def run_graph_analysis(self):
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞"""
        if not self.graph_analyzer:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ (build_graph)")
            return

        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞\n")

        # –•–∞–±—ã
        hubs = self.graph_analyzer.find_hubs(top_n=10)
        if hubs:
            print("–¢–æ–ø-10 —Ö–∞–±–æ–≤ (–ø–æ —Å—Ç–µ–ø–µ–Ω–∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏):")
            for entity, degree in hubs:
                print(f"   ‚Ä¢ {entity}: {degree} —Å–≤—è–∑–µ–π")

        # –°–æ–æ–±—â–µ—Å—Ç–≤–∞
        communities = self.graph_analyzer.detect_communities_simple()
        print(f"\n–°–æ–æ–±—â–µ—Å—Ç–≤–∞ (–ø–æ —Ç–∏–ø–∞–º):")
        for comm_type, members in sorted(communities.items(), key=lambda x: -len(x[1])):
            print(f"   {comm_type}: {len(members)} —Å—É—â–Ω–æ—Å—Ç–µ–π")

        # Centrality (—Ç–æ–ø-5)
        betweenness = self.graph_analyzer.calculate_betweenness_centrality_approx()
        top_betweenness = sorted(betweenness.items(), key=lambda x: -x[1])[:5]
        if top_betweenness:
            print(f"\n–¢–æ–ø-5 –ø–æ betweenness centrality:")
            for entity, score in top_betweenness:
                if score > 0:
                    print(f"   ‚Ä¢ {entity}: {score:.2f}")

    def export_neo4j(self):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ Neo4j Cypher"""
        if not self.neo4j_exporter:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ (build_graph)")
            return

        output_path = self.root_dir / "knowledge_graph_neo4j.cypher"
        self.neo4j_exporter.save_cypher_file(output_path)

    def export_sparql_queries(self):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤"""
        output_path = self.root_dir / "SPARQL_QUERIES.md"
        SPARQLQueryGenerator.save_sparql_queries(output_path)

    def generate_markdown_report(self):
        """–°–æ–∑–¥–∞—Ç—å Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üï∏Ô∏è –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π\n\n")
        lines.append("> –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        entity_types = defaultdict(int)
        for entity_data in self.entities.values():
            entity_types[entity_data['type']] += 1

        relation_types = defaultdict(int)
        for rel in self.relations:
            relation_types[rel['predicate']] += 1

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–°—É—â–Ω–æ—Å—Ç–µ–π**: {len(self.entities)}\n")
        lines.append(f"- **–û—Ç–Ω–æ—à–µ–Ω–∏–π**: {len(self.relations)}\n\n")

        lines.append("### –ü–æ —Ç–∏–ø–∞–º —Å—É—â–Ω–æ—Å—Ç–µ–π\n\n")
        for entity_type, count in sorted(entity_types.items(), key=lambda x: -x[1]):
            lines.append(f"- **{entity_type}**: {count}\n")

        lines.append("\n### –ü–æ —Ç–∏–ø–∞–º –æ—Ç–Ω–æ—à–µ–Ω–∏–π\n\n")
        for rel_type, count in sorted(relation_types.items(), key=lambda x: -x[1])[:10]:
            lines.append(f"- **{rel_type}**: {count}\n")

        # –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        self.calculate_entity_importance()

        lines.append("\n## –¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π\n\n")

        sorted_entities = sorted(
            self.entities.items(),
            key=lambda x: -x[1].get('importance', 0)
        )

        for entity_name, entity_data in sorted_entities[:20]:
            lines.append(f"### {entity_name}\n\n")
            lines.append(f"- **–¢–∏–ø**: {entity_data['type']}\n")
            lines.append(f"- **–í–∞–∂–Ω–æ—Å—Ç—å**: {entity_data.get('importance', 0)}\n")
            lines.append(f"- **–£–ø–æ–º–∏–Ω–∞–Ω–∏–π**: {len(entity_data['mentions'])}\n")

            if entity_data['mentions']:
                lines.append("\n**–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤:**\n")
                for mention in entity_data['mentions'][:5]:
                    lines.append(f"- [{mention['article_title']}]({mention['article']})\n")

                if len(entity_data['mentions']) > 5:
                    lines.append(f"\n...–∏ –µ—â—ë {len(entity_data['mentions']) - 5}\n")

            lines.append("\n")

        # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        lines.append("\n## –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π\n\n")

        for rel in self.relations[:30]:
            lines.append(f"- **{rel['subject']}** `{rel['predicate']}` **{rel['object']}**\n")

        if len(self.relations) > 30:
            lines.append(f"\n*...–∏ –µ—â—ë {len(self.relations) - 30} –æ—Ç–Ω–æ—à–µ–Ω–∏–π*\n")

        output_file = self.root_dir / "KNOWLEDGE_GRAPH.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown –æ—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON"""
        data = {
            'entities': {
                name: {
                    'type': entity_data['type'],
                    'mentions': entity_data['mentions'],
                    'importance': entity_data.get('importance', 0)
                }
                for name, entity_data in self.entities.items()
            },
            'relations': self.relations,
            'statistics': {
                'total_entities': len(self.entities),
                'total_relations': len(self.relations)
            }
        }

        output_file = self.root_dir / "knowledge_graph_advanced.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –≥—Ä–∞—Ñ: {output_file}")

    def export_rdf(self):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ RDF Turtle format"""
        lines = []
        lines.append("@prefix kg: <http://example.org/kg#> .\n")
        lines.append("@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n")
        lines.append("@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n")

        # –°—É—â–Ω–æ—Å—Ç–∏
        for entity_name, entity_data in self.entities.items():
            safe_name = re.sub(r'[^\w]', '_', entity_name)

            lines.append(f"kg:{safe_name}\n")
            lines.append(f"    rdf:type kg:{entity_data['type']} ;\n")
            lines.append(f"    rdfs:label \"{entity_name}\" ;\n")
            lines.append(f"    kg:importance {entity_data.get('importance', 0)} .\n\n")

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è
        for rel in self.relations:
            subj = re.sub(r'[^\w]', '_', rel['subject'])
            obj = re.sub(r'[^\w]', '_', rel['object'])
            pred = rel['predicate']

            lines.append(f"kg:{subj} kg:{pred} kg:{obj} .\n")

        output_file = self.root_dir / "knowledge_graph.ttl"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ RDF Turtle: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='üï∏Ô∏è Knowledge Graph Builder - –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∏ —Å–æ–∑–¥–∞—Ç—å –≤—Å–µ –æ—Ç—á—ë—Ç—ã
  %(prog)s --analyze          # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞
  %(prog)s --link             # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
  %(prog)s --neo4j            # –≠–∫—Å–ø–æ—Ä—Ç –≤ Neo4j Cypher
  %(prog)s --sparql           # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤
  %(prog)s --all              # –í—Å—ë: –≥—Ä–∞—Ñ + –∞–Ω–∞–ª–∏–∑—ã + —ç–∫—Å–ø–æ—Ä—Ç—ã
        """
    )

    parser.add_argument(
        '--analyze',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞ (—Ö–∞–±—ã, centrality, —Å–æ–æ–±—â–µ—Å—Ç–≤–∞)'
    )

    parser.add_argument(
        '--link',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Å—Ç–∏ –ª–∏–Ω–∫–æ–≤–∫—É –ø–æ—Ö–æ–∂–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π'
    )

    parser.add_argument(
        '--link-threshold',
        type=float,
        default=0.8,
        metavar='THRESHOLD',
        help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –ª–∏–Ω–∫–æ–≤–∫–∏ (0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.8)'
    )

    parser.add_argument(
        '--neo4j',
        action='store_true',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ Neo4j Cypher —Ñ–æ—Ä–º–∞—Ç'
    )

    parser.add_argument(
        '--sparql',
        action='store_true',
        help='–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã SPARQL –∑–∞–ø—Ä–æ—Å–æ–≤'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –∏ —ç–∫—Å–ø–æ—Ä—Ç—ã'
    )

    parser.add_argument(
        '--no-report',
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å markdown –æ—Ç—á—ë—Ç'
    )

    parser.add_argument(
        '--no-json',
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å JSON —Ñ–∞–π–ª'
    )

    parser.add_argument(
        '--no-rdf',
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å RDF Turtle —Ñ–∞–π–ª'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = AdvancedKnowledgeGraphBuilder(root_dir)

    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ
    builder.build_graph()

    # –†–µ–∂–∏–º --all
    if args.all:
        builder.run_graph_analysis()
        builder.run_entity_linking(args.link_threshold)
        if not args.no_report:
            builder.generate_markdown_report()
        if not args.no_json:
            builder.save_json()
        if not args.no_rdf:
            builder.export_rdf()
        builder.export_neo4j()
        builder.export_sparql_queries()
        return

    # –û—Ç–¥–µ–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
    if args.analyze:
        builder.run_graph_analysis()

    if args.link:
        builder.run_entity_linking(args.link_threshold)

    # –≠–∫—Å–ø–æ—Ä—Ç—ã
    if args.neo4j:
        builder.export_neo4j()

    if args.sparql:
        builder.export_sparql_queries()

    # –î–µ–π—Å—Ç–≤–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–ª–∞–≥–∏)
    if not any([args.analyze, args.link, args.neo4j, args.sparql]):
        if not args.no_report:
            builder.generate_markdown_report()
        if not args.no_json:
            builder.save_json()
        if not args.no_rdf:
            builder.export_rdf()


if __name__ == "__main__":
    main()
