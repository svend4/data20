#!/usr/bin/env python3
"""
Knowledge Graph Builder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
–°–æ–∑–¥–∞—ë—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏, –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ –∏ —Ç–∏–ø–∞–º–∏

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: DBpedia, Wikidata, Google Knowledge Graph
–§–æ—Ä–º–∞—Ç—ã: RDF, JSON-LD, Neo4j Cypher
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict
import json


class AdvancedKnowledgeGraphBuilder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –°—É—â–Ω–æ—Å—Ç–∏ —Å —Ç–∏–ø–∞–º–∏
        self.entities = defaultdict(lambda: {
            'type': 'Unknown',
            'mentions': [],
            'properties': {},
            'aliases': set()
        })

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è (—Ç—Ä–æ–π–∫–∏: subject-predicate-object)
        self.relations = []

        # –°–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_type_patterns = {
            'Technology': r'\b(Python|Docker|Kubernetes|LLM|API|Git|Linux|JavaScript|React)\b',
            'Concept': r'\*\*([–ê-–ØA-Z][–∞-—èa-z]{3,30}?)\*\*',
            'Organization': r'\b(Google|Microsoft|Apple|Amazon|Facebook|Meta)\b',
            'Product': r'\b(ChatGPT|GPT-4|Claude|Gemini)\b',
            'Method': r'\b([–ê-–Ø][–∞-—è]+(?:–∞—Ü–∏—è|–Ω–∏–µ|—Ç–æ—Ä|–∫–∞))\b'  # –†—É—Å—Å–∫–∏–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        }

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def detect_entity_type(self, entity_name):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏"""
        for entity_type, pattern in self.entity_type_patterns.items():
            if re.search(pattern, entity_name, re.IGNORECASE):
                return entity_type

        # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        if entity_name[0].isupper():
            return 'Concept'

        return 'Unknown'

    def extract_entities(self, content, article_path, article_title):
        """–ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        found_entities = []

        # 1. –í—ã–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (**—Ç–µ—Ä–º–∏–Ω**)
        bold_terms = re.findall(r'\*\*([–ê-–ØA-Z][^\*]{2,40}?)\*\*', content)

        for term in bold_terms:
            clean_term = term.strip()
            entity_type = self.detect_entity_type(clean_term)

            self.entities[clean_term]['type'] = entity_type
            self.entities[clean_term]['mentions'].append({
                'article': article_path,
                'article_title': article_title
            })

            found_entities.append(clean_term)

        # 2. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ (–ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º)
        tech_pattern = self.entity_type_patterns['Technology']
        technologies = re.findall(tech_pattern, content)

        for tech in set(technologies):
            self.entities[tech]['type'] = 'Technology'
            if article_path not in [m['article'] for m in self.entities[tech]['mentions']]:
                self.entities[tech]['mentions'].append({
                    'article': article_path,
                    'article_title': article_title
                })

            found_entities.append(tech)

        # 3. –ò–∑–≤–ª–µ—á—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headings = re.findall(r'^#{2,6}\s+(.+)$', content, re.MULTILINE)

        for heading in headings:
            clean_heading = re.sub(r'[#*`\[\]()]', '', heading).strip()

            if len(clean_heading) > 3 and clean_heading not in self.entities:
                entity_type = self.detect_entity_type(clean_heading)
                self.entities[clean_heading]['type'] = entity_type
                self.entities[clean_heading]['mentions'].append({
                    'article': article_path,
                    'article_title': article_title
                })

                found_entities.append(clean_heading)

        return found_entities

    def extract_relations(self, content, entities, article_path):
        """–ò–∑–≤–ª–µ—á—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        relation_patterns = [
            (r'(\w+)\s+—è–≤–ª—è–µ—Ç—Å—è\s+(\w+)', 'is_a'),
            (r'(\w+)\s+—á–∞—Å—Ç—å\s+(\w+)', 'part_of'),
            (r'(\w+)\s+–∏—Å–ø–æ–ª—å–∑—É–µ—Ç\s+(\w+)', 'uses'),
            (r'(\w+)\s+—Ç—Ä–µ–±—É–µ—Ç\s+(\w+)', 'requires'),
            (r'(\w+)\s+–æ—Å–Ω–æ–≤–∞–Ω\s+–Ω–∞\s+(\w+)', 'based_on'),
            (r'(\w+)\s+‚Üí\s+(\w+)', 'leads_to')
        ]

        for pattern, relation_type in relation_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)

            for subject, obj in matches:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –æ–±–∞ - –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
                if subject in entities and obj in entities:
                    self.relations.append({
                        'subject': subject,
                        'predicate': relation_type,
                        'object': obj,
                        'source': article_path
                    })

        # –ê–Ω–∞–ª–∏–∑ co-occurrence (—Å—É—â–Ω–æ—Å—Ç–∏ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏)
        sentences = re.split(r'[.!?]+', content)

        for sentence in sentences:
            # –ù–∞–π—Ç–∏ –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
            sentence_entities = [e for e in entities if e in sentence]

            # –°–æ–∑–¥–∞—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è co-occurrence
            if len(sentence_entities) >= 2:
                for i, ent1 in enumerate(sentence_entities):
                    for ent2 in sentence_entities[i+1:]:
                        self.relations.append({
                            'subject': ent1,
                            'predicate': 'co_occurs_with',
                            'object': ent2,
                            'source': article_path
                        })

    def build_graph(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π"""
        print("üï∏Ô∏è  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...\n")

        all_entities_by_article = {}

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ - –∏–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem

            entities = self.extract_entities(content, article_path, title)
            all_entities_by_article[article_path] = entities

        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥ - –∏–∑–≤–ª–µ—á—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            _, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            entities = all_entities_by_article.get(article_path, [])

            self.extract_relations(content, entities, article_path)

        print(f"   –°—É—â–Ω–æ—Å—Ç–µ–π: {len(self.entities)}")
        print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏–π: {len(self.relations)}\n")

    def calculate_entity_importance(self):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        for entity_name, entity_data in self.entities.items():
            # –í–∞–∂–Ω–æ—Å—Ç—å = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π + –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
            mentions_count = len(entity_data['mentions'])

            # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è
            relations_count = sum(
                1 for r in self.relations
                if r['subject'] == entity_name or r['object'] == entity_name
            )

            entity_data['importance'] = mentions_count + relations_count * 2

    def generate_markdown_report(self):
        """–°–æ–∑–¥–∞—Ç—å Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üï∏Ô∏è –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π\n\n")
        lines.append("> –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏\n\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        entity_types = defaultdict(int)
        for entity_data in self.entities.values():
            entity_types[entity_data['type']] += 1

        relation_types = defaultdict(int)
        for rel in self.relations:
            relation_types[rel['predicate']] += 1

        lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–°—É—â–Ω–æ—Å—Ç–µ–π**: {len(self.entities)}\n")
        lines.append(f"- **–û—Ç–Ω–æ—à–µ–Ω–∏–π**: {len(self.relations)}\n\n")

        lines.append("### –ü–æ —Ç–∏–ø–∞–º —Å—É—â–Ω–æ—Å—Ç–µ–π\n\n")
        for entity_type, count in sorted(entity_types.items(), key=lambda x: -x[1]):
            lines.append(f"- **{entity_type}**: {count}\n")

        lines.append("\n### –ü–æ —Ç–∏–ø–∞–º –æ—Ç–Ω–æ—à–µ–Ω–∏–π\n\n")
        for rel_type, count in sorted(relation_types.items(), key=lambda x: -x[1])[:10]:
            lines.append(f"- **{rel_type}**: {count}\n")

        # –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        self.calculate_entity_importance()

        lines.append("\n## –¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π\n\n")

        sorted_entities = sorted(
            self.entities.items(),
            key=lambda x: -x[1].get('importance', 0)
        )

        for entity_name, entity_data in sorted_entities[:20]:
            lines.append(f"### {entity_name}\n\n")
            lines.append(f"- **–¢–∏–ø**: {entity_data['type']}\n")
            lines.append(f"- **–í–∞–∂–Ω–æ—Å—Ç—å**: {entity_data.get('importance', 0)}\n")
            lines.append(f"- **–£–ø–æ–º–∏–Ω–∞–Ω–∏–π**: {len(entity_data['mentions'])}\n")

            if entity_data['mentions']:
                lines.append("\n**–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤:**\n")
                for mention in entity_data['mentions'][:5]:
                    lines.append(f"- [{mention['article_title']}]({mention['article']})\n")

                if len(entity_data['mentions']) > 5:
                    lines.append(f"\n...–∏ –µ—â—ë {len(entity_data['mentions']) - 5}\n")

            lines.append("\n")

        # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        lines.append("\n## –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π\n\n")

        for rel in self.relations[:30]:
            lines.append(f"- **{rel['subject']}** `{rel['predicate']}` **{rel['object']}**\n")

        if len(self.relations) > 30:
            lines.append(f"\n*...–∏ –µ—â—ë {len(self.relations) - 30} –æ—Ç–Ω–æ—à–µ–Ω–∏–π*\n")

        output_file = self.root_dir / "KNOWLEDGE_GRAPH.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown –æ—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON"""
        data = {
            'entities': {
                name: {
                    'type': entity_data['type'],
                    'mentions': entity_data['mentions'],
                    'importance': entity_data.get('importance', 0)
                }
                for name, entity_data in self.entities.items()
            },
            'relations': self.relations,
            'statistics': {
                'total_entities': len(self.entities),
                'total_relations': len(self.relations)
            }
        }

        output_file = self.root_dir / "knowledge_graph_advanced.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –≥—Ä–∞—Ñ: {output_file}")

    def export_rdf(self):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ RDF Turtle format"""
        lines = []
        lines.append("@prefix kg: <http://example.org/kg#> .\n")
        lines.append("@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n")
        lines.append("@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n")

        # –°—É—â–Ω–æ—Å—Ç–∏
        for entity_name, entity_data in self.entities.items():
            safe_name = re.sub(r'[^\w]', '_', entity_name)

            lines.append(f"kg:{safe_name}\n")
            lines.append(f"    rdf:type kg:{entity_data['type']} ;\n")
            lines.append(f"    rdfs:label \"{entity_name}\" ;\n")
            lines.append(f"    kg:importance {entity_data.get('importance', 0)} .\n\n")

        # –û—Ç–Ω–æ—à–µ–Ω–∏—è
        for rel in self.relations:
            subj = re.sub(r'[^\w]', '_', rel['subject'])
            obj = re.sub(r'[^\w]', '_', rel['object'])
            pred = rel['predicate']

            lines.append(f"kg:{subj} kg:{pred} kg:{obj} .\n")

        output_file = self.root_dir / "knowledge_graph.ttl"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ RDF Turtle: {output_file}")


def main():
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = AdvancedKnowledgeGraphBuilder(root_dir)
    builder.build_graph()
    builder.generate_markdown_report()
    builder.save_json()
    builder.export_rdf()


if __name__ == "__main__":
    main()
