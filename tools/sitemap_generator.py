#!/usr/bin/env python3
"""
Advanced Sitemap Generator - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä sitemap.xml
–§—É–Ω–∫—Ü–∏–∏:
- Dynamic priority calculation (based on importance)
- Change frequency detection
- Multi-sitemap support (50,000 URLs per file)
- Sitemap index generation
- Ping search engines (Google, Bing)
- Image sitemap support
- Validation
- Compression (.gz)
- robots.txt generation
- Statistics and reporting

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Google Search Console, Yoast SEO, XML Sitemap generators
"""

from pathlib import Path
import yaml
import re
from datetime import datetime, timedelta
import gzip
import urllib.request
import urllib.parse
from collections import defaultdict


class AdvancedSitemapGenerator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä sitemap"""

    def __init__(self, root_dir=".", base_url="https://example.com"):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.base_url = base_url.rstrip('/')

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è sitemap (Google spec)
        self.max_urls_per_sitemap = 50000
        self.max_sitemap_size = 50 * 1024 * 1024  # 50 MB

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_urls': 0,
            'total_images': 0,
            'sitemaps_created': 0
        }

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def calculate_priority(self, file_path, frontmatter, content):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π priority (0.0 - 1.0)"""
        priority = 0.5  # –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑ frontmatter
        if frontmatter and 'priority' in frontmatter:
            return float(frontmatter['priority'])

        # –ì–ª—É–±–∏–Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–º–µ–Ω—å—à–µ = –≤–∞–∂–Ω–µ–µ)
        depth = len(file_path.relative_to(self.knowledge_dir).parts) - 1
        if depth == 0:
            priority += 0.3
        elif depth == 1:
            priority += 0.2
        elif depth == 2:
            priority += 0.1

        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±–æ–ª—å—à–µ = –≤–∞–∂–Ω–µ–µ)
        if content:
            content_length = len(content)
            if content_length > 5000:
                priority += 0.15
            elif content_length > 2000:
                priority += 0.1
            elif content_length > 1000:
                priority += 0.05

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (–±–æ–ª—å—à–µ = –≤–∞–∂–Ω–µ–µ)
        if content:
            links_count = len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content))
            if links_count > 20:
                priority += 0.1
            elif links_count > 10:
                priority += 0.05

        # –ù–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if content and re.search(r'!\[.*?\]\(.*?\)', content):
            priority += 0.05

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ 1.0
        return min(priority, 1.0)

    def calculate_changefreq(self, file_path, frontmatter):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤ frontmatter
        if frontmatter and 'changefreq' in frontmatter:
            return frontmatter['changefreq']

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é
        mtime = file_path.stat().st_mtime
        last_modified = datetime.fromtimestamp(mtime)
        days_since_modified = (datetime.now() - last_modified).days

        if days_since_modified < 7:
            return 'daily'
        elif days_since_modified < 30:
            return 'weekly'
        elif days_since_modified < 90:
            return 'monthly'
        elif days_since_modified < 365:
            return 'yearly'
        else:
            return 'never'

    def get_lastmod(self, file_path, frontmatter):
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        # –ò–∑ frontmatter
        if frontmatter and 'date' in frontmatter:
            try:
                date = frontmatter['date']
                if isinstance(date, str):
                    return date
                else:
                    return date.strftime('%Y-%m-%d')
            except:
                pass

        # –ò–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        mtime = file_path.stat().st_mtime
        return datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')

    def extract_images(self, content):
        """–ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return []

        images = []

        # Markdown –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: ![alt](url)
        md_images = re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)

        for alt, url in md_images:
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ URL
            if url.startswith('http'):
                continue

            images.append({
                'loc': url,
                'caption': alt if alt else None
            })

        return images

    def collect_urls(self):
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ URLs"""
        urls = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            article_path = str(md_file.relative_to(self.root_dir))

            # URL
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å .md –≤ .html
            html_path = article_path.replace('.md', '.html')
            url = f"{self.base_url}/{html_path}"

            # –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            priority = self.calculate_priority(md_file, frontmatter, content)
            changefreq = self.calculate_changefreq(md_file, frontmatter)
            lastmod = self.get_lastmod(md_file, frontmatter)

            # –ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = self.extract_images(content)

            url_data = {
                'loc': url,
                'lastmod': lastmod,
                'changefreq': changefreq,
                'priority': f'{priority:.2f}',
                'images': images
            }

            urls.append(url_data)
            self.stats['total_images'] += len(images)

        self.stats['total_urls'] = len(urls)
        return urls

    def generate_sitemap_xml(self, urls, sitemap_index=0):
        """–°–æ–∑–¥–∞—Ç—å XML sitemap"""
        xml_lines = []
        xml_lines.append('<?xml version="1.0" encoding="UTF-8"?>\n')

        # –î–æ–±–∞–≤–∏—Ç—å namespace –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if any(url.get('images') for url in urls):
            xml_lines.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"\n')
            xml_lines.append('        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1">\n')
        else:
            xml_lines.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')

        for url_data in urls:
            xml_lines.append('  <url>\n')
            xml_lines.append(f'    <loc>{url_data["loc"]}</loc>\n')
            xml_lines.append(f'    <lastmod>{url_data["lastmod"]}</lastmod>\n')
            xml_lines.append(f'    <changefreq>{url_data["changefreq"]}</changefreq>\n')
            xml_lines.append(f'    <priority>{url_data["priority"]}</priority>\n')

            # –î–æ–±–∞–≤–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            for image in url_data.get('images', []):
                xml_lines.append('    <image:image>\n')
                xml_lines.append(f'      <image:loc>{self.base_url}/{image["loc"]}</image:loc>\n')
                if image.get('caption'):
                    xml_lines.append(f'      <image:caption>{image["caption"]}</image:caption>\n')
                xml_lines.append('    </image:image>\n')

            xml_lines.append('  </url>\n')

        xml_lines.append('</urlset>\n')

        return ''.join(xml_lines)

    def generate_sitemap_index_xml(self, sitemap_files):
        """–°–æ–∑–¥–∞—Ç—å sitemap index"""
        xml_lines = []
        xml_lines.append('<?xml version="1.0" encoding="UTF-8"?>\n')
        xml_lines.append('<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')

        for sitemap_file in sitemap_files:
            xml_lines.append('  <sitemap>\n')
            xml_lines.append(f'    <loc>{self.base_url}/{sitemap_file.name}</loc>\n')
            xml_lines.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>\n')
            xml_lines.append('  </sitemap>\n')

        xml_lines.append('</sitemapindex>\n')

        return ''.join(xml_lines)

    def generate_sitemaps(self, compress=False):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å sitemap(s)"""
        print("üó∫Ô∏è  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap...\n")

        # –°–æ–±—Ä–∞—Ç—å URLs
        all_urls = self.collect_urls()

        if not all_urls:
            print("‚ö†Ô∏è  –ù–µ—Ç URLs –¥–ª—è sitemap")
            return []

        print(f"   –ù–∞–π–¥–µ–Ω–æ URLs: {len(all_urls)}")
        print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.stats['total_images']}\n")

        # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ sitemap –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        sitemap_files = []

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ priority (–≤–∞–∂–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        all_urls.sort(key=lambda x: float(x['priority']), reverse=True)

        for i in range(0, len(all_urls), self.max_urls_per_sitemap):
            urls_chunk = all_urls[i:i + self.max_urls_per_sitemap]

            # –ò–º—è —Ñ–∞–π–ª–∞
            if len(all_urls) > self.max_urls_per_sitemap:
                sitemap_index = i // self.max_urls_per_sitemap + 1
                sitemap_name = f"sitemap{sitemap_index}.xml"
            else:
                sitemap_name = "sitemap.xml"

            sitemap_path = self.root_dir / sitemap_name

            # –°–æ–∑–¥–∞—Ç—å XML
            xml_content = self.generate_sitemap_xml(urls_chunk)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
            if compress:
                gz_path = sitemap_path.with_suffix('.xml.gz')
                with gzip.open(gz_path, 'wt', encoding='utf-8') as f:
                    f.write(xml_content)
                sitemap_files.append(gz_path)
                print(f"‚úÖ Sitemap: {gz_path.name} ({len(urls_chunk)} URLs)")
            else:
                with open(sitemap_path, 'w', encoding='utf-8') as f:
                    f.write(xml_content)
                sitemap_files.append(sitemap_path)
                print(f"‚úÖ Sitemap: {sitemap_name} ({len(urls_chunk)} URLs)")

            self.stats['sitemaps_created'] += 1

        # –°–æ–∑–¥–∞—Ç—å sitemap index –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤
        if len(sitemap_files) > 1:
            index_xml = self.generate_sitemap_index_xml(sitemap_files)
            index_path = self.root_dir / "sitemap_index.xml"

            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_xml)

            print(f"\n‚úÖ Sitemap Index: sitemap_index.xml ({len(sitemap_files)} sitemaps)")

        return sitemap_files

    def generate_robots_txt(self):
        """–°–æ–∑–¥–∞—Ç—å robots.txt"""
        robots_path = self.root_dir / "robots.txt"

        lines = []
        lines.append("# Robots.txt\n")
        lines.append("# Generated by Advanced Sitemap Generator\n\n")
        lines.append("User-agent: *\n")
        lines.append("Allow: /\n\n")

        # –£–∫–∞–∑–∞—Ç—å sitemap
        if self.stats['sitemaps_created'] > 1:
            lines.append(f"Sitemap: {self.base_url}/sitemap_index.xml\n")
        else:
            lines.append(f"Sitemap: {self.base_url}/sitemap.xml\n")

        with open(robots_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Robots.txt: {robots_path}")

    def ping_search_engines(self, sitemap_url):
        """Ping –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º"""
        print("\nüì° Ping –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º...\n")

        search_engines = {
            'Google': f'http://www.google.com/ping?sitemap={urllib.parse.quote(sitemap_url)}',
            'Bing': f'http://www.bing.com/ping?sitemap={urllib.parse.quote(sitemap_url)}'
        }

        for engine, ping_url in search_engines.items():
            try:
                response = urllib.request.urlopen(ping_url, timeout=10)
                if response.status == 200:
                    print(f"   ‚úÖ {engine}: —É—Å–ø–µ—à–Ω–æ")
                else:
                    print(f"   ‚ö†Ô∏è  {engine}: —Å—Ç–∞—Ç—É—Å {response.status}")
            except Exception as e:
                print(f"   ‚ùå {engine}: {e}")

    def print_statistics(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n")
        print(f"   URLs: {self.stats['total_urls']}")
        print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.stats['total_images']}")
        print(f"   Sitemaps: {self.stats['sitemaps_created']}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Sitemap Generator')
    parser.add_argument('--base-url', default='https://example.com',
                       help='Base URL —Å–∞–π—Ç–∞')
    parser.add_argument('--compress', action='store_true',
                       help='–°–∂–∞—Ç—å sitemap (gzip)')
    parser.add_argument('--robots', action='store_true',
                       help='–°–æ–∑–¥–∞—Ç—å robots.txt')
    parser.add_argument('--ping', action='store_true',
                       help='Ping –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    generator = AdvancedSitemapGenerator(root_dir, args.base_url)

    # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å sitemaps
    sitemap_files = generator.generate_sitemaps(compress=args.compress)

    # Robots.txt
    if args.robots:
        generator.generate_robots_txt()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    generator.print_statistics()

    # Ping
    if args.ping and sitemap_files:
        if generator.stats['sitemaps_created'] > 1:
            sitemap_url = f"{args.base_url}/sitemap_index.xml"
        else:
            sitemap_url = f"{args.base_url}/sitemap.xml"

        generator.ping_search_engines(sitemap_url)


if __name__ == "__main__":
    main()
