#!/usr/bin/env python3
"""
Advanced Sitemap Generator - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä sitemap.xml
–§—É–Ω–∫—Ü–∏–∏:
- Dynamic priority calculation (based on importance)
- Change frequency detection
- Multi-sitemap support (50,000 URLs per file)
- Sitemap index generation
- Ping search engines (Google, Bing)
- Image sitemap support
- Validation
- Compression (.gz)
- robots.txt generation
- Statistics and reporting

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Google Search Console, Yoast SEO, XML Sitemap generators
"""

from pathlib import Path
import yaml
import re
from datetime import datetime, timedelta
import gzip
import urllib.request
import urllib.parse
from collections import defaultdict, Counter
import xml.etree.ElementTree as ET
import subprocess
import json
import time


class SEOAnalyzer:
    """
    SEO –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å—Ç–∞—Ç–µ–π
    –ü—Ä–æ–≤–µ—Ä–∫–∞ title, meta description, H1, keyword density, SEO score
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

    def analyze_article(self, file_path, frontmatter, content):
        """
        –ê–Ω–∞–ª–∏–∑ SEO –º–µ—Ç—Ä–∏–∫ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏

        Returns: dict —Å SEO –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ score (0-100)
        """
        seo_score = 0
        issues = []
        recommendations = []

        # 1. Title analysis (20 –±–∞–ª–ª–æ–≤)
        title = frontmatter.get('title', '') if frontmatter else ''
        title_length = len(title)

        if 50 <= title_length <= 60:
            seo_score += 20
        elif 40 <= title_length <= 70:
            seo_score += 15
            recommendations.append(f"Title length {title_length} chars (optimal: 50-60)")
        elif title_length > 0:
            seo_score += 10
            issues.append(f"Title too {'long' if title_length > 70 else 'short'} ({title_length} chars)")
        else:
            issues.append("Missing title")

        # 2. H1 heading analysis (15 –±–∞–ª–ª–æ–≤)
        h1_headings = re.findall(r'^# ([^\n]+)', content, re.MULTILINE)

        if len(h1_headings) == 1:
            seo_score += 15
        elif len(h1_headings) == 0:
            issues.append("No H1 heading found")
        else:
            seo_score += 5
            issues.append(f"Multiple H1 headings ({len(h1_headings)}) - should be 1")

        # 3. Meta description (15 –±–∞–ª–ª–æ–≤)
        description = frontmatter.get('description', '') if frontmatter else ''
        desc_length = len(description)

        if 150 <= desc_length <= 160:
            seo_score += 15
        elif 120 <= desc_length <= 180:
            seo_score += 10
            recommendations.append(f"Description length {desc_length} chars (optimal: 150-160)")
        elif desc_length > 0:
            seo_score += 5
            issues.append(f"Description too {'long' if desc_length > 180 else 'short'} ({desc_length} chars)")
        else:
            issues.append("Missing meta description")

        # 4. Content length (15 –±–∞–ª–ª–æ–≤)
        word_count = len(re.findall(r'\b[–∞-—è—ëa-z]+\b', content.lower()))

        if word_count >= 1000:
            seo_score += 15
        elif word_count >= 500:
            seo_score += 10
        elif word_count >= 300:
            seo_score += 5
        else:
            issues.append(f"Content too short ({word_count} words, recommended: 500+)")

        # 5. Headings structure (10 –±–∞–ª–ª–æ–≤)
        h2_count = len(re.findall(r'^## ', content, re.MULTILINE))
        h3_count = len(re.findall(r'^### ', content, re.MULTILINE))

        if h2_count >= 3 and h3_count >= 2:
            seo_score += 10
        elif h2_count >= 2:
            seo_score += 7
        elif h2_count >= 1:
            seo_score += 4
        else:
            issues.append("Poor heading structure (need H2, H3)")

        # 6. Internal links (10 –±–∞–ª–ª–æ–≤)
        all_links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
        internal_links = [l for l in all_links if not l[1].startswith('http')]

        if len(internal_links) >= 3:
            seo_score += 10
        elif len(internal_links) >= 1:
            seo_score += 5
        else:
            recommendations.append("Add internal links to improve SEO")

        # 7. External links (5 –±–∞–ª–ª–æ–≤)
        external_links = [l for l in all_links if l[1].startswith('http')]

        if len(external_links) >= 2:
            seo_score += 5
        elif len(external_links) >= 1:
            seo_score += 3

        # 8. Images (5 –±–∞–ª–ª–æ–≤)
        images = re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)

        if len(images) >= 1:
            seo_score += 5
            # Check alt text
            images_without_alt = [img for img in images if not img[0]]
            if images_without_alt:
                recommendations.append(f"{len(images_without_alt)} images missing alt text")

        # 9. Keyword density (5 –±–∞–ª–ª–æ–≤)
        if title:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ title –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            title_words = set(re.findall(r'\b[–∞-—è—ëa-z]{4,}\b', title.lower()))
            content_words = re.findall(r'\b[–∞-—è—ëa-z]+\b', content.lower())
            content_word_count = Counter(content_words)

            keywords_mentioned = 0
            for keyword in title_words:
                if keyword in content_word_count:
                    keywords_mentioned += 1

            if keywords_mentioned >= len(title_words) * 0.7:
                seo_score += 5
            elif keywords_mentioned >= len(title_words) * 0.5:
                seo_score += 3
            else:
                recommendations.append("Use more keywords from title in content")

        return {
            'score': min(seo_score, 100),
            'title_length': title_length,
            'description_length': desc_length,
            'word_count': word_count,
            'h1_count': len(h1_headings),
            'h2_count': h2_count,
            'internal_links': len(internal_links),
            'external_links': len(external_links),
            'images': len(images),
            'issues': issues,
            'recommendations': recommendations
        }

    def analyze_all(self):
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        results = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
                if match:
                    fm = yaml.safe_load(match.group(1))
                    article_content = match.group(2)
                else:
                    fm = None
                    article_content = content

                seo_analysis = self.analyze_article(md_file, fm, article_content)
                seo_analysis['file'] = str(md_file.relative_to(self.root_dir))
                seo_analysis['title'] = fm.get('title', md_file.stem) if fm else md_file.stem

                results.append(seo_analysis)
            except:
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ score
        results.sort(key=lambda x: x['score'], reverse=True)

        return results

    def generate_seo_report_html(self, output_file):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML –æ—Ç—á—ë—Ç –ø–æ SEO"""
        results = self.analyze_all()

        if not results:
            return

        avg_score = sum(r['score'] for r in results) / len(results)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
        excellent = [r for r in results if r['score'] >= 80]
        good = [r for r in results if 60 <= r['score'] < 80]
        fair = [r for r in results if 40 <= r['score'] < 60]
        poor = [r for r in results if r['score'] < 40]

        html = f'''<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üìä SEO Analysis Report</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            margin: 0;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }}
        h1 {{ color: #667eea; margin-bottom: 10px; }}
        .summary {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .stat-card {{
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }}
        .stat-card h3 {{ margin: 0; font-size: 0.9em; color: #666; }}
        .stat-card .value {{ font-size: 2em; font-weight: bold; color: #667eea; margin: 10px 0; }}
        .article-card {{
            background: #f8f9fa;
            padding: 20px;
            margin: 15px 0;
            border-radius: 10px;
            border-left: 5px solid #ddd;
        }}
        .score-excellent {{ border-left-color: #4CAF50; }}
        .score-good {{ border-left-color: #2196F3; }}
        .score-fair {{ border-left-color: #FF9800; }}
        .score-poor {{ border-left-color: #f44336; }}
        .score-badge {{
            display: inline-block;
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            color: white;
            font-size: 0.9em;
        }}
        .badge-excellent {{ background: #4CAF50; }}
        .badge-good {{ background: #2196F3; }}
        .badge-fair {{ background: #FF9800; }}
        .badge-poor {{ background: #f44336; }}
        .issues {{ color: #d32f2f; margin-top: 10px; }}
        .recommendations {{ color: #ff9800; margin-top: 10px; }}
        .metrics {{ display: flex; gap: 15px; flex-wrap: wrap; margin-top: 10px; font-size: 0.9em; color: #666; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üìä SEO Analysis Report</h1>
        <p>–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M')}</p>

        <div class="summary">
            <div class="stat-card">
                <h3>–°—Ä–µ–¥–Ω–∏–π SEO Score</h3>
                <div class="value">{avg_score:.1f}/100</div>
            </div>
            <div class="stat-card">
                <h3>–û—Ç–ª–∏—á–Ω–æ (80+)</h3>
                <div class="value">{len(excellent)}</div>
            </div>
            <div class="stat-card">
                <h3>–•–æ—Ä–æ—à–æ (60-79)</h3>
                <div class="value">{len(good)}</div>
            </div>
            <div class="stat-card">
                <h3>–¢—Ä–µ–±—É—é—Ç —É–ª—É—á—à–µ–Ω–∏—è</h3>
                <div class="value">{len(fair) + len(poor)}</div>
            </div>
        </div>

        <h2>üî¥ –¢—Ä–µ–±—É—é—Ç —Å—Ä–æ—á–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è (Score < 40)</h2>
'''

        for article in poor[:10]:
            score_class = 'score-poor'
            badge_class = 'badge-poor'

            html += f'''
        <div class="article-card {score_class}">
            <h3>{article['title']}</h3>
            <span class="{badge_class} score-badge">Score: {article['score']}/100</span>
            <div class="metrics">
                <span>üìù {article['word_count']} words</span>
                <span>üîó {article['internal_links']} internal links</span>
                <span>üñºÔ∏è {article['images']} images</span>
            </div>
'''
            if article['issues']:
                html += '<div class="issues">‚ùå Issues:<ul>'
                for issue in article['issues']:
                    html += f'<li>{issue}</li>'
                html += '</ul></div>'

            if article['recommendations']:
                html += '<div class="recommendations">üí° Recommendations:<ul>'
                for rec in article['recommendations']:
                    html += f'<li>{rec}</li>'
                html += '</ul></div>'

            html += f'<div style="margin-top:10px;"><code>{article["file"]}</code></div></div>'

        html += '''
    </div>
</body>
</html>
'''

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)


class SitemapValidator:
    """
    –í–∞–ª–∏–¥–∞—Ç–æ—Ä sitemap.xml
    –ü—Ä–æ–≤–µ—Ä–∫–∞ XML schema, URL structure, –¥—É–±–ª–∏–∫–∞—Ç—ã, —Ä–∞–∑–º–µ—Ä
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.errors = []
        self.warnings = []

    def validate_file(self, sitemap_file):
        """–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å sitemap —Ñ–∞–π–ª"""
        print(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è: {sitemap_file.name}")

        # 1. Check file exists
        if not sitemap_file.exists():
            self.errors.append(f"File not found: {sitemap_file}")
            return False

        # 2. Check file size (max 50MB uncompressed)
        file_size = sitemap_file.stat().st_size

        if sitemap_file.suffix == '.gz':
            # –î–ª—è gzip —Ñ–∞–π–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
            try:
                with gzip.open(sitemap_file, 'rt', encoding='utf-8') as f:
                    content = f.read()
                    file_size = len(content.encode('utf-8'))
            except Exception as e:
                self.errors.append(f"Failed to decompress: {e}")
                return False
        else:
            with open(sitemap_file, 'r', encoding='utf-8') as f:
                content = f.read()

        max_size = 50 * 1024 * 1024
        if file_size > max_size:
            self.errors.append(f"File too large: {file_size / (1024*1024):.1f}MB (max 50MB)")

        # 3. Parse XML
        try:
            root = ET.fromstring(content)
        except ET.ParseError as e:
            self.errors.append(f"XML parse error: {e}")
            return False

        # 4. Validate namespace
        expected_ns = 'http://www.sitemaps.org/schemas/sitemap/0.9'
        if expected_ns not in root.tag:
            self.errors.append(f"Invalid namespace: {root.tag}")

        # 5. Check URL count (max 50,000)
        urls = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url')
        url_count = len(urls)

        if url_count > 50000:
            self.errors.append(f"Too many URLs: {url_count} (max 50,000)")
        elif url_count == 0:
            self.warnings.append("No URLs found in sitemap")

        print(f"   URLs: {url_count}")

        # 6. Check for duplicates
        locs = [url.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc').text
                for url in urls if url.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc') is not None]

        duplicates = [loc for loc in set(locs) if locs.count(loc) > 1]
        if duplicates:
            self.errors.append(f"Duplicate URLs found: {len(duplicates)}")
            for dup in duplicates[:5]:
                self.warnings.append(f"  Duplicate: {dup}")

        # 7. Validate URL format
        invalid_urls = []
        for loc_text in locs[:100]:  # Check first 100
            if not loc_text.startswith(('http://', 'https://')):
                invalid_urls.append(loc_text)

        if invalid_urls:
            self.errors.append(f"Invalid URL format found: {len(invalid_urls)}")

        # 8. Check priority values
        priorities = [url.find('{http://www.sitemaps.org/schemas/sitemap/0.9}priority')
                     for url in urls]
        for priority in priorities:
            if priority is not None:
                try:
                    val = float(priority.text)
                    if not (0.0 <= val <= 1.0):
                        self.warnings.append(f"Priority out of range: {val}")
                except:
                    self.warnings.append(f"Invalid priority value: {priority.text}")

        return len(self.errors) == 0

    def print_report(self):
        """–í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        print("\nüìã –û—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\n")

        if not self.errors and not self.warnings:
            print("‚úÖ Sitemap –≤–∞–ª–∏–¥–µ–Ω! –û—à–∏–±–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n")
            return

        if self.errors:
            print(f"‚ùå –û—à–∏–±–æ–∫: {len(self.errors)}\n")
            for error in self.errors:
                print(f"   ‚Ä¢ {error}")
            print()

        if self.warnings:
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(self.warnings)}\n")
            for warning in self.warnings[:10]:
                print(f"   ‚Ä¢ {warning}")
            if len(self.warnings) > 10:
                print(f"   ... –∏ –µ—â—ë {len(self.warnings) - 10}")
            print()


class ChangeDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ sitemap
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç git –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è modified/new/deleted —Ñ–∞–π–ª–æ–≤
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

    def get_changed_files_from_git(self, since_days=7):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–∑ git"""
        try:
            since_date = (datetime.now() - timedelta(days=since_days)).strftime('%Y-%m-%d')

            # Modified files
            result = subprocess.run(
                ['git', 'log', f'--since={since_date}', '--name-only',
                 '--pretty=format:', '--', 'knowledge/'],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if not result.stdout.strip():
                return set()

            changed_files = set()
            for line in result.stdout.strip().split('\n'):
                if line and line.endswith('.md'):
                    file_path = self.root_dir / line
                    if file_path.exists():
                        changed_files.add(str(file_path.relative_to(self.root_dir)))

            return changed_files
        except:
            return set()

    def get_intelligent_changefreq(self, file_path):
        """
        –£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ changefreq –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ git

        Returns: changefreq (always/hourly/daily/weekly/monthly/yearly/never)
        """
        try:
            # –ü–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∫–æ–º–º–∏—Ç–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞
            result = subprocess.run(
                ['git', 'log', '--format=%ai', '--', str(file_path)],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if not result.stdout.strip():
                return 'monthly'  # Default

            dates = []
            for line in result.stdout.strip().split('\n'):
                try:
                    date = datetime.strptime(line.split()[0], '%Y-%m-%d')
                    dates.append(date)
                except:
                    continue

            if len(dates) < 2:
                return 'monthly'

            # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω—é—é —á–∞—Å—Ç–æ—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏–π
            dates.sort(reverse=True)
            intervals = []

            for i in range(min(5, len(dates) - 1)):
                delta = (dates[i] - dates[i + 1]).days
                intervals.append(delta)

            avg_interval = sum(intervals) / len(intervals) if intervals else 365

            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å changefreq
            if avg_interval < 1:
                return 'always'
            elif avg_interval < 7:
                return 'daily'
            elif avg_interval < 30:
                return 'weekly'
            elif avg_interval < 90:
                return 'monthly'
            elif avg_interval < 365:
                return 'yearly'
            else:
                return 'never'

        except:
            return 'monthly'

    def get_change_summary(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        changed_30d = self.get_changed_files_from_git(since_days=30)
        changed_7d = self.get_changed_files_from_git(since_days=7)

        return {
            'changed_last_30_days': len(changed_30d),
            'changed_last_7_days': len(changed_7d),
            'files_30d': list(changed_30d),
            'files_7d': list(changed_7d)
        }


class SearchEngineNotifier:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π notifier –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º
    Ping Google, Bing, Yandex, DuckDuckGo —Å retry –∏ rate limiting
    """

    def __init__(self, base_url):
        self.base_url = base_url
        self.ping_history_file = Path(".sitemap_ping_history.json")
        self.ping_history = self.load_history()

    def load_history(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –ø–∏–Ω–≥–æ–≤"""
        if self.ping_history_file.exists():
            try:
                with open(self.ping_history_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_history(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –ø–∏–Ω–≥–æ–≤"""
        with open(self.ping_history_file, 'w') as f:
            json.dump(self.ping_history, f, indent=2)

    def should_ping(self, engine, min_interval_hours=24):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –ø–∏–Ω–≥–æ–≤–∞—Ç—å (rate limiting)"""
        if engine not in self.ping_history:
            return True

        last_ping = self.ping_history[engine].get('last_ping')
        if not last_ping:
            return True

        try:
            last_ping_time = datetime.fromisoformat(last_ping)
            elapsed_hours = (datetime.now() - last_ping_time).total_seconds() / 3600

            return elapsed_hours >= min_interval_hours
        except:
            return True

    def ping_engine(self, engine, ping_url, retries=3):
        """–ü–∏–Ω–≥–æ–≤–∞—Ç—å –æ–¥–∏–Ω –ø–æ–∏—Å–∫–æ–≤–∏–∫ —Å retry logic"""
        for attempt in range(retries):
            try:
                response = urllib.request.urlopen(ping_url, timeout=10)
                status = response.status

                # –ó–∞–ø–∏—Å–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é
                self.ping_history[engine] = {
                    'last_ping': datetime.now().isoformat(),
                    'status': status,
                    'success': status == 200
                }
                self.save_history()

                return status == 200, status
            except Exception as e:
                if attempt == retries - 1:
                    # Last attempt failed
                    self.ping_history[engine] = {
                        'last_ping': datetime.now().isoformat(),
                        'error': str(e),
                        'success': False
                    }
                    self.save_history()
                    return False, str(e)

                # Wait before retry (exponential backoff)
                time.sleep(2 ** attempt)

        return False, "Max retries exceeded"

    def ping_all(self, sitemap_url):
        """–ü–∏–Ω–≥–æ–≤–∞—Ç—å –≤—Å–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã"""
        print("\nüì° –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º...\n")

        engines = {
            'Google': f'http://www.google.com/ping?sitemap={urllib.parse.quote(sitemap_url)}',
            'Bing': f'http://www.bing.com/ping?sitemap={urllib.parse.quote(sitemap_url)}',
            'Yandex': f'http://webmaster.yandex.com/ping?sitemap={urllib.parse.quote(sitemap_url)}',
        }

        for engine, ping_url in engines.items():
            # Check rate limiting
            if not self.should_ping(engine):
                last_ping = self.ping_history[engine]['last_ping']
                print(f"   ‚è≠Ô∏è  {engine}: –ø—Ä–æ–ø—É—â–µ–Ω (–ø–æ—Å–ª–µ–¥–Ω–∏–π ping: {last_ping})")
                continue

            success, result = self.ping_engine(engine, ping_url)

            if success:
                print(f"   ‚úÖ {engine}: —É—Å–ø–µ—à–Ω–æ (status {result})")
            else:
                print(f"   ‚ùå {engine}: –æ—à–∏–±–∫–∞ ({result})")

        print()


class AdvancedSitemapGenerator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä sitemap"""

    def __init__(self, root_dir=".", base_url="https://example.com"):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.base_url = base_url.rstrip('/')

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è sitemap (Google spec)
        self.max_urls_per_sitemap = 50000
        self.max_sitemap_size = 50 * 1024 * 1024  # 50 MB

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_urls': 0,
            'total_images': 0,
            'sitemaps_created': 0
        }

    def extract_frontmatter_and_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def calculate_priority(self, file_path, frontmatter, content):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π priority (0.0 - 1.0)"""
        priority = 0.5  # –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑ frontmatter
        if frontmatter and 'priority' in frontmatter:
            return float(frontmatter['priority'])

        # –ì–ª—É–±–∏–Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–º–µ–Ω—å—à–µ = –≤–∞–∂–Ω–µ–µ)
        depth = len(file_path.relative_to(self.knowledge_dir).parts) - 1
        if depth == 0:
            priority += 0.3
        elif depth == 1:
            priority += 0.2
        elif depth == 2:
            priority += 0.1

        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±–æ–ª—å—à–µ = –≤–∞–∂–Ω–µ–µ)
        if content:
            content_length = len(content)
            if content_length > 5000:
                priority += 0.15
            elif content_length > 2000:
                priority += 0.1
            elif content_length > 1000:
                priority += 0.05

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (–±–æ–ª—å—à–µ = –≤–∞–∂–Ω–µ–µ)
        if content:
            links_count = len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content))
            if links_count > 20:
                priority += 0.1
            elif links_count > 10:
                priority += 0.05

        # –ù–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if content and re.search(r'!\[.*?\]\(.*?\)', content):
            priority += 0.05

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ 1.0
        return min(priority, 1.0)

    def calculate_changefreq(self, file_path, frontmatter):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤ frontmatter
        if frontmatter and 'changefreq' in frontmatter:
            return frontmatter['changefreq']

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é
        mtime = file_path.stat().st_mtime
        last_modified = datetime.fromtimestamp(mtime)
        days_since_modified = (datetime.now() - last_modified).days

        if days_since_modified < 7:
            return 'daily'
        elif days_since_modified < 30:
            return 'weekly'
        elif days_since_modified < 90:
            return 'monthly'
        elif days_since_modified < 365:
            return 'yearly'
        else:
            return 'never'

    def get_lastmod(self, file_path, frontmatter):
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        # –ò–∑ frontmatter
        if frontmatter and 'date' in frontmatter:
            try:
                date = frontmatter['date']
                if isinstance(date, str):
                    return date
                else:
                    return date.strftime('%Y-%m-%d')
            except:
                pass

        # –ò–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        mtime = file_path.stat().st_mtime
        return datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')

    def extract_images(self, content):
        """–ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return []

        images = []

        # Markdown –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: ![alt](url)
        md_images = re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)

        for alt, url in md_images:
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ URL
            if url.startswith('http'):
                continue

            images.append({
                'loc': url,
                'caption': alt if alt else None
            })

        return images

    def collect_urls(self):
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ URLs"""
        urls = []

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            article_path = str(md_file.relative_to(self.root_dir))

            # URL
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å .md –≤ .html
            html_path = article_path.replace('.md', '.html')
            url = f"{self.base_url}/{html_path}"

            # –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            priority = self.calculate_priority(md_file, frontmatter, content)
            changefreq = self.calculate_changefreq(md_file, frontmatter)
            lastmod = self.get_lastmod(md_file, frontmatter)

            # –ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = self.extract_images(content)

            url_data = {
                'loc': url,
                'lastmod': lastmod,
                'changefreq': changefreq,
                'priority': f'{priority:.2f}',
                'images': images
            }

            urls.append(url_data)
            self.stats['total_images'] += len(images)

        self.stats['total_urls'] = len(urls)
        return urls

    def generate_sitemap_xml(self, urls, sitemap_index=0):
        """–°–æ–∑–¥–∞—Ç—å XML sitemap"""
        xml_lines = []
        xml_lines.append('<?xml version="1.0" encoding="UTF-8"?>\n')

        # –î–æ–±–∞–≤–∏—Ç—å namespace –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if any(url.get('images') for url in urls):
            xml_lines.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"\n')
            xml_lines.append('        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1">\n')
        else:
            xml_lines.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')

        for url_data in urls:
            xml_lines.append('  <url>\n')
            xml_lines.append(f'    <loc>{url_data["loc"]}</loc>\n')
            xml_lines.append(f'    <lastmod>{url_data["lastmod"]}</lastmod>\n')
            xml_lines.append(f'    <changefreq>{url_data["changefreq"]}</changefreq>\n')
            xml_lines.append(f'    <priority>{url_data["priority"]}</priority>\n')

            # –î–æ–±–∞–≤–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            for image in url_data.get('images', []):
                xml_lines.append('    <image:image>\n')
                xml_lines.append(f'      <image:loc>{self.base_url}/{image["loc"]}</image:loc>\n')
                if image.get('caption'):
                    xml_lines.append(f'      <image:caption>{image["caption"]}</image:caption>\n')
                xml_lines.append('    </image:image>\n')

            xml_lines.append('  </url>\n')

        xml_lines.append('</urlset>\n')

        return ''.join(xml_lines)

    def generate_sitemap_index_xml(self, sitemap_files):
        """–°–æ–∑–¥–∞—Ç—å sitemap index"""
        xml_lines = []
        xml_lines.append('<?xml version="1.0" encoding="UTF-8"?>\n')
        xml_lines.append('<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')

        for sitemap_file in sitemap_files:
            xml_lines.append('  <sitemap>\n')
            xml_lines.append(f'    <loc>{self.base_url}/{sitemap_file.name}</loc>\n')
            xml_lines.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>\n')
            xml_lines.append('  </sitemap>\n')

        xml_lines.append('</sitemapindex>\n')

        return ''.join(xml_lines)

    def generate_sitemaps(self, compress=False):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å sitemap(s)"""
        print("üó∫Ô∏è  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap...\n")

        # –°–æ–±—Ä–∞—Ç—å URLs
        all_urls = self.collect_urls()

        if not all_urls:
            print("‚ö†Ô∏è  –ù–µ—Ç URLs –¥–ª—è sitemap")
            return []

        print(f"   –ù–∞–π–¥–µ–Ω–æ URLs: {len(all_urls)}")
        print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.stats['total_images']}\n")

        # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ sitemap –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        sitemap_files = []

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ priority (–≤–∞–∂–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        all_urls.sort(key=lambda x: float(x['priority']), reverse=True)

        for i in range(0, len(all_urls), self.max_urls_per_sitemap):
            urls_chunk = all_urls[i:i + self.max_urls_per_sitemap]

            # –ò–º—è —Ñ–∞–π–ª–∞
            if len(all_urls) > self.max_urls_per_sitemap:
                sitemap_index = i // self.max_urls_per_sitemap + 1
                sitemap_name = f"sitemap{sitemap_index}.xml"
            else:
                sitemap_name = "sitemap.xml"

            sitemap_path = self.root_dir / sitemap_name

            # –°–æ–∑–¥–∞—Ç—å XML
            xml_content = self.generate_sitemap_xml(urls_chunk)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
            if compress:
                gz_path = sitemap_path.with_suffix('.xml.gz')
                with gzip.open(gz_path, 'wt', encoding='utf-8') as f:
                    f.write(xml_content)
                sitemap_files.append(gz_path)
                print(f"‚úÖ Sitemap: {gz_path.name} ({len(urls_chunk)} URLs)")
            else:
                with open(sitemap_path, 'w', encoding='utf-8') as f:
                    f.write(xml_content)
                sitemap_files.append(sitemap_path)
                print(f"‚úÖ Sitemap: {sitemap_name} ({len(urls_chunk)} URLs)")

            self.stats['sitemaps_created'] += 1

        # –°–æ–∑–¥–∞—Ç—å sitemap index –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤
        if len(sitemap_files) > 1:
            index_xml = self.generate_sitemap_index_xml(sitemap_files)
            index_path = self.root_dir / "sitemap_index.xml"

            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(index_xml)

            print(f"\n‚úÖ Sitemap Index: sitemap_index.xml ({len(sitemap_files)} sitemaps)")

        return sitemap_files

    def generate_robots_txt(self):
        """–°–æ–∑–¥–∞—Ç—å robots.txt"""
        robots_path = self.root_dir / "robots.txt"

        lines = []
        lines.append("# Robots.txt\n")
        lines.append("# Generated by Advanced Sitemap Generator\n\n")
        lines.append("User-agent: *\n")
        lines.append("Allow: /\n\n")

        # –£–∫–∞–∑–∞—Ç—å sitemap
        if self.stats['sitemaps_created'] > 1:
            lines.append(f"Sitemap: {self.base_url}/sitemap_index.xml\n")
        else:
            lines.append(f"Sitemap: {self.base_url}/sitemap.xml\n")

        with open(robots_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Robots.txt: {robots_path}")

    def ping_search_engines(self, sitemap_url):
        """Ping –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º"""
        print("\nüì° Ping –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º...\n")

        search_engines = {
            'Google': f'http://www.google.com/ping?sitemap={urllib.parse.quote(sitemap_url)}',
            'Bing': f'http://www.bing.com/ping?sitemap={urllib.parse.quote(sitemap_url)}'
        }

        for engine, ping_url in search_engines.items():
            try:
                response = urllib.request.urlopen(ping_url, timeout=10)
                if response.status == 200:
                    print(f"   ‚úÖ {engine}: —É—Å–ø–µ—à–Ω–æ")
                else:
                    print(f"   ‚ö†Ô∏è  {engine}: —Å—Ç–∞—Ç—É—Å {response.status}")
            except Exception as e:
                print(f"   ‚ùå {engine}: {e}")

    def print_statistics(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n")
        print(f"   URLs: {self.stats['total_urls']}")
        print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.stats['total_images']}")
        print(f"   Sitemaps: {self.stats['sitemaps_created']}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Advanced Sitemap Generator - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä sitemap —Å SEO –∞–Ω–∞–ª–∏–∑–æ–º',
        epilog='''–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s --base-url https://example.com            # –ë–∞–∑–æ–≤—ã–π sitemap
  %(prog)s --validate                                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ sitemap.xml
  %(prog)s --seo-check --html seo_report.html        # SEO –∞–Ω–∞–ª–∏–∑ + HTML –æ—Ç—á—ë—Ç
  %(prog)s --compress --robots --notify              # Sitemap + gzip + robots.txt + ping
  %(prog)s --all                                     # –í—Å—ë –≤–º–µ—Å—Ç–µ: sitemap + SEO + –≤–∞–ª–∏–¥–∞—Ü–∏—è
        ''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--base-url', default='https://example.com',
                       help='Base URL —Å–∞–π—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: https://example.com)')
    parser.add_argument('--compress', action='store_true',
                       help='–°–∂–∞—Ç—å sitemap –≤ .gz —Ñ–æ—Ä–º–∞—Ç')
    parser.add_argument('--robots', action='store_true',
                       help='–°–æ–∑–¥–∞—Ç—å robots.txt')
    parser.add_argument('--validate', action='store_true',
                       help='–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π sitemap.xml')
    parser.add_argument('--seo-check', action='store_true',
                       help='–ó–∞–ø—É—Å—Ç–∏—Ç—å SEO –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π')
    parser.add_argument('--html', metavar='FILE',
                       help='–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML –æ—Ç—á—ë—Ç –ø–æ SEO')
    parser.add_argument('--notify', action='store_true',
                       help='–£–≤–µ–¥–æ–º–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã (Google, Bing, Yandex)')
    parser.add_argument('--all', action='store_true',
                       help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å sitemap + SEO –∞–Ω–∞–ª–∏–∑ + –≤–∞–ª–∏–¥–∞—Ü–∏—è + notify')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    # SEO –∞–Ω–∞–ª–∏–∑
    if args.seo_check or args.all:
        print("üìä SEO –ê–Ω–∞–ª–∏–∑...\n")
        seo_analyzer = SEOAnalyzer(root_dir)
        results = seo_analyzer.analyze_all()

        if results:
            avg_score = sum(r['score'] for r in results) / len(results)
            excellent = len([r for r in results if r['score'] >= 80])
            needs_improvement = len([r for r in results if r['score'] < 60])

            print(f"   –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(results)}")
            print(f"   –°—Ä–µ–¥–Ω–∏–π SEO score: {avg_score:.1f}/100")
            print(f"   –û—Ç–ª–∏—á–Ω—ã–π SEO (80+): {excellent}")
            print(f"   –¢—Ä–µ–±—É—é—Ç —É–ª—É—á—à–µ–Ω–∏—è (<60): {needs_improvement}\n")

        # HTML –æ—Ç—á—ë—Ç
        if args.html or args.all:
            html_file = args.html if args.html else root_dir / "SEO_REPORT.html"
            seo_analyzer.generate_seo_report_html(html_file)
            print(f"‚úÖ SEO HTML –æ—Ç—á—ë—Ç: {html_file}\n")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap
    if not args.validate and not (args.seo_check and not args.all):
        generator = AdvancedSitemapGenerator(root_dir, args.base_url)

        # –î–µ—Ç–µ–∫—Ç–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π
        change_detector = ChangeDetector(root_dir)
        change_summary = change_detector.get_change_summary()

        if change_summary['changed_last_7_days'] > 0:
            print(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π: {change_summary['changed_last_7_days']}")
            print(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π: {change_summary['changed_last_30_days']}\n")

        # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å sitemaps
        sitemap_files = generator.generate_sitemaps(compress=args.compress)

        # Robots.txt
        if args.robots or args.all:
            generator.generate_robots_txt()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        generator.print_statistics()

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å URL sitemap
        if generator.stats['sitemaps_created'] > 1:
            sitemap_url = f"{args.base_url}/sitemap_index.xml"
            sitemap_file = root_dir / "sitemap_index.xml"
        else:
            sitemap_url = f"{args.base_url}/sitemap.xml"
            sitemap_file = root_dir / "sitemap.xml"

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if args.validate or args.all:
            if sitemap_file.exists():
                print()
                validator = SitemapValidator(root_dir)
                is_valid = validator.validate_file(sitemap_file)
                validator.print_report()

        # Notification
        if (args.notify or args.all) and sitemap_files:
            notifier = SearchEngineNotifier(args.base_url)
            notifier.ping_all(sitemap_url)

    # –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è
    elif args.validate:
        sitemap_file = root_dir / "sitemap.xml"
        if sitemap_file.exists():
            validator = SitemapValidator(root_dir)
            is_valid = validator.validate_file(sitemap_file)
            validator.print_report()
        else:
            print(f"‚ùå Sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω: {sitemap_file}")


if __name__ == "__main__":
    main()
