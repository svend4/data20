#!/usr/bin/env python3
"""
Popular Articles Tracking - –¢—Ä–µ–∫–∏–Ω–≥ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–µ—Ç—Ä–∏–∫

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Google Analytics, Wikipedia pageviews
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict
import subprocess
import json
import math


class PopularityTracker:
    """–¢—Ä–µ–∫–µ—Ä –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–µ–π"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.articles = {}
        self.popularity_scores = {}

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def count_incoming_links(self, target_path):
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –≤—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏"""
        count = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            _, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            # –°—Å—ã–ª–∫–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)

            for text, link in links:
                if link.startswith('http'):
                    continue

                try:
                    resolved = (md_file.parent / link.split('#')[0]).resolve()
                    if resolved.exists() and resolved.is_relative_to(self.root_dir):
                        resolved_path = str(resolved.relative_to(self.root_dir))
                        if resolved_path == target_path:
                            count += 1
                except:
                    pass

        return count

    def get_edit_count(self, file_path):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏–∑ git"""
        try:
            result = subprocess.run(
                ['git', 'log', '--oneline', '--', str(file_path)],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                return len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
        except:
            pass

        return 0

    def get_recent_activity(self, file_path):
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω—é—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–¥–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è)"""
        try:
            result = subprocess.run(
                ['git', 'log', '-1', '--pretty=format:%ad', '--date=short', '--', str(file_path)],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if result.returncode == 0 and result.stdout:
                from datetime import datetime
                last_edit = datetime.strptime(result.stdout.strip(), '%Y-%m-%d')
                days_ago = (datetime.now() - last_edit).days
                return days_ago
        except:
            pass

        return 999  # –û—á–µ–Ω—å —Å—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è

    def calculate_content_quality(self, content):
        """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return 0.0

        score = 0.0

        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        length = len(content)
        if length > 3000:
            score += 1.0
        elif length > 1000:
            score += 0.5
        elif length > 500:
            score += 0.25

        # –ù–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headings = len(re.findall(r'^#{2,6}\s', content, re.MULTILINE))
        score += min(headings * 0.1, 1.0)

        # –ù–∞–ª–∏—á–∏–µ —Å–ø–∏—Å–∫–æ–≤
        lists = len(re.findall(r'^\s*[-*]\s', content, re.MULTILINE))
        score += min(lists * 0.05, 0.5)

        # –ù–∞–ª–∏—á–∏–µ –∫–æ–¥–∞
        code_blocks = len(re.findall(r'```', content))
        score += min(code_blocks * 0.1, 0.5)

        # –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫
        links = len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content))
        score += min(links * 0.05, 1.0)

        return score

    def analyze_all(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("‚≠ê –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–µ–π...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))

            # –°–æ–±—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            incoming_links = self.count_incoming_links(article_path)
            edit_count = self.get_edit_count(md_file)
            days_since_edit = self.get_recent_activity(md_file)
            content_quality = self.calculate_content_quality(content)

            self.articles[article_path] = {
                'title': frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem,
                'tags': frontmatter.get('tags', []) if frontmatter else [],
                'incoming_links': incoming_links,
                'edit_count': edit_count,
                'days_since_edit': days_since_edit,
                'content_quality': content_quality,
                'length': len(content)
            }

            # –í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–∏–π –±–∞–ª–ª –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            popularity = self.calculate_popularity(
                incoming_links, edit_count, days_since_edit, content_quality
            )

            self.popularity_scores[article_path] = popularity

        print(f"   –°—Ç–∞—Ç–µ–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(self.articles)}\n")

    def calculate_popularity(self, incoming_links, edit_count, days_since_edit, content_quality):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–∏–π –±–∞–ª–ª –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
        link_score = math.log(1 + incoming_links) * 2.0
        edit_score = math.log(1 + edit_count) * 1.5

        # –®—Ç—Ä–∞—Ñ –∑–∞ –¥–∞–≤–Ω–æ—Å—Ç—å (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ)
        recency_score = math.exp(-days_since_edit / 30) * 2.0

        quality_score = content_quality * 1.0

        # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
        total = link_score + edit_score + recency_score + quality_score

        return total

    def get_top_articles(self, limit=10):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø —Å—Ç–∞—Ç–µ–π"""
        sorted_articles = sorted(
            self.popularity_scores.items(),
            key=lambda x: -x[1]
        )

        return sorted_articles[:limit]

    def get_trending_articles(self, limit=10):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ (–Ω–µ–¥–∞–≤–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã–µ)"""
        trending = []

        for article_path, data in self.articles.items():
            if data['days_since_edit'] <= 30:  # –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
                score = data['edit_count'] * (1.0 / (1 + data['days_since_edit']))
                trending.append((article_path, score))

        trending.sort(key=lambda x: -x[1])
        return trending[:limit]

    def get_hidden_gems(self, limit=10):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–æ –º–∞–ª–æ —Å—Å—ã–ª–æ–∫)"""
        gems = []

        for article_path, data in self.articles.items():
            if data['content_quality'] > 2.0 and data['incoming_links'] < 3:
                gems.append((article_path, data['content_quality']))

        gems.sort(key=lambda x: -x[1])
        return gems[:limit]

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# ‚≠ê –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n\n")
        lines.append("> –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–µ—Ç—Ä–∏–∫\n\n")

        # –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
        lines.append("## –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è\n\n")
        lines.append("–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ:\n\n")
        lines.append("- **–í—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏** (–≤–µ—Å: 2.0) ‚Äî —Å–∫–æ–ª—å–∫–æ –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç–µ–π —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ —ç—Ç—É\n")
        lines.append("- **–ò—Å—Ç–æ—Ä–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π** (–≤–µ—Å: 1.5) ‚Äî –∫–∞–∫ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è\n")
        lines.append("- **–°–≤–µ–∂–µ—Å—Ç—å** (–≤–µ—Å: 2.0) ‚Äî –∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å\n")
        lines.append("- **–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞** (–≤–µ—Å: 1.0) ‚Äî –¥–ª–∏–Ω–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —Å—Å—ã–ª–∫–∏\n\n")

        # –¢–æ–ø —Å—Ç–∞—Ç–µ–π
        lines.append("## –¢–æ–ø-10 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö\n\n")

        top_articles = self.get_top_articles(10)

        for i, (article_path, score) in enumerate(top_articles, 1):
            data = self.articles[article_path]

            lines.append(f"### {i}. {data['title']}\n\n")
            lines.append(f"- **–§–∞–π–ª**: [{article_path}]({article_path})\n")
            lines.append(f"- **–ë–∞–ª–ª –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏**: {score:.2f}\n")
            lines.append(f"- **–í—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫**: {data['incoming_links']}\n")
            lines.append(f"- **–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π**: {data['edit_count']}\n")
            lines.append(f"- **–û–±–Ω–æ–≤–ª–µ–Ω–æ**: {data['days_since_edit']} –¥–Ω–µ–π –Ω–∞–∑–∞–¥\n")
            lines.append(f"- **–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞**: {data['content_quality']:.2f}\n")
            lines.append(f"- **–†–∞–∑–º–µ—Ä**: {data['length']} —Å–∏–º–≤–æ–ª–æ–≤\n\n")

        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏
        trending = self.get_trending_articles(10)

        if trending:
            lines.append("\n## üî• –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏\n\n")
            lines.append("> –ê–∫—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è\n\n")

            for i, (article_path, trend_score) in enumerate(trending, 1):
                data = self.articles[article_path]

                lines.append(f"{i}. **{data['title']}**\n")
                lines.append(f"   - [{article_path}]({article_path})\n")
                lines.append(f"   - –û–±–Ω–æ–≤–ª–µ–Ω–æ {data['days_since_edit']} –¥–Ω–µ–π –Ω–∞–∑–∞–¥\n")
                lines.append(f"   - –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {data['edit_count']}\n\n")

        # –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã
        gems = self.get_hidden_gems(10)

        if gems:
            lines.append("\n## üíé –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã\n\n")
            lines.append("> –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Å–ª—É–∂–∏–≤–∞—é—Ç –±–æ–ª—å—à–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n\n")

            for i, (article_path, quality) in enumerate(gems, 1):
                data = self.articles[article_path]

                lines.append(f"{i}. **{data['title']}**\n")
                lines.append(f"   - [{article_path}]({article_path})\n")
                lines.append(f"   - –ö–∞—á–µ—Å—Ç–≤–æ: {quality:.2f}\n")
                lines.append(f"   - –í—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫: {data['incoming_links']}\n\n")

        output_file = self.root_dir / "POPULAR_ARTICLES.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ JSON"""
        data = {
            'articles': {
                article_path: {
                    **article_data,
                    'tags': list(article_data['tags']),
                    'popularity_score': self.popularity_scores[article_path]
                }
                for article_path, article_data in self.articles.items()
            },
            'rankings': {
                'top': [
                    {'article': article, 'score': score}
                    for article, score in self.get_top_articles(20)
                ],
                'trending': [
                    {'article': article, 'score': score}
                    for article, score in self.get_trending_articles(20)
                ],
                'hidden_gems': [
                    {'article': article, 'quality': quality}
                    for article, quality in self.get_hidden_gems(20)
                ]
            }
        }

        output_file = self.root_dir / "popular_articles.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")


def main():
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    tracker = PopularityTracker(root_dir)
    tracker.analyze_all()
    tracker.generate_report()
    tracker.save_json()


if __name__ == "__main__":
    main()
