#!/usr/bin/env python3
"""
Popular Articles Tracking - –¢—Ä–µ–∫–∏–Ω–≥ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–µ—Ç—Ä–∏–∫

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Google Analytics, Wikipedia pageviews
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
import subprocess
import json
import math
import argparse
from typing import Dict, List, Tuple
from datetime import datetime, timedelta


class TrendAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ"""

    def __init__(self, articles: Dict):
        self.articles = articles

    def calculate_growth_rate(self, article_path: str, window_days: int = 30) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ç–µ–º–ø —Ä–æ—Å—Ç–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π)"""
        data = self.articles.get(article_path)
        if not data:
            return 0.0

        edit_count = data.get('edit_count', 0)
        days_since_edit = data.get('days_since_edit', 999)

        if days_since_edit >= window_days:
            return 0.0

        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π / –¥–Ω–µ–π
        recent_edits = edit_count  # –í –∏–¥–µ–∞–ª–µ —Å—á–∏—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –∑–∞ window_days
        growth = recent_edits / max(1, days_since_edit)

        return growth

    def detect_viral_content(self, min_links: int = 5, max_age_days: int = 60) -> List[Tuple[str, float]]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –≤–∏—Ä—É—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–º–Ω–æ–≥–æ —Å—Å—ã–ª–æ–∫ –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è)"""
        viral = []

        for article_path, data in self.articles.items():
            if data['days_since_edit'] <= max_age_days:
                links = data.get('incoming_links', 0)
                if links >= min_links:
                    # Viral coefficient: links / age
                    age = max(1, data['days_since_edit'])
                    viral_score = links / math.sqrt(age)
                    viral.append((article_path, viral_score))

        return sorted(viral, key=lambda x: -x[1])

    def calculate_momentum(self, article_path: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –∏–º–ø—É–ª—å—Å (velocity √ó quality)"""
        data = self.articles.get(article_path)
        if not data:
            return 0.0

        # Velocity: edits / age
        age = max(1, data['days_since_edit'])
        velocity = data.get('edit_count', 0) / age

        # Quality boost
        quality = data.get('content_quality', 0)

        momentum = velocity * (1 + quality)
        return momentum

    def predict_trend_direction(self, article_path: str) -> str:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞: rising, stable, declining"""
        data = self.articles.get(article_path)
        if not data:
            return 'unknown'

        days_since_edit = data['days_since_edit']
        edit_count = data['edit_count']

        # Rising: –Ω–µ–¥–∞–≤–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ –∏ –º–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        if days_since_edit <= 14 and edit_count >= 3:
            return 'rising'

        # Declining: –¥–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–æ—Å—å
        if days_since_edit > 90:
            return 'declining'

        # Stable: —É–º–µ—Ä–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        return 'stable'


class CategoryPopularityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""

    def __init__(self, articles: Dict, popularity_scores: Dict):
        self.articles = articles
        self.popularity_scores = popularity_scores

    def get_popular_by_category(self) -> Dict[str, List[Tuple[str, float]]]:
        """–¢–æ–ø —Å—Ç–∞—Ç–µ–π –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏–∑ —Ç–µ–≥–æ–≤)"""
        category_articles = defaultdict(list)

        for article_path, data in self.articles.items():
            tags = data.get('tags', [])
            score = self.popularity_scores.get(article_path, 0.0)

            for tag in tags:
                category_articles[tag].append((article_path, score))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
        for tag in category_articles:
            category_articles[tag].sort(key=lambda x: -x[1])

        return dict(category_articles)

    def get_category_stats(self) -> Dict[str, Dict]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        stats = defaultdict(lambda: {
            'count': 0,
            'total_score': 0.0,
            'avg_score': 0.0,
            'max_score': 0.0
        })

        for article_path, data in self.articles.items():
            tags = data.get('tags', [])
            score = self.popularity_scores.get(article_path, 0.0)

            for tag in tags:
                stats[tag]['count'] += 1
                stats[tag]['total_score'] += score
                stats[tag]['max_score'] = max(stats[tag]['max_score'], score)

        # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–∏–µ
        for tag in stats:
            if stats[tag]['count'] > 0:
                stats[tag]['avg_score'] = stats[tag]['total_score'] / stats[tag]['count']

        return dict(stats)

    def get_dominant_categories(self, top_n: int = 10) -> List[Tuple[str, float]]:
        """–ù–∞–π—Ç–∏ –¥–æ–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–ø–æ —Å—Ä–µ–¥–Ω–µ–π –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏)"""
        stats = self.get_category_stats()

        dominant = [
            (tag, data['avg_score'])
            for tag, data in stats.items()
            if data['count'] >= 2  # –ú–∏–Ω–∏–º—É–º 2 —Å—Ç–∞—Ç—å–∏
        ]

        return sorted(dominant, key=lambda x: -x[1])[:top_n]


class TimeSeriesPopularityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏"""

    def __init__(self, root_dir: Path):
        self.root_dir = root_dir
        self.knowledge_dir = root_dir / "knowledge"

    def get_edit_timeline(self, file_path: Path, months: int = 6) -> List[Tuple[str, int]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é —à–∫–∞–ª—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π"""
        try:
            cutoff_date = datetime.now() - timedelta(days=months * 30)
            cutoff_str = cutoff_date.strftime('%Y-%m-%d')

            result = subprocess.run(
                ['git', 'log', '--pretty=format:%ad', '--date=short', f'--since={cutoff_str}', '--', str(file_path)],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if result.returncode == 0 and result.stdout:
                dates = result.stdout.strip().split('\n')

                # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –º–µ—Å—è—Ü–∞–º
                monthly_counts = Counter()
                for date_str in dates:
                    if date_str:
                        # YYYY-MM
                        month_key = date_str[:7]
                        monthly_counts[month_key] += 1

                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
                timeline = sorted(monthly_counts.items())
                return timeline

        except:
            pass

        return []

    def detect_activity_spikes(self, file_path: Path) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –≤—Å–ø–ª–µ—Å–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        timeline = self.get_edit_timeline(file_path, months=12)

        if len(timeline) < 3:
            return []

        spikes = []
        counts = [count for _, count in timeline]

        if counts:
            avg = sum(counts) / len(counts)
            threshold = avg * 2  # –í—Å–ø–ª–µ—Å–∫ = 2√ó –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ

            for month, count in timeline:
                if count >= threshold:
                    spikes.append(month)

        return spikes

    def calculate_consistency_score(self, file_path: Path, months: int = 6) -> float:
        """–û—Ü–µ–Ω–∏—Ç—å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π (0.0-1.0)"""
        timeline = self.get_edit_timeline(file_path, months=months)

        if not timeline:
            return 0.0

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—è—Ü–µ–≤ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏
        active_months = len(timeline)

        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å = active_months / total_months
        consistency = active_months / months

        return min(consistency, 1.0)


class EngagementScorer:
    """–û—Ü–µ–Ω–∫–∞ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""

    def __init__(self, articles: Dict):
        self.articles = articles

    def calculate_engagement_score(self, article_path: str) -> float:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏"""
        data = self.articles.get(article_path)
        if not data:
            return 0.0

        score = 0.0

        # 1. –°—Å—ã–ª–∫–∏ (—Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª)
        links = data.get('incoming_links', 0)
        score += links * 3.0

        # 2. –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)
        edits = data.get('edit_count', 0)
        score += math.sqrt(edits) * 2.0

        # 3. –°–≤–µ–∂–µ—Å—Ç—å
        days = data.get('days_since_edit', 999)
        recency_bonus = max(0, (90 - days) / 90) * 2.0
        score += recency_bonus

        # 4. –ö–∞—á–µ—Å—Ç–≤–æ
        quality = data.get('content_quality', 0)
        score += quality * 1.5

        # 5. –†–∞–∑–º–µ—Ä (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ = –±–æ–ª—å—à–µ —É—Å–∏–ª–∏–π)
        length = data.get('length', 0)
        length_score = min(math.log(1 + length / 1000), 2.0)
        score += length_score

        return score

    def get_engagement_distribution(self) -> Dict[str, int]:
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏"""
        distribution = {
            'very_high': 0,   # >15
            'high': 0,        # 10-15
            'medium': 0,      # 5-10
            'low': 0,         # 1-5
            'very_low': 0     # <1
        }

        for article_path in self.articles:
            score = self.calculate_engagement_score(article_path)

            if score > 15:
                distribution['very_high'] += 1
            elif score > 10:
                distribution['high'] += 1
            elif score > 5:
                distribution['medium'] += 1
            elif score > 1:
                distribution['low'] += 1
            else:
                distribution['very_low'] += 1

        return distribution


class PopularityTracker:
    """–¢—Ä–µ–∫–µ—Ä –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–µ–π"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.articles = {}
        self.popularity_scores = {}

        # –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.trend_analyzer = None
        self.category_analyzer = None
        self.timeseries_analyzer = None
        self.engagement_scorer = None

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return yaml.safe_load(match.group(1)), match.group(2)
        except:
            pass
        return None, None

    def count_incoming_links(self, target_path):
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –≤—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏"""
        count = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            _, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            # –°—Å—ã–ª–∫–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)

            for text, link in links:
                if link.startswith('http'):
                    continue

                try:
                    resolved = (md_file.parent / link.split('#')[0]).resolve()
                    if resolved.exists() and resolved.is_relative_to(self.root_dir):
                        resolved_path = str(resolved.relative_to(self.root_dir))
                        if resolved_path == target_path:
                            count += 1
                except:
                    pass

        return count

    def get_edit_count(self, file_path):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –∏–∑ git"""
        try:
            result = subprocess.run(
                ['git', 'log', '--oneline', '--', str(file_path)],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                return len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
        except:
            pass

        return 0

    def get_recent_activity(self, file_path):
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω—é—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–¥–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è)"""
        try:
            result = subprocess.run(
                ['git', 'log', '-1', '--pretty=format:%ad', '--date=short', '--', str(file_path)],
                cwd=self.root_dir,
                capture_output=True,
                text=True
            )

            if result.returncode == 0 and result.stdout:
                from datetime import datetime
                last_edit = datetime.strptime(result.stdout.strip(), '%Y-%m-%d')
                days_ago = (datetime.now() - last_edit).days
                return days_ago
        except:
            pass

        return 999  # –û—á–µ–Ω—å —Å—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è

    def calculate_content_quality(self, content):
        """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return 0.0

        score = 0.0

        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        length = len(content)
        if length > 3000:
            score += 1.0
        elif length > 1000:
            score += 0.5
        elif length > 500:
            score += 0.25

        # –ù–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headings = len(re.findall(r'^#{2,6}\s', content, re.MULTILINE))
        score += min(headings * 0.1, 1.0)

        # –ù–∞–ª–∏—á–∏–µ —Å–ø–∏—Å–∫–æ–≤
        lists = len(re.findall(r'^\s*[-*]\s', content, re.MULTILINE))
        score += min(lists * 0.05, 0.5)

        # –ù–∞–ª–∏—á–∏–µ –∫–æ–¥–∞
        code_blocks = len(re.findall(r'```', content))
        score += min(code_blocks * 0.1, 0.5)

        # –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫
        links = len(re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content))
        score += min(links * 0.05, 1.0)

        return score

    def analyze_all(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("‚≠ê –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–µ–π...\n")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))

            # –°–æ–±—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            incoming_links = self.count_incoming_links(article_path)
            edit_count = self.get_edit_count(md_file)
            days_since_edit = self.get_recent_activity(md_file)
            content_quality = self.calculate_content_quality(content)

            self.articles[article_path] = {
                'title': frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem,
                'tags': frontmatter.get('tags', []) if frontmatter else [],
                'incoming_links': incoming_links,
                'edit_count': edit_count,
                'days_since_edit': days_since_edit,
                'content_quality': content_quality,
                'length': len(content)
            }

            # –í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–∏–π –±–∞–ª–ª –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            popularity = self.calculate_popularity(
                incoming_links, edit_count, days_since_edit, content_quality
            )

            self.popularity_scores[article_path] = popularity

        print(f"   –°—Ç–∞—Ç–µ–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(self.articles)}\n")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        self.trend_analyzer = TrendAnalyzer(self.articles)
        self.category_analyzer = CategoryPopularityAnalyzer(self.articles, self.popularity_scores)
        self.timeseries_analyzer = TimeSeriesPopularityAnalyzer(self.root_dir)
        self.engagement_scorer = EngagementScorer(self.articles)

    def calculate_popularity(self, incoming_links, edit_count, days_since_edit, content_quality):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ–±—â–∏–π –±–∞–ª–ª –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
        link_score = math.log(1 + incoming_links) * 2.0
        edit_score = math.log(1 + edit_count) * 1.5

        # –®—Ç—Ä–∞—Ñ –∑–∞ –¥–∞–≤–Ω–æ—Å—Ç—å (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ)
        recency_score = math.exp(-days_since_edit / 30) * 2.0

        quality_score = content_quality * 1.0

        # –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
        total = link_score + edit_score + recency_score + quality_score

        return total

    def get_top_articles(self, limit=10):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø —Å—Ç–∞—Ç–µ–π"""
        sorted_articles = sorted(
            self.popularity_scores.items(),
            key=lambda x: -x[1]
        )

        return sorted_articles[:limit]

    def get_trending_articles(self, limit=10):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ (–Ω–µ–¥–∞–≤–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã–µ)"""
        trending = []

        for article_path, data in self.articles.items():
            if data['days_since_edit'] <= 30:  # –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
                score = data['edit_count'] * (1.0 / (1 + data['days_since_edit']))
                trending.append((article_path, score))

        trending.sort(key=lambda x: -x[1])
        return trending[:limit]

    def get_hidden_gems(self, limit=10):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–æ –º–∞–ª–æ —Å—Å—ã–ª–æ–∫)"""
        gems = []

        for article_path, data in self.articles.items():
            if data['content_quality'] > 2.0 and data['incoming_links'] < 3:
                gems.append((article_path, data['content_quality']))

        gems.sort(key=lambda x: -x[1])
        return gems[:limit]

    def run_trend_analysis(self):
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        if not self.trend_analyzer:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ analyze_all()")
            return

        print("\nüìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤\n")

        # –í–∏—Ä—É—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        viral = self.trend_analyzer.detect_viral_content(min_links=2, max_age_days=60)
        if viral:
            print(f"üî• –í–∏—Ä—É—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–æ–ø-5):")
            for article_path, viral_score in viral[:5]:
                title = self.articles[article_path]['title']
                print(f"   ‚Ä¢ {title} (viral score: {viral_score:.2f})")

        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤
        print(f"\nüéØ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤:")
        trend_counts = Counter()

        for article_path in self.articles:
            direction = self.trend_analyzer.predict_trend_direction(article_path)
            trend_counts[direction] += 1

        for direction, count in trend_counts.most_common():
            print(f"   {direction}: {count} —Å—Ç–∞—Ç–µ–π")

    def run_category_analysis(self):
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        if not self.category_analyzer:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ analyze_all()")
            return

        print("\nüè∑Ô∏è  –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n")

        # –î–æ–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        dominant = self.category_analyzer.get_dominant_categories(top_n=5)
        if dominant:
            print("–¢–æ–ø-5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø–æ —Å—Ä–µ–¥–Ω–µ–π –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏):")
            for tag, avg_score in dominant:
                print(f"   ‚Ä¢ {tag}: {avg_score:.2f}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        stats = self.category_analyzer.get_category_stats()
        for tag, data in sorted(stats.items(), key=lambda x: -x[1]['count'])[:10]:
            print(f"   ‚Ä¢ {tag}: {data['count']} —Å—Ç–∞—Ç–µ–π, –º–∞–∫—Å –±–∞–ª–ª: {data['max_score']:.2f}")

    def run_engagement_analysis(self):
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏"""
        if not self.engagement_scorer:
            print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ analyze_all()")
            return

        print("\nüí¨ –ê–Ω–∞–ª–∏–∑ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏\n")

        distribution = self.engagement_scorer.get_engagement_distribution()

        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏:")
        print(f"   –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (>15): {distribution['very_high']}")
        print(f"   –í—ã—Å–æ–∫–∞—è (10-15):     {distribution['high']}")
        print(f"   –°—Ä–µ–¥–Ω—è—è (5-10):      {distribution['medium']}")
        print(f"   –ù–∏–∑–∫–∞—è (1-5):        {distribution['low']}")
        print(f"   –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è (<1):   {distribution['very_low']}")

        # –¢–æ–ø –ø–æ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏
        engagement_scores = [
            (article_path, self.engagement_scorer.calculate_engagement_score(article_path))
            for article_path in self.articles
        ]
        engagement_scores.sort(key=lambda x: -x[1])

        print(f"\n–¢–æ–ø-5 –ø–æ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏:")
        for article_path, score in engagement_scores[:5]:
            title = self.articles[article_path]['title']
            print(f"   ‚Ä¢ {title} ({score:.2f})")

    def export_html(self, output_file: str = "popular_articles.html"):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HTML —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        html = []
        html.append("<!DOCTYPE html>")
        html.append("<html lang='ru'>")
        html.append("<head>")
        html.append("  <meta charset='UTF-8'>")
        html.append("  <meta name='viewport' content='width=device-width, initial-scale=1.0'>")
        html.append("  <title>–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏</title>")
        html.append("  <style>")
        html.append("    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 40px; background: #f5f5f5; }")
        html.append("    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }")
        html.append("    h1 { color: #333; border-bottom: 3px solid #FF9800; padding-bottom: 10px; }")
        html.append("    h2 { color: #555; margin-top: 30px; }")
        html.append("    .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }")
        html.append("    .stat-card { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; }")
        html.append("    .stat-value { font-size: 32px; font-weight: bold; }")
        html.append("    .stat-label { font-size: 14px; opacity: 0.9; }")
        html.append("    .article-card { margin: 15px 0; padding: 20px; background: #fafafa; border-radius: 8px; border-left: 4px solid #FF9800; }")
        html.append("    .article-title { font-size: 18px; font-weight: bold; color: #333; margin-bottom: 10px; }")
        html.append("    .article-meta { display: flex; gap: 15px; flex-wrap: wrap; margin-top: 10px; }")
        html.append("    .meta-item { font-size: 14px; color: #666; }")
        html.append("    .badge { display: inline-block; background: #FF9800; color: white; padding: 4px 12px; border-radius: 12px; font-size: 12px; margin-left: 5px; }")
        html.append("    .trend-badge { background: #4CAF50; }")
        html.append("    .gem-badge { background: #9C27B0; }")
        html.append("  </style>")
        html.append("</head>")
        html.append("<body>")
        html.append("  <div class='container'>")
        html.append("    <h1>‚≠ê –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏</h1>")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_articles = len(self.articles)
        avg_score = sum(self.popularity_scores.values()) / len(self.popularity_scores) if self.popularity_scores else 0

        html.append("    <div class='stats'>")
        html.append(f"      <div class='stat-card'><div class='stat-value'>{total_articles}</div><div class='stat-label'>–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π</div></div>")
        html.append(f"      <div class='stat-card'><div class='stat-value'>{avg_score:.1f}</div><div class='stat-label'>–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª</div></div>")

        # –¢–æ–ø —Å—Ç–∞—Ç—å–∏
        html.append("    </div>")
        html.append("    <h2>–¢–æ–ø-20 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö</h2>")

        top_articles = self.get_top_articles(20)

        for i, (article_path, score) in enumerate(top_articles, 1):
            data = self.articles[article_path]

            html.append(f"    <div class='article-card'>")
            html.append(f"      <div class='article-title'>#{i} {data['title']} <span class='badge'>{score:.1f}</span></div>")
            html.append(f"      <div class='article-meta'>")
            html.append(f"        <div class='meta-item'>üìé –°—Å—ã–ª–æ–∫: {data['incoming_links']}</div>")
            html.append(f"        <div class='meta-item'>‚úèÔ∏è –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {data['edit_count']}</div>")
            html.append(f"        <div class='meta-item'>üìÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {data['days_since_edit']} –¥–Ω. –Ω–∞–∑–∞–¥</div>")
            html.append(f"        <div class='meta-item'>‚≠ê –ö–∞—á–µ—Å—Ç–≤–æ: {data['content_quality']:.1f}</div>")
            html.append(f"      </div>")
            html.append(f"    </div>")

        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ
        trending = self.get_trending_articles(10)
        if trending:
            html.append("    <h2>üî• –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏</h2>")
            for article_path, _ in trending:
                data = self.articles[article_path]
                html.append(f"    <div class='article-card'>")
                html.append(f"      <div class='article-title'>{data['title']} <span class='badge trend-badge'>Trending</span></div>")
                html.append(f"      <div class='meta-item'>–û–±–Ω–æ–≤–ª–µ–Ω–æ {data['days_since_edit']} –¥–Ω–µ–π –Ω–∞–∑–∞–¥</div>")
                html.append(f"    </div>")

        # –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã
        gems = self.get_hidden_gems(10)
        if gems:
            html.append("    <h2>üíé –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã</h2>")
            for article_path, quality in gems:
                data = self.articles[article_path]
                html.append(f"    <div class='article-card'>")
                html.append(f"      <div class='article-title'>{data['title']} <span class='badge gem-badge'>Gem</span></div>")
                html.append(f"      <div class='meta-item'>–ö–∞—á–µ—Å—Ç–≤–æ: {quality:.1f}, –°—Å—ã–ª–æ–∫: {data['incoming_links']}</div>")
                html.append(f"    </div>")

        html.append("  </div>")
        html.append("</body>")
        html.append("</html>")

        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(html))

        print(f"\n‚úÖ HTML —ç–∫—Å–ø–æ—Ä—Ç: {output_path}")

    def generate_report(self):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# ‚≠ê –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n\n")
        lines.append("> –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–µ—Ç—Ä–∏–∫\n\n")

        # –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
        lines.append("## –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è\n\n")
        lines.append("–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ:\n\n")
        lines.append("- **–í—Ö–æ–¥—è—â–∏–µ —Å—Å—ã–ª–∫–∏** (–≤–µ—Å: 2.0) ‚Äî —Å–∫–æ–ª—å–∫–æ –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç–µ–π —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ —ç—Ç—É\n")
        lines.append("- **–ò—Å—Ç–æ—Ä–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π** (–≤–µ—Å: 1.5) ‚Äî –∫–∞–∫ —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è\n")
        lines.append("- **–°–≤–µ–∂–µ—Å—Ç—å** (–≤–µ—Å: 2.0) ‚Äî –∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å\n")
        lines.append("- **–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞** (–≤–µ—Å: 1.0) ‚Äî –¥–ª–∏–Ω–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —Å—Å—ã–ª–∫–∏\n\n")

        # –¢–æ–ø —Å—Ç–∞—Ç–µ–π
        lines.append("## –¢–æ–ø-10 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö\n\n")

        top_articles = self.get_top_articles(10)

        for i, (article_path, score) in enumerate(top_articles, 1):
            data = self.articles[article_path]

            lines.append(f"### {i}. {data['title']}\n\n")
            lines.append(f"- **–§–∞–π–ª**: [{article_path}]({article_path})\n")
            lines.append(f"- **–ë–∞–ª–ª –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏**: {score:.2f}\n")
            lines.append(f"- **–í—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫**: {data['incoming_links']}\n")
            lines.append(f"- **–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π**: {data['edit_count']}\n")
            lines.append(f"- **–û–±–Ω–æ–≤–ª–µ–Ω–æ**: {data['days_since_edit']} –¥–Ω–µ–π –Ω–∞–∑–∞–¥\n")
            lines.append(f"- **–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞**: {data['content_quality']:.2f}\n")
            lines.append(f"- **–†–∞–∑–º–µ—Ä**: {data['length']} —Å–∏–º–≤–æ–ª–æ–≤\n\n")

        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏
        trending = self.get_trending_articles(10)

        if trending:
            lines.append("\n## üî• –¢—Ä–µ–Ω–¥–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏\n\n")
            lines.append("> –ê–∫—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è\n\n")

            for i, (article_path, trend_score) in enumerate(trending, 1):
                data = self.articles[article_path]

                lines.append(f"{i}. **{data['title']}**\n")
                lines.append(f"   - [{article_path}]({article_path})\n")
                lines.append(f"   - –û–±–Ω–æ–≤–ª–µ–Ω–æ {data['days_since_edit']} –¥–Ω–µ–π –Ω–∞–∑–∞–¥\n")
                lines.append(f"   - –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π: {data['edit_count']}\n\n")

        # –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã
        gems = self.get_hidden_gems(10)

        if gems:
            lines.append("\n## üíé –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã\n\n")
            lines.append("> –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Å–ª—É–∂–∏–≤–∞—é—Ç –±–æ–ª—å—à–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n\n")

            for i, (article_path, quality) in enumerate(gems, 1):
                data = self.articles[article_path]

                lines.append(f"{i}. **{data['title']}**\n")
                lines.append(f"   - [{article_path}]({article_path})\n")
                lines.append(f"   - –ö–∞—á–µ—Å—Ç–≤–æ: {quality:.2f}\n")
                lines.append(f"   - –í—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫: {data['incoming_links']}\n\n")

        output_file = self.root_dir / "POPULAR_ARTICLES.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –û—Ç—á—ë—Ç: {output_file}")

    def save_json(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ JSON"""
        data = {
            'articles': {
                article_path: {
                    **article_data,
                    'tags': list(article_data['tags']),
                    'popularity_score': self.popularity_scores[article_path]
                }
                for article_path, article_data in self.articles.items()
            },
            'rankings': {
                'top': [
                    {'article': article, 'score': score}
                    for article, score in self.get_top_articles(20)
                ],
                'trending': [
                    {'article': article, 'score': score}
                    for article, score in self.get_trending_articles(20)
                ],
                'hidden_gems': [
                    {'article': article, 'quality': quality}
                    for article, quality in self.get_hidden_gems(20)
                ]
            }
        }

        output_file = self.root_dir / "popular_articles.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='‚≠ê Popular Articles Tracking - –¢—Ä–µ–∫–∏–Ω–≥ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                      # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –æ—Ç—á—ë—Ç—ã
  %(prog)s --trending           # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
  %(prog)s --category           # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
  %(prog)s --engagement         # –ê–Ω–∞–ª–∏–∑ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏
  %(prog)s --html report.html   # –≠–∫—Å–ø–æ—Ä—Ç –≤ HTML
  %(prog)s --all                # –í—Å—ë: –∞–Ω–∞–ª–∏–∑ + –æ—Ç—á—ë—Ç—ã + —ç–∫—Å–ø–æ—Ä—Ç—ã
        """
    )

    parser.add_argument(
        '--trending',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞'
    )

    parser.add_argument(
        '--category',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º/—Ç–µ–≥–∞–º'
    )

    parser.add_argument(
        '--engagement',
        action='store_true',
        help='–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏'
    )

    parser.add_argument(
        '--html',
        metavar='FILE',
        nargs='?',
        const='popular_articles.html',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ HTML (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: popular_articles.html)'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –∏ —ç–∫—Å–ø–æ—Ä—Ç—ã'
    )

    parser.add_argument(
        '--no-report',
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å markdown –æ—Ç—á—ë—Ç'
    )

    parser.add_argument(
        '--no-json',
        action='store_true',
        help='–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å JSON —Ñ–∞–π–ª'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    tracker = PopularityTracker(root_dir)

    # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
    tracker.analyze_all()

    # –†–µ–∂–∏–º --all
    if args.all:
        tracker.run_trend_analysis()
        tracker.run_category_analysis()
        tracker.run_engagement_analysis()
        if not args.no_report:
            tracker.generate_report()
        if not args.no_json:
            tracker.save_json()
        tracker.export_html(args.html or 'popular_articles.html')
        return

    # –û—Ç–¥–µ–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
    if args.trending:
        tracker.run_trend_analysis()

    if args.category:
        tracker.run_category_analysis()

    if args.engagement:
        tracker.run_engagement_analysis()

    # HTML —ç–∫—Å–ø–æ—Ä—Ç
    if args.html:
        tracker.export_html(args.html)

    # –î–µ–π—Å—Ç–≤–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–ª–∞–≥–∏)
    if not any([args.trending, args.category, args.engagement, args.html]):
        if not args.no_report:
            tracker.generate_report()
        if not args.no_json:
            tracker.save_json()


if __name__ == "__main__":
    main()
