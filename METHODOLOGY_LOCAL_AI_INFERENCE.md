# ü§ñ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø: –õ–æ–∫–∞–ª—å–Ω—ã–π AI/ML Inference –¥–ª—è Flutter –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–∞—Ä–∏–∞–Ω—Ç 1: Ollama/LM Studio (–ì–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è)](#–≤–∞—Ä–∏–∞–Ω—Ç-1-ollamlm-studio)
2. [–í–∞—Ä–∏–∞–Ω—Ç 2: Desktop Server DIY ML Stack](#–≤–∞—Ä–∏–∞–Ω—Ç-2-desktop-server-diy-ml-stack)
3. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Flutter](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-flutter)
4. [–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä](#—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ-–∏-–≤—ã–±–æ—Ä)
5. [Troubleshooting](#troubleshooting)

---

# –í–ê–†–ò–ê–ù–¢ 1: Ollama/LM Studio (–ì–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è)

## üéØ –î–ª—è –∫–æ–≥–æ: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –±–µ–∑ –≥–ª—É–±–æ–∫–∏—Ö –∑–Ω–∞–Ω–∏–π ML

**–¶–µ–ª—å:** –ó–∞–ø—É—Å—Ç–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é LLM –º–æ–¥–µ–ª—å –∑–∞ 10 –º–∏–Ω—É—Ç

---

## 1.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

### Windows:

```powershell
# –°–∫–∞—á–∞—Ç—å —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞
https://ollama.ai/download

# –ò–ª–∏ —á–µ—Ä–µ–∑ winget
winget install Ollama.Ollama

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
ollama --version
```

### macOS:

```bash
# Homebrew
brew install ollama

# –ò–ª–∏ —Å–∫–∞—á–∞—Ç—å DMG
# https://ollama.ai/download
```

### Linux:

```bash
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫
curl https://ollama.ai/install.sh | sh

# –ü—Ä–æ–≤–µ—Ä–∫–∞
ollama --version
```

---

## 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π

### –®–∞–≥ 1: –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏

```bash
# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list

# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏:
# - llama2 (7B) - –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å, ~4 –ì–ë
# - llama2:13b - –±–æ–ª—å—à–µ –º–æ–¥–µ–ª—å, ~8 –ì–ë
# - mistral - –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å, ~4 –ì–ë
# - codellama - –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
# - phi - –º–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å, ~2 –ì–ë
```

### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
ollama pull llama2

# –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ª—É—á—à–µ:
ollama pull llama2:13b

# –î–ª—è –∫–æ–¥–∞:
ollama pull codellama
```

### –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```bash
# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
ollama run llama2

# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å –≤ —á–∞—Ç:
# >>> –ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python
# >>> /bye  # –≤—ã—Ö–æ–¥
```

---

## 1.3 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API —Å–µ—Ä–≤–µ—Ä–∞

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞:

```bash
# Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç API –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ
# API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: http://localhost:11434

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã API
curl http://localhost:11434/api/tags
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

```bash
# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (Windows PowerShell)
$env:OLLAMA_HOST = "0.0.0.0:11434"  # –î–æ—Å—Ç—É–ø –∏–∑ —Å–µ—Ç–∏
$env:OLLAMA_MODELS = "D:\Models"     # –ü—É—Ç—å –¥–ª—è –º–æ–¥–µ–ª–µ–π

# Linux/macOS
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MODELS=/path/to/models
```

---

## 1.4 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API

### –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:

```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python",
  "stream": false
}'

# –û—Ç–≤–µ—Ç:
{
  "model": "llama2",
  "created_at": "2024-01-08T...",
  "response": "Python - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π...",
  "done": true
}
```

### Streaming —Ä–µ–∂–∏–º:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö",
  "stream": true
}'

# –û—Ç–≤–µ—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç —á–∞—Å—Ç—è–º–∏ (Server-Sent Events)
data: {"response": "–í"}
data: {"response": " –ø–æ–ª–µ"}
data: {"response": " –±–µ—Ä—ë–∑–∫–∞"}
...
```

### –ß–∞—Ç —Ä–µ–∂–∏–º (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º):

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama2",
  "messages": [
    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω"},
    {"role": "assistant", "content": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π, –ò–≤–∞–Ω!"},
    {"role": "user", "content": "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?"}
  ]
}'

# –û—Ç–≤–µ—Ç: "–¢–µ–±—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω"
```

---

## 1.5 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ LM Studio (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Å GUI)

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```
1. –°–∫–∞—á–∞—Ç—å: https://lmstudio.ai/
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (Windows/Mac/Linux)
3. –ó–∞–ø—É—Å—Ç–∏—Ç—å LM Studio
```

### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ GUI

```
1. –û—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥–∫—É "Discover"
2. –ù–∞–π—Ç–∏ –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "llama-2-7b-chat")
3. –ù–∞–∂–∞—Ç—å "Download"
4. –î–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∫–∏
```

### –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```
1. –í–∫–ª–∞–¥–∫–∞ "Chat"
2. –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞
3. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
   - Temperature: 0.7 (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)
   - Max tokens: 2048 (–¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞)
   - Top P: 0.9
4. –ù–∞—á–∞—Ç—å —á–∞—Ç
```

### –®–∞–≥ 4: –í–∫–ª—é—á–µ–Ω–∏–µ API —Å–µ—Ä–≤–µ—Ä–∞

```
1. –í–∫–ª–∞–¥–∫–∞ "Local Server"
2. –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å
3. –ù–∞–∂–∞—Ç—å "Start Server"
4. API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:1234
```

---

## 1.6 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ –º–æ—â–Ω–æ—Å—Ç–∏:

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | RAM | GPU | –°–∫–æ—Ä–æ—Å—Ç—å |
|--------|--------|-----|-----|----------|
| phi-2 | 2.7B | 4 –ì–ë | –û–ø—Ü. | ‚ö°‚ö°‚ö° –ë—ã—Å—Ç—Ä–æ |
| llama2 | 7B | 8 –ì–ë | –û–ø—Ü. | ‚ö°‚ö° –°—Ä–µ–¥–Ω–µ |
| llama2:13b | 13B | 16 –ì–ë | –ñ–µ–ª–∞—Ç. | ‚ö° –ú–µ–¥–ª–µ–Ω–Ω–æ |
| llama2:70b | 70B | 64 –ì–ë | –¢—Ä–µ–±—É–µ—Ç—Å—è | üêå –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ |

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è:

```bash
# Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
nvidia-smi  # NVIDIA
rocm-smi    # AMD

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU
OLLAMA_GPU=0 ollama run llama2
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

```json
{
  "model": "llama2",
  "prompt": "...",
  "options": {
    "num_ctx": 2048,        // –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–º–µ–Ω—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ)
    "num_gpu": 1,           // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU —Å–ª–æ–µ–≤
    "num_thread": 8,        // CPU –ø–æ—Ç–æ–∫–∏
    "temperature": 0.7,     // –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0-1)
    "top_p": 0.9,          // Nucleus sampling
    "repeat_penalty": 1.1   // –ò–∑–±–µ–≥–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
  }
}
```

---

## 1.7 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "system": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.",
  "prompt": "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤ Python?"
}'
```

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Modelfile):

```dockerfile
# Modelfile
FROM llama2

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM """
–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ Data Science.
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—à—å—Å—è –Ω–∞ Python, pandas, numpy.
–í—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—à—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞.
"""

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
PARAMETER temperature 0.5
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
```

```bash
# –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å
ollama create data-scientist -f Modelfile

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
ollama run data-scientist
```

### Embedding –º–æ–¥–µ–ª–∏ (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è):

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å embedding –º–æ–¥–µ–ª—å
ollama pull nomic-embed-text

# –ü–æ–ª—É—á–∏—Ç—å embedding
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "Python programming language"
}'

# –û—Ç–≤–µ—Ç: –º–∞—Å—Å–∏–≤ —á–∏—Å–µ–ª [0.1, -0.5, 0.3, ...]
# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
```

---

# –í–ê–†–ò–ê–ù–¢ 2: Desktop Server DIY ML Stack

## üéØ –î–ª—è –∫–æ–≥–æ: –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å –æ–ø—ã—Ç–æ–º Python

**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å –≥–∏–±–∫–∏–π ML backend —Å –ª—é–±—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏

---

## 2.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ —Å—Ç–µ–∫–∞

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ XAMPP

#### Windows:
```
1. –°–∫–∞—á–∞—Ç—å: https://www.apachefriends.org/download.html
2. –ó–∞–ø—É—Å—Ç–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫
3. –í—ã–±—Ä–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
   ‚úÖ Apache
   ‚úÖ MySQL
   ‚úÖ PHP
   ‚ùå Perl (–Ω–µ –Ω—É–∂–µ–Ω)
   ‚ùå FileZilla (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
4. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ C:\xampp
5. –ó–∞–ø—É—Å—Ç–∏—Ç—å XAMPP Control Panel
6. Start: Apache, MySQL
```

#### Linux (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ - –Ω–∞—Ç–∏–≤–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞):
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install apache2 mysql-server php libapache2-mod-php

# –í–∫–ª—é—á–∏—Ç—å —Å–µ—Ä–≤–∏—Å—ã
sudo systemctl start apache2
sudo systemctl start mysql
```

### –®–∞–≥ 2: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Python –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
python --version  # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å 3.9+

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
cd C:\xampp\htdocs  # –∏–ª–∏ /var/www/html
mkdir ml-backend
cd ml-backend
python -m venv venv

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è (Windows)
venv\Scripts\activate

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è (Linux/Mac)
source venv/bin/activate

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ pip
pip install --upgrade pip
```

### –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ ML –±–∏–±–ª–∏–æ—Ç–µ–∫

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install flask flask-cors
pip install numpy pandas scikit-learn
pip install torch torchvision torchaudio  # CPU version
# –î–ª—è GPU: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install transformers
pip install sentence-transformers
pip install spacy

# Computer Vision
pip install opencv-python
pip install pillow

# –£—Ç–∏–ª–∏—Ç—ã
pip install python-dotenv
pip install requests
pip install celery redis  # –î–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
```

---

## 2.2 –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ ML API

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:

```
ml-backend/
‚îú‚îÄ‚îÄ venv/                 # –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
‚îú‚îÄ‚îÄ models/               # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ uploads/              # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
‚îú‚îÄ‚îÄ app.py               # –ì–ª–∞–≤–Ω—ã–π Flask —Å–µ—Ä–≤–µ—Ä
‚îú‚îÄ‚îÄ config.py            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ requirements.txt     # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ services/
    ‚îú‚îÄ‚îÄ llm_service.py      # LLM —Å–µ—Ä–≤–∏—Å
    ‚îú‚îÄ‚îÄ vision_service.py   # Computer Vision
    ‚îî‚îÄ‚îÄ audio_service.py    # –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
```

### app.py - –ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª:

```python
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
from pathlib import Path

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Flask
app = Flask(__name__)
CORS(app)  # –†–∞–∑—Ä–µ—à–∏—Ç—å CORS –¥–ª—è Flutter

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPLOAD_FOLDER = Path('uploads')
UPLOAD_FOLDER.mkdir(exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100 –ú–ë

# –ò–º–ø–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–æ–≤
from services.llm_service import LLMService
from services.vision_service import VisionService

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
llm_service = LLMService()
vision_service = VisionService()

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return jsonify({
        'status': 'ok',
        'services': {
            'llm': llm_service.is_ready(),
            'vision': vision_service.is_ready()
        }
    })

@app.route('/api/llm/generate', methods=['POST'])
def generate_text():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        max_length = data.get('max_length', 200)

        logger.info(f"Generating text for prompt: {prompt[:50]}...")

        result = llm_service.generate(prompt, max_length)

        return jsonify({
            'success': True,
            'text': result['text'],
            'model': result['model'],
            'time': result['time']
        })

    except Exception as e:
        logger.error(f"Error in text generation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/vision/classify', methods=['POST'])
def classify_image():
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        if 'image' not in request.files:
            return jsonify({'success': False, 'error': 'No image provided'}), 400

        file = request.files['image']

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        filepath = UPLOAD_FOLDER / file.filename
        file.save(filepath)

        logger.info(f"Classifying image: {file.filename}")

        result = vision_service.classify(filepath)

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        filepath.unlink()

        return jsonify({
            'success': True,
            'predictions': result['predictions'],
            'model': result['model'],
            'time': result['time']
        })

    except Exception as e:
        logger.error(f"Error in image classification: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    logger.info("Starting ML Backend Server...")
    logger.info("Loading models...")

    # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    llm_service.load_model()
    vision_service.load_model()

    logger.info("‚úÖ All models loaded!")
    logger.info("üöÄ Server running on http://localhost:5000")

    app.run(
        host='0.0.0.0',
        port=5000,
        debug=True,
        threaded=True
    )
```

### services/llm_service.py - LLM —Å–µ—Ä–≤–∏—Å:

```python
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
import time
from pathlib import Path

class LLMService:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.model_name = "distilgpt2"  # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞—Ä—Ç–∞
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"Loading LLM model: {self.model_name} on {self.device}")

        # –ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏:
        # - "distilgpt2" - –±—ã—Å—Ç—Ä–∞—è, –º–∞–ª–µ–Ω—å–∫–∞—è (80 –ú–ë)
        # - "gpt2" - —Å—Ä–µ–¥–Ω—è—è (500 –ú–ë)
        # - "EleutherAI/gpt-neo-1.3B" - –±–æ–ª—å—à–∞—è (5 –ì–ë)

        self.pipeline = pipeline(
            "text-generation",
            model=self.model_name,
            device=0 if self.device == "cuda" else -1
        )

        print("‚úÖ LLM model loaded")

    def generate(self, prompt, max_length=200):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if self.pipeline is None:
            raise RuntimeError("Model not loaded")

        start_time = time.time()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        result = self.pipeline(
            prompt,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )

        elapsed_time = time.time() - start_time

        return {
            'text': result[0]['generated_text'],
            'model': self.model_name,
            'time': round(elapsed_time, 2)
        }

    def is_ready(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏"""
        return self.pipeline is not None
```

### services/vision_service.py - Vision —Å–µ—Ä–≤–∏—Å:

```python
from transformers import pipeline
import torch
import time
from PIL import Image

class VisionService:
    def __init__(self):
        self.pipeline = None
        self.model_name = "google/vit-base-patch16-224"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"Loading Vision model: {self.model_name} on {self.device}")

        self.pipeline = pipeline(
            "image-classification",
            model=self.model_name,
            device=0 if self.device == "cuda" else -1
        )

        print("‚úÖ Vision model loaded")

    def classify(self, image_path):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if self.pipeline is None:
            raise RuntimeError("Model not loaded")

        start_time = time.time()

        # –û—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(image_path)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        results = self.pipeline(image, top_k=5)

        elapsed_time = time.time() - start_time

        return {
            'predictions': [
                {
                    'label': r['label'],
                    'score': round(r['score'], 4)
                }
                for r in results
            ],
            'model': self.model_name,
            'time': round(elapsed_time, 2)
        }

    def is_ready(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏"""
        return self.pipeline is not None
```

---

## 2.3 –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π LLM:

```python
# services/llm_service.py

class LLMService:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
        self.model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
        # –ò–ª–∏ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é: "ai-forever/rugpt3large_based_on_gpt2"

    def generate_russian(self, prompt, max_length=200):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"""
        result = self.pipeline(
            prompt,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.8,
            top_p=0.9,
            repetition_penalty=1.2  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–æ–≤
        )

        return result[0]['generated_text']
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ Stable Diffusion:

```python
# services/image_generation_service.py

from diffusers import StableDiffusionPipeline
import torch

class ImageGenerationService:
    def __init__(self):
        self.model_name = "runwayml/stable-diffusion-v1-5"
        self.pipeline = None

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Stable Diffusion"""
        print(f"Loading Stable Diffusion: {self.model_name}")

        self.pipeline = StableDiffusionPipeline.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )

        if torch.cuda.is_available():
            self.pipeline = self.pipeline.to("cuda")

        print("‚úÖ Stable Diffusion loaded")

    def generate_image(self, prompt, negative_prompt="", num_steps=50):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        image = self.pipeline(
            prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_steps,
            guidance_scale=7.5
        ).images[0]

        return image
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—á–µ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:

```python
# services/audio_service.py

from transformers import pipeline
import torch

class AudioService:
    def __init__(self):
        self.model_name = "openai/whisper-base"
        self.pipeline = None

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Whisper"""
        print(f"Loading Whisper: {self.model_name}")

        self.pipeline = pipeline(
            "automatic-speech-recognition",
            model=self.model_name,
            device=0 if torch.cuda.is_available() else -1
        )

        print("‚úÖ Whisper loaded")

    def transcribe(self, audio_path):
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ"""
        result = self.pipeline(
            str(audio_path),
            return_timestamps=True
        )

        return {
            'text': result['text'],
            'chunks': result.get('chunks', [])
        }
```

---

## 2.4 –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö

### MySQL –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏:

```python
# database.py

import mysql.connector
from datetime import datetime
import json

class Database:
    def __init__(self):
        self.conn = mysql.connector.connect(
            host="localhost",
            user="root",
            password="",
            database="ml_backend"
        )
        self.cursor = self.conn.cursor()
        self.create_tables()

    def create_tables(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS requests (
                id INT AUTO_INCREMENT PRIMARY KEY,
                service VARCHAR(50),
                input_data TEXT,
                output_data TEXT,
                processing_time FLOAT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()

    def log_request(self, service, input_data, output_data, processing_time):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        self.cursor.execute("""
            INSERT INTO requests (service, input_data, output_data, processing_time)
            VALUES (%s, %s, %s, %s)
        """, (
            service,
            json.dumps(input_data),
            json.dumps(output_data),
            processing_time
        ))
        self.conn.commit()

    def get_history(self, service=None, limit=100):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏"""
        if service:
            self.cursor.execute("""
                SELECT * FROM requests
                WHERE service = %s
                ORDER BY created_at DESC
                LIMIT %s
            """, (service, limit))
        else:
            self.cursor.execute("""
                SELECT * FROM requests
                ORDER BY created_at DESC
                LIMIT %s
            """, (limit,))

        return self.cursor.fetchall()
```

---

## 2.5 –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å Celery

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Celery:

```python
# celery_app.py

from celery import Celery
import redis

# Celery –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
celery_app = Celery(
    'ml_backend',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)
```

### –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á:

```python
# tasks.py

from celery_app import celery_app
from services.llm_service import LLMService
import time

@celery_app.task(name='tasks.train_model')
def train_model(dataset_path, model_config):
    """–î–æ–ª–≥–∞—è –∑–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    # –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å —á–∞—Å—ã

    # –°–∏–º—É–ª—è—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    for epoch in range(model_config['epochs']):
        time.sleep(10)  # –°–∏–º—É–ª—è—Ü–∏—è —ç–ø–æ—Ö–∏

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        train_model.update_state(
            state='PROGRESS',
            meta={'current': epoch, 'total': model_config['epochs']}
        )

    return {'status': 'completed', 'model_path': '/models/trained_model.pt'}

@celery_app.task(name='tasks.process_large_dataset')
def process_large_dataset(file_path):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    import pandas as pd

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(file_path)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    result = df.groupby('category').agg({
        'value': ['sum', 'mean', 'count']
    })

    return result.to_dict()
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ API:

```python
# app.py

from tasks import train_model, process_large_dataset

@app.route('/api/train', methods=['POST'])
def start_training():
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ"""
    data = request.get_json()

    # –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏
    task = train_model.delay(
        dataset_path=data['dataset'],
        model_config=data['config']
    )

    return jsonify({
        'success': True,
        'task_id': task.id,
        'status_url': f'/api/task/{task.id}/status'
    })

@app.route('/api/task/<task_id>/status', methods=['GET'])
def task_status(task_id):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏"""
    task = celery_app.AsyncResult(task_id)

    if task.state == 'PENDING':
        response = {
            'state': task.state,
            'status': '–ó–∞–¥–∞—á–∞ –≤ –æ—á–µ—Ä–µ–¥–∏...'
        }
    elif task.state == 'PROGRESS':
        response = {
            'state': task.state,
            'current': task.info.get('current', 0),
            'total': task.info.get('total', 1),
            'status': f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {task.info.get('current', 0)}/{task.info.get('total', 1)}"
        }
    elif task.state == 'SUCCESS':
        response = {
            'state': task.state,
            'result': task.result,
            'status': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ!'
        }
    else:
        response = {
            'state': task.state,
            'status': str(task.info)
        }

    return jsonify(response)
```

---

## 2.6 –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### Prometheus –º–µ—Ç—Ä–∏–∫–∏:

```python
# metrics.py

from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
import time

# –ú–µ—Ç—Ä–∏–∫–∏
request_count = Counter(
    'ml_backend_requests_total',
    'Total number of requests',
    ['service', 'status']
)

request_duration = Histogram(
    'ml_backend_request_duration_seconds',
    'Request duration in seconds',
    ['service']
)

# Middleware –¥–ª—è –º–µ—Ç—Ä–∏–∫
@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    request_duration.labels(
        service=request.endpoint
    ).observe(time.time() - request.start_time)

    request_count.labels(
        service=request.endpoint,
        status=response.status_code
    ).inc()

    return response

# Endpoint –¥–ª—è –º–µ—Ç—Ä–∏–∫
@app.route('/metrics')
def metrics():
    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}
```

---

# –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° FLUTTER

## 3.1 Flutter –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama

```dart
// lib/services/ollama_service.dart

import 'package:dio/dio.dart';

class OllamaService {
  final Dio _dio;
  final String baseUrl;

  OllamaService({
    this.baseUrl = 'http://localhost:11434',
  }) : _dio = Dio(BaseOptions(baseUrl: baseUrl));

  /// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
  Future<String> generate({
    required String model,
    required String prompt,
    bool stream = false,
  }) async {
    try {
      final response = await _dio.post(
        '/api/generate',
        data: {
          'model': model,
          'prompt': prompt,
          'stream': stream,
        },
      );

      return response.data['response'];
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: $e');
    }
  }

  /// –ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
  Future<String> chat({
    required String model,
    required List<Map<String, String>> messages,
  }) async {
    try {
      final response = await _dio.post(
        '/api/chat',
        data: {
          'model': model,
          'messages': messages,
        },
      );

      return response.data['message']['content'];
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ —á–∞—Ç–∞: $e');
    }
  }

  /// Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
  Stream<String> generateStream({
    required String model,
    required String prompt,
  }) async* {
    try {
      final response = await _dio.post(
        '/api/generate',
        data: {
          'model': model,
          'prompt': prompt,
          'stream': true,
        },
        options: Options(responseType: ResponseType.stream),
      );

      await for (var chunk in response.data.stream) {
        final text = String.fromCharCodes(chunk);
        final lines = text.split('\n');

        for (var line in lines) {
          if (line.trim().isEmpty) continue;

          try {
            final json = jsonDecode(line);
            if (json['response'] != null) {
              yield json['response'];
            }
          } catch (_) {}
        }
      }
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ streaming: $e');
    }
  }

  /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
  Future<bool> isAvailable() async {
    try {
      await _dio.get('/api/tags');
      return true;
    } catch (e) {
      return false;
    }
  }

  /// –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
  Future<List<String>> listModels() async {
    try {
      final response = await _dio.get('/api/tags');
      final models = response.data['models'] as List;
      return models.map((m) => m['name'] as String).toList();
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: $e');
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Flutter:

```dart
// lib/screens/chat_screen.dart

import 'package:flutter/material.dart';
import '../services/ollama_service.dart';

class ChatScreen extends StatefulWidget {
  @override
  _ChatScreenState createState() => _ChatScreenState();
}

class _ChatScreenState extends State<ChatScreen> {
  final OllamaService _ollama = OllamaService();
  final TextEditingController _controller = TextEditingController();
  final List<Map<String, String>> _messages = [];
  bool _isLoading = false;

  @override
  void initState() {
    super.initState();
    _checkAvailability();
  }

  Future<void> _checkAvailability() async {
    final available = await _ollama.isAvailable();
    if (!available) {
      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä.')),
      );
    }
  }

  Future<void> _sendMessage() async {
    if (_controller.text.trim().isEmpty) return;

    final userMessage = _controller.text;
    _controller.clear();

    setState(() {
      _messages.add({'role': 'user', 'content': userMessage});
      _isLoading = true;
    });

    try {
      // –í–∞—Ä–∏–∞–Ω—Ç 1: –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
      final response = await _ollama.chat(
        model: 'llama2',
        messages: _messages,
      );

      setState(() {
        _messages.add({'role': 'assistant', 'content': response});
        _isLoading = false;
      });

      // –í–∞—Ä–∏–∞–Ω—Ç 2: Streaming (–±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ)
      // String assistantMessage = '';
      // setState(() {
      //   _messages.add({'role': 'assistant', 'content': ''});
      // });
      //
      // await for (var chunk in _ollama.generateStream(
      //   model: 'llama2',
      //   prompt: userMessage,
      // )) {
      //   assistantMessage += chunk;
      //   setState(() {
      //     _messages.last['content'] = assistantMessage;
      //   });
      // }
      //
      // setState(() {
      //   _isLoading = false;
      // });

    } catch (e) {
      setState(() {
        _isLoading = false;
      });

      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('–û—à–∏–±–∫–∞: $e')),
      );
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text('Ollama Chat'),
      ),
      body: Column(
        children: [
          // –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
          Expanded(
            child: ListView.builder(
              padding: EdgeInsets.all(16),
              itemCount: _messages.length,
              itemBuilder: (context, index) {
                final message = _messages[index];
                final isUser = message['role'] == 'user';

                return Align(
                  alignment: isUser ? Alignment.centerRight : Alignment.centerLeft,
                  child: Container(
                    margin: EdgeInsets.only(bottom: 8),
                    padding: EdgeInsets.all(12),
                    decoration: BoxDecoration(
                      color: isUser ? Colors.blue : Colors.grey[300],
                      borderRadius: BorderRadius.circular(12),
                    ),
                    child: Text(
                      message['content']!,
                      style: TextStyle(
                        color: isUser ? Colors.white : Colors.black,
                      ),
                    ),
                  ),
                );
              },
            ),
          ),

          // –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∑–∫–∏
          if (_isLoading)
            Padding(
              padding: EdgeInsets.all(8),
              child: CircularProgressIndicator(),
            ),

          // –ü–æ–ª–µ –≤–≤–æ–¥–∞
          Padding(
            padding: EdgeInsets.all(16),
            child: Row(
              children: [
                Expanded(
                  child: TextField(
                    controller: _controller,
                    decoration: InputDecoration(
                      hintText: '–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...',
                      border: OutlineInputBorder(),
                    ),
                    onSubmitted: (_) => _sendMessage(),
                  ),
                ),
                SizedBox(width: 8),
                IconButton(
                  icon: Icon(Icons.send),
                  onPressed: _sendMessage,
                ),
              ],
            ),
          ),
        ],
      ),
    );
  }
}
```

---

## 3.2 Flutter –∫–ª–∏–µ–Ω—Ç –¥–ª—è Desktop Server

```dart
// lib/services/ml_backend_service.dart

import 'package:dio/dio.dart';
import 'dart:io';

class MLBackendService {
  final Dio _dio;
  final String baseUrl;

  MLBackendService({
    this.baseUrl = 'http://localhost:5000',
  }) : _dio = Dio(BaseOptions(baseUrl: baseUrl));

  /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞
  Future<Map<String, dynamic>> healthCheck() async {
    final response = await _dio.get('/health');
    return response.data;
  }

  /// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM
  Future<String> generateText({
    required String prompt,
    int maxLength = 200,
  }) async {
    try {
      final response = await _dio.post(
        '/api/llm/generate',
        data: {
          'prompt': prompt,
          'max_length': maxLength,
        },
      );

      if (response.data['success']) {
        return response.data['text'];
      } else {
        throw Exception(response.data['error']);
      }
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: $e');
    }
  }

  /// –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
  Future<List<Map<String, dynamic>>> classifyImage(File imageFile) async {
    try {
      // –°–æ–∑–¥–∞–Ω–∏–µ FormData –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
      final formData = FormData.fromMap({
        'image': await MultipartFile.fromFile(
          imageFile.path,
          filename: imageFile.path.split('/').last,
        ),
      });

      final response = await _dio.post(
        '/api/vision/classify',
        data: formData,
      );

      if (response.data['success']) {
        return List<Map<String, dynamic>>.from(response.data['predictions']);
      } else {
        throw Exception(response.data['error']);
      }
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: $e');
    }
  }

  /// –ó–∞–ø—É—Å–∫ –¥–æ–ª–≥–æ–π –∑–∞–¥–∞—á–∏
  Future<String> startTraining({
    required String datasetPath,
    required Map<String, dynamic> config,
  }) async {
    try {
      final response = await _dio.post(
        '/api/train',
        data: {
          'dataset': datasetPath,
          'config': config,
        },
      );

      return response.data['task_id'];
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è: $e');
    }
  }

  /// –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏
  Future<Map<String, dynamic>> checkTaskStatus(String taskId) async {
    try {
      final response = await _dio.get('/api/task/$taskId/status');
      return response.data;
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞: $e');
    }
  }

  /// Polling –∑–∞–¥–∞—á–∏ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
  Stream<Map<String, dynamic>> pollTask(String taskId) async* {
    while (true) {
      await Future.delayed(Duration(seconds: 2));

      final status = await checkTaskStatus(taskId);
      yield status;

      if (status['state'] == 'SUCCESS' || status['state'] == 'FAILURE') {
        break;
      }
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:

```dart
// lib/screens/image_classifier_screen.dart

import 'package:flutter/material.dart';
import 'package:image_picker/image_picker.dart';
import 'dart:io';
import '../services/ml_backend_service.dart';

class ImageClassifierScreen extends StatefulWidget {
  @override
  _ImageClassifierScreenState createState() => _ImageClassifierScreenState();
}

class _ImageClassifierScreenState extends State<ImageClassifierScreen> {
  final MLBackendService _mlService = MLBackendService();
  final ImagePicker _picker = ImagePicker();

  File? _image;
  List<Map<String, dynamic>>? _predictions;
  bool _isLoading = false;

  Future<void> _pickImage() async {
    final XFile? pickedFile = await _picker.pickImage(
      source: ImageSource.gallery,
    );

    if (pickedFile != null) {
      setState(() {
        _image = File(pickedFile.path);
        _predictions = null;
      });

      await _classifyImage();
    }
  }

  Future<void> _classifyImage() async {
    if (_image == null) return;

    setState(() {
      _isLoading = true;
    });

    try {
      final predictions = await _mlService.classifyImage(_image!);

      setState(() {
        _predictions = predictions;
        _isLoading = false;
      });
    } catch (e) {
      setState(() {
        _isLoading = false;
      });

      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('–û—à–∏–±–∫–∞: $e')),
      );
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text('Image Classifier'),
      ),
      body: SingleChildScrollView(
        padding: EdgeInsets.all(16),
        child: Column(
          crossAxisAlignment: CrossAxisAlignment.stretch,
          children: [
            // –ö–Ω–æ–ø–∫–∞ –≤—ã–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            ElevatedButton.icon(
              icon: Icon(Icons.image),
              label: Text('–í—ã–±—Ä–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'),
              onPressed: _pickImage,
            ),

            SizedBox(height: 16),

            // –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if (_image != null)
              Container(
                height: 300,
                decoration: BoxDecoration(
                  border: Border.all(color: Colors.grey),
                  borderRadius: BorderRadius.circular(8),
                ),
                child: ClipRRect(
                  borderRadius: BorderRadius.circular(8),
                  child: Image.file(
                    _image!,
                    fit: BoxFit.contain,
                  ),
                ),
              ),

            SizedBox(height: 16),

            // –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∑–∫–∏
            if (_isLoading)
              Center(child: CircularProgressIndicator()),

            // –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if (_predictions != null && !_isLoading)
              Card(
                child: Padding(
                  padding: EdgeInsets.all(16),
                  child: Column(
                    crossAxisAlignment: CrossAxisAlignment.start,
                    children: [
                      Text(
                        '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:',
                        style: Theme.of(context).textTheme.headline6,
                      ),
                      SizedBox(height: 8),
                      ..._predictions!.map((prediction) {
                        return ListTile(
                          title: Text(prediction['label']),
                          trailing: Text(
                            '${(prediction['score'] * 100).toStringAsFixed(1)}%',
                            style: TextStyle(fontWeight: FontWeight.bold),
                          ),
                          leading: CircularProgressIndicator(
                            value: prediction['score'],
                            backgroundColor: Colors.grey[300],
                          ),
                        );
                      }).toList(),
                    ],
                  ),
                ),
              ),
          ],
        ),
      ),
    );
  }
}
```

---

## 3.3 –£–º–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ backend

```dart
// lib/services/smart_backend_router.dart

import 'ollama_service.dart';
import 'ml_backend_service.dart';

enum BackendType {
  ollama,      // –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö LLM –∑–∞–¥–∞—á
  mlBackend,   // –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö ML –∑–∞–¥–∞—á
}

class SmartBackendRouter {
  final OllamaService _ollama = OllamaService();
  final MLBackendService _mlBackend = MLBackendService();

  /// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä backend
  Future<BackendType> selectBackend({
    required String taskType,
    int? dataSize,
  }) async {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
    final ollamaAvailable = await _ollama.isAvailable();
    final mlBackendAvailable = await _checkMLBackend();

    // –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞
    switch (taskType) {
      case 'text_generation':
      case 'chat':
        // –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama (–±—ã—Å—Ç—Ä–µ–µ)
        return ollamaAvailable ? BackendType.ollama : BackendType.mlBackend;

      case 'image_classification':
      case 'object_detection':
        // Computer Vision - —Ç–æ–ª—å–∫–æ ML Backend
        return BackendType.mlBackend;

      case 'complex_nlp':
        // –°–ª–æ–∂–Ω—ã–π NLP - ML Backend (–±–æ–ª—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—å)
        return BackendType.mlBackend;

      default:
        // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Ollama
        return ollamaAvailable ? BackendType.ollama : BackendType.mlBackend;
    }
  }

  Future<bool> _checkMLBackend() async {
    try {
      await _mlBackend.healthCheck();
      return true;
    } catch (e) {
      return false;
    }
  }

  /// –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
  Future<String> generateText(String prompt) async {
    final backend = await selectBackend(taskType: 'text_generation');

    switch (backend) {
      case BackendType.ollama:
        return await _ollama.generate(
          model: 'llama2',
          prompt: prompt,
        );

      case BackendType.mlBackend:
        return await _mlBackend.generateText(prompt: prompt);
    }
  }
}
```

---

–≠—Ç–æ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥–∞
- Troubleshooting –∏ FAQ
- Best practices
- –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤?

---

# –°–†–ê–í–ù–ï–ù–ò–ï –ò –í–´–ë–û–†

## 4.1 –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **OLLAMA** –µ—Å–ª–∏:

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| **–û–ø—ã—Ç** | –ù–µ—Ç –æ–ø—ã—Ç–∞ —Å Python/ML |
| **–ó–∞–¥–∞—á–∏** | –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ (—á–∞—Ç, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è) |
| **–í—Ä–µ–º—è** | –ù—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∑–∞ 10 –º–∏–Ω—É—Ç |
| **–ú–æ–¥–µ–ª–∏** | –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Llama, Mistral, etc.) |
| **–ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è** | –ù–µ –Ω—É–∂–Ω–∞ –≥–ª—É–±–æ–∫–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è |
| **–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ** | –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ (–∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è) |
| **GUI** | –ù—É–∂–µ–Ω –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å |

**–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ–≤:**
- –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
- –ß–∞—Ç-–±–æ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü–æ–º–æ—â–Ω–∏–∫ –ø–æ –∫–æ–¥—É

---

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **DESKTOP SERVER** –µ—Å–ª–∏:

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| **–û–ø—ã—Ç** | –ó–Ω–∞–Ω–∏–µ Python, ML, API —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ |
| **–ó–∞–¥–∞—á–∏** | –ú–Ω–æ–∂–µ—Å—Ç–≤–æ ML –∑–∞–¥–∞—á (LLM + CV + Audio + Custom) |
| **–í—Ä–µ–º—è** | –ú–æ–∂–Ω–æ –ø–æ—Ç—Ä–∞—Ç–∏—Ç—å 1-2 –¥–Ω—è –Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫—É |
| **–ú–æ–¥–µ–ª–∏** | –ù—É–∂–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ fine-tuning |
| **–ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è** | –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ pipeline |
| **–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ** | –ì–æ—Ç–æ–≤—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å |
| **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** | –°–ª–æ–∂–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ |

**–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ–≤:**
- –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + –∞—É–¥–∏–æ)
- –°–∏—Å—Ç–µ–º–∞ —Å fine-tuned –º–æ–¥–µ–ª—è–º–∏
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è data science
- –ö–∞—Å—Ç–æ–º–Ω—ã–µ ML pipeline

---

## 4.2 –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

| –§—É–Ω–∫—Ü–∏—è | Ollama | Desktop Server |
|---------|--------|---------------|
| **–¢–µ–∫—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ |
| **–ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º** | ‚úÖ –î–∞ | ‚úÖ –î–∞ |
| **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ |
| **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (Stable Diffusion) |
| **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (Whisper) |
| **Fine-tuning –º–æ–¥–µ–ª–µ–π** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ |
| **–ö–∞—Å—Ç–æ–º–Ω—ã–µ pipeline** | ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ | ‚úÖ –ü–æ–ª–Ω–∞—è —Å–≤–æ–±–æ–¥–∞ |
| **–§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (Celery) |
| **–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (MySQL/PostgreSQL) |
| **–ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** | ‚ö†Ô∏è –ë–∞–∑–æ–≤—ã–π | ‚úÖ –ü–æ–ª–Ω—ã–π (Prometheus) |
| **API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** | ‚úÖ OpenAI-like | ‚úÖ –ö–∞—Å—Ç–æ–º–Ω—ã–π |
| **–£—Å—Ç–∞–Ω–æ–≤–∫–∞** | ‚ö° 5 –º–∏–Ω—É—Ç | üêå 1-2 —á–∞—Å–∞ |
| **–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ** | üü¢ –ü—Ä–æ—Å—Ç–æ–µ | üî¥ –°–ª–æ–∂–Ω–æ–µ |

---

## 4.3 –°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç-–±–æ—Ç

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –õ–æ–∫–∞–ª—å–Ω—ã–π ChatGPT
- –ß–∞—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
- –ü—Ä–æ—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

–†–ï–®–ï–ù–ò–ï: OLLAMA ‚úÖ
–í—Ä–µ–º—è: 10 –º–∏–Ω—É—Ç
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üü¢ –õ–µ–≥–∫–æ
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π + —á–∞—Ç

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —Ñ–æ—Ç–æ
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ß–∞—Ç –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º

–†–ï–®–ï–ù–ò–ï: DESKTOP SERVER ‚úÖ
–í—Ä–µ–º—è: 2 –¥–Ω—è
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üî¥ –°–ª–æ–∂–Ω–æ
–ü—Ä–∏—á–∏–Ω–∞: –ù—É–∂–µ–Ω CV + LLM
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- –†–∞–±–æ—Ç–∞ –æ—Ñ–ª–∞–π–Ω

–†–ï–®–ï–ù–ò–ï: OLLAMA ‚úÖ
–í—Ä–µ–º—è: 15 –º–∏–Ω—É—Ç
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üü¢ –õ–µ–≥–∫–æ
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 4: Data Science –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –û–±—Ä–∞–±–æ—Ç–∫–∞ CSV/Excel
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- ML –∞–Ω–∞–ª–∏–∑
- Fine-tuned –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞

–†–ï–®–ï–ù–ò–ï: DESKTOP SERVER ‚úÖ
–í—Ä–µ–º—è: 1 –Ω–µ–¥–µ–ª—è
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üî¥ –û—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ
–ü—Ä–∏—á–∏–Ω–∞: –ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π ML —Å—Ç–µ–∫
```

---

# TROUBLESHOOTING –ò FAQ

## üîß –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞ 1: Ollama –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

**–°–∏–º–ø—Ç–æ–º—ã:**
```bash
Error: Failed to connect to Ollama
Connection refused at http://localhost:11434
```

**–†–µ—à–µ–Ω–∏—è:**

**Windows:**
```powershell
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–∏—Å
Get-Process ollama

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama
Stop-Process -Name ollama -Force
ollama serve

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ä—Ç
netstat -ano | findstr :11434
```

**Linux/macOS:**
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å
ps aux | grep ollama

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
pkill ollama
ollama serve

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ä—Ç
lsof -i :11434
netstat -tuln | grep 11434
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:**
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–º –ø–æ—Ä—Ç—É
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

---

### –ü—Ä–æ–±–ª–µ–º–∞ 2: –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ

**–°–∏–º–ø—Ç–æ–º—ã:**
- –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –∑–∞–Ω–∏–º–∞–µ—Ç 30+ —Å–µ–∫—É–Ω–¥
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∞–µ—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏

**–ü—Ä–∏—á–∏–Ω–∞:** –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ

**–†–µ—à–µ–Ω–∏–µ 1: –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ**

```python
# desktop_server.py

def warmup_model():
    """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    print("üî• Warming up model...")
    try:
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
        _ = pipeline("Hello", max_length=5)
        print("‚úÖ Model ready!")
    except Exception as e:
        print(f"‚ùå Warmup failed: {e}")

# –í—ã–∑–≤–∞—Ç—å –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Flask
if __name__ == '__main__':
    warmup_model()
    app.run(host='0.0.0.0', port=5000)
```

**–†–µ—à–µ–Ω–∏–µ 2: Keep-alive –¥–ª—è Ollama**

```bash
# –î–µ—Ä–∂–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ
ollama run llama2
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º –æ–∫–Ω–µ - –∑–∞–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –±—ã—Å—Ç—Ä—ã–º–∏
```

**–†–µ—à–µ–Ω–∏–µ 3: –ú–µ–Ω—å—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏**

```bash
# –í–º–µ—Å—Ç–æ llama2:13b (8 –ì–ë, –º–µ–¥–ª–µ–Ω–Ω–æ)
ollama pull phi  # 2 –ì–ë, –±—ã—Å—Ç—Ä–æ

# –î–ª—è production - –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å
# –î–ª—è dev - –º–∞–ª–µ–Ω—å–∫–∞—è
```

---

### –ü—Ä–æ–±–ª–µ–º–∞ 3: Out of Memory (OOM)

**–°–∏–º–ø—Ç–æ–º—ã:**
```
RuntimeError: CUDA out of memory
Killed (OOM)
```

**–ü—Ä–∏—á–∏–Ω–∞:** –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ RAM/VRAM

**–†–µ—à–µ–Ω–∏–µ 1: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏**

```python
# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ 8-bit
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8-bit –≤–º–µ—Å—Ç–æ 32-bit (4x —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
)

model = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    quantization_config=quantization_config,
    device_map="auto"
)

# –î–ª—è –µ—â–µ –±–æ–ª—å—à–µ–π —ç–∫–æ–Ω–æ–º–∏–∏ - 4-bit
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4-bit (8x —ç–∫–æ–Ω–æ–º–∏—è)
    bnb_4bit_compute_dtype=torch.float16
)
```

**–†–µ—à–µ–Ω–∏–µ 2: –ú–µ–Ω—å—à–∞—è –º–æ–¥–µ–ª—å**

```bash
# –í–º–µ—Å—Ç–æ llama2:13b (—Ç—Ä–µ–±—É–µ—Ç 16 –ì–ë RAM)
ollama pull llama2:7b   # —Ç—Ä–µ–±—É–µ—Ç 8 –ì–ë RAM
ollama pull phi         # —Ç—Ä–µ–±—É–µ—Ç 4 –ì–ë RAM
```

**–†–µ—à–µ–Ω–∏–µ 3: CPU inference**

```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –≤–º–µ—Å—Ç–æ GPU (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
model = AutoModelForCausalLM.from_pretrained("gpt2")
model = model.to('cpu')  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ CPU
```

**–†–µ—à–µ–Ω–∏–µ 4: Batch size = 1**

```python
# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
for item in data:
    result = model.generate(item, max_length=100)
    # –ù–ï model.generate(data, ...) - –≤–µ—Å—å batch —Å—Ä–∞–∑—É
```

---

### –ü—Ä–æ–±–ª–µ–º–∞ 4: Flutter –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Desktop Server

**–°–∏–º–ø—Ç–æ–º—ã:**
```dart
SocketException: Connection refused
DioError: Failed to connect
```

**–ü—Ä–∏—á–∏–Ω–∞:** Firewall, –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π IP, –∏–ª–∏ CORS

**–†–µ—à–µ–Ω–∏–µ 1: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API**

```bash
# –ù–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ —Å —Å–µ—Ä–≤–µ—Ä–æ–º
curl http://localhost:5000/generate -X POST \
  -H "Content-Type: application/json" \
  -d '{"prompt":"test"}'

# –ï—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–æ –Ω–µ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞ - –ø—Ä–æ–±–ª–µ–º–∞ –≤ —Å–µ—Ç–∏
```

**–†–µ—à–µ–Ω–∏–µ 2: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π IP –∞–¥—Ä–µ—Å**

```dart
// ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ)
final apiUrl = 'http://localhost:5000';

// ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏)
final apiUrl = 'http://192.168.1.100:5000';  // IP –∫–æ–º–ø—å—é—Ç–µ—Ä–∞

// –£–∑–Ω–∞—Ç—å IP –∫–æ–º–ø—å—é—Ç–µ—Ä–∞:
// Windows: ipconfig
// Linux/macOS: ifconfig | grep inet
```

**–†–µ—à–µ–Ω–∏–µ 3: CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏**

```python
# desktop_server.py

from flask import Flask
from flask_cors import CORS

app = Flask(__name__)

# –†–∞–∑—Ä–µ—à–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã —Å –ª—é–±—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
CORS(app, resources={
    r"/*": {
        "origins": "*",  # –í production —É–∫–∞–∂–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type"]
    }
})
```

**–†–µ—à–µ–Ω–∏–µ 4: Firewall**

```bash
# Windows: –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª–æ –≤ Windows Defender Firewall
# Settings ‚Üí Windows Security ‚Üí Firewall ‚Üí Allow an app

# Linux: –û—Ç–∫—Ä—ã—Ç—å –ø–æ—Ä—Ç –≤ ufw
sudo ufw allow 5000/tcp
sudo ufw reload

# macOS: System Preferences ‚Üí Security & Privacy ‚Üí Firewall
# –†–∞–∑—Ä–µ—à–∏—Ç—å –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è Python/Flask
```

**–†–µ—à–µ–Ω–∏–µ 5: –ü—Ä–∏–≤—è–∑–∫–∞ –∫ 0.0.0.0**

```python
# –°–ª—É—à–∞—Ç—å –Ω–∞ –≤—Å–µ—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö, –Ω–µ —Ç–æ–ª—å–∫–æ localhost
app.run(host='0.0.0.0', port=5000)  # ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ

# –ù–ï:
app.run(host='localhost', port=5000)  # ‚ùå –¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ
app.run(host='127.0.0.1', port=5000)  # ‚ùå –¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ
```

---

### –ü—Ä–æ–±–ª–µ–º–∞ 5: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

**–°–∏–º–ø—Ç–æ–º—ã:**
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 100 —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–Ω–∏–º–∞–µ—Ç 30+ —Å–µ–∫—É–Ω–¥
- Flutter –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Ç–æ—Ä–º–æ–∑–∏—Ç

**–†–µ—à–µ–Ω–∏–µ 1: –£–º–µ–Ω—å—à–∏—Ç—å max_length**

```python
# ‚ùå –î–æ–ª–≥–æ (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–æ 1000 —Ç–æ–∫–µ–Ω–æ–≤)
result = pipeline(prompt, max_length=1000)

# ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–æ 100 —Ç–æ–∫–µ–Ω–æ–≤)
result = pipeline(prompt, max_length=100)
```

```dart
// Flutter
final response = await dio.post('/generate', data: {
  'prompt': prompt,
  'max_length': 50,  // –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã = –±—ã—Å—Ç—Ä–µ–µ
});
```

**–†–µ—à–µ–Ω–∏–µ 2: Streaming –æ—Ç–≤–µ—Ç—ã**

```python
# desktop_server.py

from flask import Response, stream_with_context
import json

@app.route('/generate_stream', methods=['POST'])
def generate_stream():
    """Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è - —Ç–æ–∫–µ–Ω—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    prompt = request.json['prompt']

    def generate():
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º
        for token in model.generate_streaming(prompt):
            yield f"data: {json.dumps({'token': token})}\n\n"

    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream'
    )
```

```dart
// Flutter - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
Stream<String> generateStream(String prompt) async* {
  final response = await dio.post(
    '/generate_stream',
    data: {'prompt': prompt},
    options: Options(responseType: ResponseType.stream),
  );

  await for (var chunk in response.data.stream) {
    final text = utf8.decode(chunk);
    yield text;
  }
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
generateStream('–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Flutter').listen((token) {
  setState(() {
    fullText += token;  // –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ –º–µ—Ä–µ –ø–æ–ª—É—á–µ–Ω–∏—è
  });
});
```

**–†–µ—à–µ–Ω–∏–µ 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU**

```python
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ GPU
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device: {torch.cuda.get_device_name(0)}")

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ GPU
model = AutoModelForCausalLM.from_pretrained("gpt2")
model = model.to('cuda')  # –ù–∞ GPU

# CPU vs GPU —Å–∫–æ—Ä–æ—Å—Ç—å:
# CPU (Intel i7): ~2-5 tokens/sec
# GPU (RTX 3060): ~30-50 tokens/sec
# GPU (RTX 4090): ~100-150 tokens/sec
```

**–†–µ—à–µ–Ω–∏–µ 4: –ú–µ–Ω—å—à–∞—è –º–æ–¥–µ–ª—å**

```bash
# –°–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏:

# phi (1.3B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) - ~50 tokens/sec –Ω–∞ CPU
ollama pull phi

# llama2:7b (7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) - ~10 tokens/sec –Ω–∞ CPU
ollama pull llama2:7b

# llama2:13b (13B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) - ~5 tokens/sec –Ω–∞ CPU
ollama pull llama2:13b

# –î–ª—è production —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏:
# GPU + –º–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å > CPU + –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å
```

---

### –ü—Ä–æ–±–ª–µ–º–∞ 6: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è)

**–°–∏–º–ø—Ç–æ–º—ã:**
```
"ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ" –≤–º–µ—Å—Ç–æ "–ü—Ä–∏–≤–µ—Ç"
UnicodeDecodeError
```

**–†–µ—à–µ–Ω–∏–µ:**

```python
# desktop_server.py

from flask import Flask, request, jsonify

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Unicode

@app.route('/generate', methods=['POST'])
def generate():
    prompt = request.json['prompt']
    result = pipeline(prompt)

    return jsonify({
        'result': result
    }), 200, {'Content-Type': 'application/json; charset=utf-8'}
```

```dart
// Flutter

final response = await dio.post(
  '/generate',
  data: {'prompt': '–ü—Ä–∏–≤–µ—Ç'},
  options: Options(
    responseType: ResponseType.json,
    headers: {
      'Content-Type': 'application/json; charset=utf-8',
    },
  ),
);
```

---

## ‚ùì FAQ (–ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã)

### Q1: –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama –≤ production?

**A:** –î–∞, –Ω–æ —Å –æ–≥–æ–≤–æ—Ä–∫–∞–º–∏:

‚úÖ **–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è:**
- –ü—Ä–æ—Ç–æ—Ç–∏–ø—ã –∏ MVP
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏
- –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
- –î–µ–º–æ –∏ proof-of-concept

‚ö†Ô∏è **–ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è:**
- High-load —Å–∏—Å—Ç–µ–º—ã (>1000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω)
- Mission-critical –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å SLA —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Desktop Server –∏–ª–∏ –æ–±–ª–∞—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è production.

---

### Q2: –°–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ RAM –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ AI?

**–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**

| –ú–æ–¥–µ–ª—å | RAM | VRAM (GPU) | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è |
|--------|-----|------------|------------|
| phi (1.3B) | 4 –ì–ë | 2 –ì–ë | –ú–∞–ª–µ–Ω—å–∫–∞—è, –±—ã—Å—Ç—Ä–∞—è |
| llama2:7b | 8 –ì–ë | 6 –ì–ë | –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è |
| llama2:13b | 16 –ì–ë | 12 –ì–ë | –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è |
| llama2:70b | 64 –ì–ë | 48 –ì–ë | –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è |
| GPT-J-6B | 12 –ì–ë | 8 –ì–ë | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- **–î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:** 8-16 –ì–ë RAM
- **–î–ª—è production:** 16-32 –ì–ë RAM + GPU
- **–î–ª—è enterprise:** 64+ –ì–ë RAM + –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU

---

### Q3: –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —ç—Ç–æ –æ—Ñ–ª–∞–π–Ω?

**A:** –î–∞!

‚úÖ **–ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ–ª–∞–π–Ω:**
- Ollama - —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
- Desktop Server - —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
- Flutter app - –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—É

üì∂ **–ò–Ω—Ç–µ—Ä–Ω–µ—Ç –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è:**
- –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
- –£—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
- –û–±–Ω–æ–≤–ª–µ–Ω–∏–π

**–°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ—Ñ–ª–∞–π–Ω:**
```
1. –ó–∞–≥—Ä—É–∑–∫–∞ (—Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç):
   ollama pull llama2  # –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å

2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–æ—Ñ–ª–∞–π–Ω):
   ollama run llama2   # –†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
   Flutter app ‚Üí Desktop Server ‚Üí Ollama ‚úÖ
```

---

### Q4: –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–ª–∞–∫–æ + –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –≤–º–µ—Å—Ç–µ?

**A:** –î–∞! –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –ª—É—á—à–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

```dart
// Flutter - —É–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è

class HybridAIService {
  Future<String> generate(String prompt, {bool useCloud = false}) async {
    // –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ ‚Üí –ª–æ–∫–∞–ª—å–Ω–æ (–±—ã—Å—Ç—Ä–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    if (!useCloud && prompt.length < 500) {
      return _generateLocal(prompt);
    }

    // –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ ‚Üí –æ–±–ª–∞–∫–æ (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, –Ω–æ –ø–ª–∞—Ç–Ω–æ)
    return _generateCloud(prompt);
  }

  Future<String> _generateLocal(String prompt) async {
    // Ollama –∏–ª–∏ Desktop Server
    final response = await dio.post('http://localhost:11434/api/generate');
    return response.data['response'];
  }

  Future<String> _generateCloud(String prompt) async {
    // OpenAI, Anthropic, etc.
    final response = await dio.post('https://api.openai.com/v1/chat/completions');
    return response.data['choices'][0]['message']['content'];
  }
}
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –õ–æ–∫–∞–ª—å–Ω–æ - –±—ã—Å—Ç—Ä–æ –∏ –±–µ—Å–ø–ª–∞—Ç–Ω–æ
- ‚úÖ –û–±–ª–∞–∫–æ - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
- ‚úÖ –û—Ñ–ª–∞–π–Ω fallback
- ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –Ω–∞ API costs

---

### Q5: –ö–∞–∫ –∑–∞—â–∏—Ç–∏—Ç—å API —Å–µ—Ä–≤–µ—Ä?

**–†–µ—à–µ–Ω–∏–µ: JWT –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è**

```python
# desktop_server.py

from flask import Flask, request, jsonify
import jwt
from functools import wraps

app = Flask(__name__)
SECRET_KEY = 'your-secret-key-here'  # –•—Ä–∞–Ω–∏—Ç—å –≤ .env

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = request.headers.get('Authorization')

        if not token:
            return jsonify({'error': 'Token missing'}), 401

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞
            data = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        except:
            return jsonify({'error': 'Invalid token'}), 401

        return f(*args, **kwargs)
    return decorated

@app.route('/generate', methods=['POST'])
@token_required  # ‚úÖ –ó–∞—â–∏—â–µ–Ω–æ
def generate():
    prompt = request.json['prompt']
    result = pipeline(prompt)
    return jsonify({'result': result})

@app.route('/login', methods=['POST'])
def login():
    """–í—ã–¥–∞—Ç—å —Ç–æ–∫–µ–Ω –ø–æ—Å–ª–µ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    username = request.json['username']
    password = request.json['password']

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å credentials (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
    if username == 'admin' and password == 'secret':
        token = jwt.encode({'user': username}, SECRET_KEY, algorithm="HS256")
        return jsonify({'token': token})

    return jsonify({'error': 'Invalid credentials'}), 401
```

```dart
// Flutter

class SecureAIService {
  String? _token;

  Future<void> login(String username, String password) async {
    final response = await dio.post('/login', data: {
      'username': username,
      'password': password,
    });

    _token = response.data['token'];
  }

  Future<String> generate(String prompt) async {
    if (_token == null) {
      throw Exception('Not authenticated');
    }

    final response = await dio.post(
      '/generate',
      data: {'prompt': prompt},
      options: Options(headers: {
        'Authorization': _token,  // ‚úÖ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω
      }),
    );

    return response.data['result'];
  }
}
```

---

### Q6: –°–∫–æ–ª—å–∫–æ —ç—Ç–æ —Å—Ç–æ–∏—Ç?

**–õ–æ–∫–∞–ª—å–Ω—ã–π AI (Ollama / Desktop Server):**

üí∞ **–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω–æ (–º–æ–¥–µ–ª–∏ open-source)
- ‚úÖ –ù–µ—Ç API costs
- ‚úÖ –ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

üíª **–¢—Ä–µ–±—É–µ—Ç—Å—è:**
- –ö–æ–º–ø—å—é—Ç–µ—Ä/—Å–µ—Ä–≤–µ—Ä (–æ–¥–∏–Ω —Ä–∞–∑)
- –≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ (~50-100W –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ)

**–û–±–ª–∞—á–Ω—ã–π AI (OpenAI, Anthropic):**

üí∞ **–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- OpenAI GPT-3.5: $0.002 –∑–∞ 1K —Ç–æ–∫–µ–Ω–æ–≤
- OpenAI GPT-4: $0.03 –∑–∞ 1K —Ç–æ–∫–µ–Ω–æ–≤
- Claude: $0.015 –∑–∞ 1K —Ç–æ–∫–µ–Ω–æ–≤

üìä **–ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞:**
```
1000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó 10 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å √ó 500 —Ç–æ–∫–µ–Ω–æ–≤ = 5M —Ç–æ–∫–µ–Ω–æ–≤/–¥–µ–Ω—å

OpenAI GPT-3.5: 5M √ó $0.002/1K = $10/–¥–µ–Ω—å = $300/–º–µ—Å—è—Ü
OpenAI GPT-4: 5M √ó $0.03/1K = $150/–¥–µ–Ω—å = $4500/–º–µ—Å—è—Ü

–õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä: $0/–º–µ—Å—è—Ü + —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è (~$1000-2000 –æ–¥–∏–Ω —Ä–∞–∑)
```

**–í—ã–≤–æ–¥:** –õ–æ–∫–∞–ª—å–Ω—ã–π AI –æ–∫—É–ø–∞–µ—Ç—Å—è –ø—Ä–∏ >100 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–ª–∏ >10000 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å

---


# BEST PRACTICES

## üèÜ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ AI

### 1. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞–º–∏

#### ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
# desktop_server.py

from functools import lru_cache
import hashlib

# In-memory –∫—ç—à –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
@lru_cache(maxsize=100)
def generate_cached(prompt_hash: str, prompt: str) -> str:
    """–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    result = pipeline(prompt, max_length=100)
    return result[0]['generated_text']

@app.route('/generate', methods=['POST'])
def generate():
    prompt = request.json['prompt']

    # –°–æ–∑–¥–∞—Ç—å —Ö—ç—à –ø—Ä–æ–º–ø—Ç–∞
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
    result = generate_cached(prompt_hash, prompt)

    return jsonify({'result': result})
```

**Redis –∫—ç—à –¥–ª—è production:**

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

@app.route('/generate', methods=['POST'])
def generate():
    prompt = request.json['prompt']

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
    cached = redis_client.get(f"ai:{prompt}")
    if cached:
        return jsonify({'result': json.loads(cached), 'cached': True})

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    result = pipeline(prompt)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à (–Ω–∞ 1 —á–∞—Å)
    redis_client.setex(f"ai:{prompt}", 3600, json.dumps(result))

    return jsonify({'result': result, 'cached': False})
```

---

#### ‚úÖ –õ–∏–º–∏—Ç—ã –∏ rate limiting

```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

app = Flask(__name__)

# Rate limiter
limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    default_limits=["100 per hour"]  # 100 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —á–∞—Å —Å –æ–¥–Ω–æ–≥–æ IP
)

@app.route('/generate', methods=['POST'])
@limiter.limit("10 per minute")  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ - 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
def generate():
    prompt = request.json['prompt']

    # –õ–∏–º–∏—Ç –Ω–∞ –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
    if len(prompt) > 1000:
        return jsonify({'error': 'Prompt too long (max 1000 chars)'}), 400

    result = pipeline(prompt, max_length=200)  # –õ–∏–º–∏—Ç –Ω–∞ output
    return jsonify({'result': result})
```

---

#### ‚úÖ Graceful shutdown

```python
import signal
import sys

def signal_handler(sig, frame):
    """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
    print('\nüõë Shutting down gracefully...')

    # –í—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏
    global pipeline
    del pipeline

    # –û—á–∏—Å—Ç–∏—Ç—å GPU –ø–∞–º—è—Ç—å
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print('‚úÖ Cleanup complete')
    sys.exit(0)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ Ctrl+C
signal.signal(signal.SIGINT, signal_handler)

if __name__ == '__main__':
    print('üöÄ Starting server... (Press Ctrl+C to stop)')
    app.run(host='0.0.0.0', port=5000)
```

---

### 2. –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

#### ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è input

```python
import re

def validate_prompt(prompt: str) -> tuple[bool, str]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞"""

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
    if len(prompt) < 3:
        return False, "Prompt too short (min 3 chars)"

    if len(prompt) > 2000:
        return False, "Prompt too long (max 2000 chars)"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–Ω—ä–µ–∫—Ü–∏–∏
    dangerous_patterns = [
        r'<script',  # XSS
        r'DROP\s+TABLE',  # SQL injection
        r'system\(',  # Command injection
        r'eval\(',  # Code injection
    ]

    for pattern in dangerous_patterns:
        if re.search(pattern, prompt, re.IGNORECASE):
            return False, f"Dangerous pattern detected: {pattern}"

    return True, "OK"

@app.route('/generate', methods=['POST'])
def generate():
    prompt = request.json['prompt']

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    valid, message = validate_prompt(prompt)
    if not valid:
        return jsonify({'error': message}), 400

    result = pipeline(prompt)
    return jsonify({'result': result})
```

---

#### ‚úÖ HTTPS –¥–ª—è production

```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è self-signed —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
openssl req -x509 -newkey rsa:4096 \
  -keyout key.pem -out cert.pem \
  -days 365 -nodes
```

```python
# desktop_server.py

if __name__ == '__main__':
    # Development
    if os.getenv('ENVIRONMENT') == 'development':
        app.run(host='0.0.0.0', port=5000)

    # Production - —Å HTTPS
    else:
        app.run(
            host='0.0.0.0',
            port=5000,
            ssl_context=('cert.pem', 'key.pem')  # HTTPS
        )
```

```dart
// Flutter - –æ–±—Ä–∞–±–æ—Ç–∫–∞ HTTPS

final dio = Dio();

// –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å self-signed —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–º
(dio.httpClientAdapter as DefaultHttpClientAdapter).onHttpClientCreate = (client) {
  client.badCertificateCallback = (cert, host, port) {
    return true;  // –ü—Ä–∏–Ω–∏–º–∞—Ç—å self-signed —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã (—Ç–æ–ª—å–∫–æ –¥–ª—è dev!)
  };
  return client;
};
```

---

#### ‚úÖ Environment variables

```python
# .env —Ñ–∞–π–ª (–ù–ï –∫–æ–º–º–∏—Ç–∏—Ç—å –≤ Git!)
SECRET_KEY=your-secret-key-here
DATABASE_URL=postgresql://user:pass@localhost/db
OLLAMA_URL=http://localhost:11434
MAX_WORKERS=4
```

```python
# desktop_server.py

from dotenv import load_dotenv
import os

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env
load_dotenv()

SECRET_KEY = os.getenv('SECRET_KEY')
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434')
MAX_WORKERS = int(os.getenv('MAX_WORKERS', 4))

app = Flask(__name__)
app.config['SECRET_KEY'] = SECRET_KEY
```

---

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

#### ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import logging
from logging.handlers import RotatingFileHandler
import json
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
handler = RotatingFileHandler('ai_server.log', maxBytes=10000000, backupCount=5)
handler.setLevel(logging.INFO)

formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
handler.setFormatter(formatter)

logger = logging.getLogger(__name__)
logger.addHandler(handler)
logger.setLevel(logging.INFO)

@app.route('/generate', methods=['POST'])
def generate():
    start_time = datetime.now()
    prompt = request.json['prompt']

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
    logger.info(json.dumps({
        'event': 'generation_started',
        'prompt_length': len(prompt),
        'ip': request.remote_addr,
        'timestamp': start_time.isoformat()
    }))

    try:
        result = pipeline(prompt)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—Ö–∞
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(json.dumps({
            'event': 'generation_completed',
            'duration': duration,
            'result_length': len(result),
            'timestamp': datetime.now().isoformat()
        }))

        return jsonify({'result': result})

    except Exception as e:
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
        logger.error(json.dumps({
            'event': 'generation_failed',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }))

        return jsonify({'error': str(e)}), 500
```

---

#### ‚úÖ Prometheus –º–µ—Ç—Ä–∏–∫–∏

```python
from prometheus_client import Counter, Histogram, generate_latest

# –ú–µ—Ç—Ä–∏–∫–∏
request_count = Counter('ai_requests_total', 'Total AI requests')
request_duration = Histogram('ai_request_duration_seconds', 'AI request duration')
error_count = Counter('ai_errors_total', 'Total AI errors')

@app.route('/generate', methods=['POST'])
def generate():
    request_count.inc()  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤

    with request_duration.time():  # –ò–∑–º–µ—Ä–∏—Ç—å –≤—Ä–µ–º—è
        try:
            result = pipeline(prompt)
            return jsonify({'result': result})
        except Exception as e:
            error_count.inc()  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
            raise

@app.route('/metrics')
def metrics():
    """Endpoint –¥–ª—è Prometheus"""
    return generate_latest()
```

---

### 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

#### ‚úÖ –ë–∞—Ç—á–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–æ–≤

```python
from collections import deque
import threading
import time

# –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤
request_queue = deque()
result_dict = {}

def batch_processor():
    """–§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –±–∞—Ç—á–∞–º–∏"""
    while True:
        if len(request_queue) >= 4 or (len(request_queue) > 0 and time.time() % 1 < 0.1):
            # –°–æ–±—Ä–∞—Ç—å batch
            batch = []
            request_ids = []

            for _ in range(min(4, len(request_queue))):
                req_id, prompt = request_queue.popleft()
                batch.append(prompt)
                request_ids.append(req_id)

            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å batch (–±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø–æ –æ–¥–Ω–æ–º—É)
            results = pipeline(batch, max_length=100)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for req_id, result in zip(request_ids, results):
                result_dict[req_id] = result

        time.sleep(0.1)

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å
threading.Thread(target=batch_processor, daemon=True).start()

@app.route('/generate', methods=['POST'])
def generate():
    prompt = request.json['prompt']

    # –î–æ–±–∞–≤–∏—Ç—å –≤ –æ—á–µ—Ä–µ–¥—å
    request_id = str(uuid.uuid4())
    request_queue.append((request_id, prompt))

    # –ñ–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    timeout = 30
    start = time.time()
    while request_id not in result_dict:
        if time.time() - start > timeout:
            return jsonify({'error': 'Timeout'}), 408
        time.sleep(0.1)

    # –í–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = result_dict.pop(request_id)
    return jsonify({'result': result})
```

---

#### ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
from celery import Celery

# Celery –¥–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
celery_app = Celery('ai_server', broker='redis://localhost:6379/0')

@celery_app.task
def generate_async(prompt: str) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"""
    result = pipeline(prompt, max_length=200)
    return result[0]['generated_text']

@app.route('/generate_async', methods=['POST'])
def generate_async_endpoint():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ —Ñ–æ–Ω–µ"""
    prompt = request.json['prompt']

    # –°–æ–∑–¥–∞—Ç—å —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
    task = generate_async.delay(prompt)

    return jsonify({
        'task_id': task.id,
        'status': 'processing'
    }), 202

@app.route('/result/<task_id>', methods=['GET'])
def get_result(task_id):
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏"""
    task = generate_async.AsyncResult(task_id)

    if task.ready():
        return jsonify({
            'status': 'completed',
            'result': task.result
        })
    else:
        return jsonify({
            'status': 'processing'
        }), 202
```

---

### 5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

#### ‚úÖ Unit —Ç–µ—Å—Ç—ã

```python
# test_ai_server.py

import pytest
from desktop_server import app

@pytest.fixture
def client():
    app.config['TESTING'] = True
    with app.test_client() as client:
        yield client

def test_generate_success(client):
    """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    response = client.post('/generate', json={
        'prompt': 'Hello, world!'
    })

    assert response.status_code == 200
    assert 'result' in response.json

def test_generate_empty_prompt(client):
    """–¢–µ—Å—Ç —Å –ø—É—Å—Ç—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"""
    response = client.post('/generate', json={
        'prompt': ''
    })

    assert response.status_code == 400

def test_generate_long_prompt(client):
    """–¢–µ—Å—Ç —Å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"""
    response = client.post('/generate', json={
        'prompt': 'x' * 10000
    })

    assert response.status_code == 400

def test_rate_limiting(client):
    """–¢–µ—Å—Ç rate limiting"""
    # –û—Ç–ø—Ä–∞–≤–∏—Ç—å 20 –∑–∞–ø—Ä–æ—Å–æ–≤
    for _ in range(20):
        response = client.post('/generate', json={
            'prompt': 'test'
        })

    # 21-–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω
    response = client.post('/generate', json={
        'prompt': 'test'
    })

    assert response.status_code == 429  # Too Many Requests
```

---

#### ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã

```python
# test_integration.py

import requests
import pytest

BASE_URL = 'http://localhost:5000'

def test_full_workflow():
    """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ workflow"""

    # 1. –õ–æ–≥–∏–Ω
    response = requests.post(f'{BASE_URL}/login', json={
        'username': 'test',
        'password': 'test'
    })
    assert response.status_code == 200
    token = response.json()['token']

    # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ç–æ–∫–µ–Ω–æ–º
    response = requests.post(
        f'{BASE_URL}/generate',
        json={'prompt': 'Test prompt'},
        headers={'Authorization': token}
    )
    assert response.status_code == 200
    assert 'result' in response.json()

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    response = requests.post(
        f'{BASE_URL}/generate',
        json={'prompt': 'Test prompt'},
        headers={'Authorization': token}
    )
    assert response.status_code == 200
    assert response.json()['cached'] == True

def test_flutter_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Flutter"""

    # –°–∏–º—É–ª—è—Ü–∏—è Flutter –∑–∞–ø—Ä–æ—Å–∞
    response = requests.post(
        f'{BASE_URL}/generate',
        json={
            'prompt': 'Explain quantum computing',
            'max_length': 100
        },
        headers={
            'User-Agent': 'Flutter/3.0',
            'Content-Type': 'application/json'
        }
    )

    assert response.status_code == 200
    result = response.json()['result']
    assert len(result) > 0
```

---

### 6. Deployment Best Practices

#### ‚úÖ Docker –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏

```dockerfile
# Dockerfile –¥–ª—è Desktop Server

FROM python:3.11-slim

WORKDIR /app

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞
COPY desktop_server.py .

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
RUN python -c "from transformers import pipeline; pipeline('text-generation', model='gpt2')"

# Healthcheck
HEALTHCHECK --interval=30s --timeout=5s --start-period=60s \
  CMD curl -f http://localhost:5000/health || exit 1

# Expose port
EXPOSE 5000

# –ó–∞–ø—É—Å–∫
CMD ["python", "desktop_server.py"]
```

```yaml
# docker-compose.yml

version: '3.8'

services:
  ai-server:
    build: .
    ports:
      - "5000:5000"
    environment:
      - ENVIRONMENT=production
      - SECRET_KEY=${SECRET_KEY}
    volumes:
      - ./models:/models  # –ö—ç—à –º–æ–¥–µ–ª–µ–π
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    restart: unless-stopped

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped
```

---

#### ‚úÖ Systemd service –¥–ª—è Linux

```ini
# /etc/systemd/system/ai-server.service

[Unit]
Description=AI Server
After=network.target

[Service]
Type=simple
User=aiserver
WorkingDirectory=/opt/ai-server
Environment="PATH=/opt/ai-server/venv/bin"
ExecStart=/opt/ai-server/venv/bin/python desktop_server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫
sudo systemctl daemon-reload
sudo systemctl enable ai-server
sudo systemctl start ai-server

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
sudo systemctl status ai-server

# –õ–æ–≥–∏
sudo journalctl -u ai-server -f
```

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

–í–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:

### Performance:
- **Latency (p50, p95, p99):** <500ms (—Ö–æ—Ä–æ—à–æ), <1000ms (–Ω–æ—Ä–º–∞–ª—å–Ω–æ)
- **Throughput:** –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫—É–Ω–¥—É
- **Token generation speed:** —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫—É–Ω–¥—É

### Resources:
- **CPU usage:** <80%
- **Memory usage:** <90%
- **GPU utilization:** >80% (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)

### Reliability:
- **Error rate:** <1%
- **Uptime:** >99.9%
- **Cache hit rate:** >50%

### Business:
- **Daily active users**
- **Total requests/day**
- **Average session duration**

## –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É:

**–î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:**
‚Üí –ù–∞—á–Ω–∏—Ç–µ —Å **Ollama**
- –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- –ú–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤
- –°–æ–æ–±—â–µ—Å—Ç–≤–æ –ø–æ–¥–¥–µ—Ä–∂–∫–∏

**–î–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤:**
‚Üí **Ollama** –¥–ª—è MVP
‚Üí –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ **Desktop Server** –ø—Ä–∏ —Ä–æ—Å—Ç–µ

**–î–ª—è production:**
‚Üí **Desktop Server** + –æ–±–ª–∞–∫–æ
- –õ–æ–∫–∞–ª—å–Ω–æ: –±—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏
- –û–±–ª–∞–∫–æ: —Ç—è–∂–µ–ª—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

**–î–ª—è enterprise:**
‚Üí –ü–æ–ª–Ω—ã–π **Desktop Server** —Å—Ç–µ–∫
- –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
- –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã:

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
- Ollama: https://ollama.ai/docs
- Transformers: https://huggingface.co/docs/transformers
- Flask: https://flask.palletsprojects.com/
- Flutter HTTP: https://pub.dev/packages/http

### –°–æ–æ–±—â–µ—Å—Ç–≤–∞:
- Ollama Discord: https://discord.gg/ollama
- Hugging Face Forum: https://discuss.huggingface.co/
- r/LocalLLaMA: https://reddit.com/r/LocalLLaMA

### –ú–æ–¥–µ–ª–∏:
- Ollama Library: https://ollama.ai/library
- Hugging Face Hub: https://huggingface.co/models
- Model Zoo: https://modelzoo.co/

---

**–ê–≤—Ç–æ—Ä –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏:** AI/ML Integration Team  
**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** 2026-01-08  
**–õ–∏—Ü–µ–Ω–∑–∏—è:** Open Source (MIT)
