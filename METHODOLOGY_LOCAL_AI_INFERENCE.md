# ü§ñ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø: –õ–æ–∫–∞–ª—å–Ω—ã–π AI/ML Inference –¥–ª—è Flutter –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–∞—Ä–∏–∞–Ω—Ç 1: Ollama/LM Studio (–ì–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è)](#–≤–∞—Ä–∏–∞–Ω—Ç-1-ollamlm-studio)
2. [–í–∞—Ä–∏–∞–Ω—Ç 2: Desktop Server DIY ML Stack](#–≤–∞—Ä–∏–∞–Ω—Ç-2-desktop-server-diy-ml-stack)
3. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Flutter](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-flutter)
4. [–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä](#—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ-–∏-–≤—ã–±–æ—Ä)
5. [Troubleshooting](#troubleshooting)

---

# –í–ê–†–ò–ê–ù–¢ 1: Ollama/LM Studio (–ì–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è)

## üéØ –î–ª—è –∫–æ–≥–æ: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –±–µ–∑ –≥–ª—É–±–æ–∫–∏—Ö –∑–Ω–∞–Ω–∏–π ML

**–¶–µ–ª—å:** –ó–∞–ø—É—Å—Ç–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é LLM –º–æ–¥–µ–ª—å –∑–∞ 10 –º–∏–Ω—É—Ç

---

## 1.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

### Windows:

```powershell
# –°–∫–∞—á–∞—Ç—å —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞
https://ollama.ai/download

# –ò–ª–∏ —á–µ—Ä–µ–∑ winget
winget install Ollama.Ollama

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
ollama --version
```

### macOS:

```bash
# Homebrew
brew install ollama

# –ò–ª–∏ —Å–∫–∞—á–∞—Ç—å DMG
# https://ollama.ai/download
```

### Linux:

```bash
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫
curl https://ollama.ai/install.sh | sh

# –ü—Ä–æ–≤–µ—Ä–∫–∞
ollama --version
```

---

## 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π

### –®–∞–≥ 1: –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏

```bash
# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list

# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏:
# - llama2 (7B) - –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å, ~4 –ì–ë
# - llama2:13b - –±–æ–ª—å—à–µ –º–æ–¥–µ–ª—å, ~8 –ì–ë
# - mistral - –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å, ~4 –ì–ë
# - codellama - –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
# - phi - –º–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å, ~2 –ì–ë
```

### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
ollama pull llama2

# –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ª—É—á—à–µ:
ollama pull llama2:13b

# –î–ª—è –∫–æ–¥–∞:
ollama pull codellama
```

### –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```bash
# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
ollama run llama2

# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å –≤ —á–∞—Ç:
# >>> –ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python
# >>> /bye  # –≤—ã—Ö–æ–¥
```

---

## 1.3 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API —Å–µ—Ä–≤–µ—Ä–∞

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞:

```bash
# Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç API –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ
# API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: http://localhost:11434

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã API
curl http://localhost:11434/api/tags
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

```bash
# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (Windows PowerShell)
$env:OLLAMA_HOST = "0.0.0.0:11434"  # –î–æ—Å—Ç—É–ø –∏–∑ —Å–µ—Ç–∏
$env:OLLAMA_MODELS = "D:\Models"     # –ü—É—Ç—å –¥–ª—è –º–æ–¥–µ–ª–µ–π

# Linux/macOS
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MODELS=/path/to/models
```

---

## 1.4 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API

### –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:

```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python",
  "stream": false
}'

# –û—Ç–≤–µ—Ç:
{
  "model": "llama2",
  "created_at": "2024-01-08T...",
  "response": "Python - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π...",
  "done": true
}
```

### Streaming —Ä–µ–∂–∏–º:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "–ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö",
  "stream": true
}'

# –û—Ç–≤–µ—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç —á–∞—Å—Ç—è–º–∏ (Server-Sent Events)
data: {"response": "–í"}
data: {"response": " –ø–æ–ª–µ"}
data: {"response": " –±–µ—Ä—ë–∑–∫–∞"}
...
```

### –ß–∞—Ç —Ä–µ–∂–∏–º (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º):

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama2",
  "messages": [
    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω"},
    {"role": "assistant", "content": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π, –ò–≤–∞–Ω!"},
    {"role": "user", "content": "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?"}
  ]
}'

# –û—Ç–≤–µ—Ç: "–¢–µ–±—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω"
```

---

## 1.5 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ LM Studio (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Å GUI)

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```
1. –°–∫–∞—á–∞—Ç—å: https://lmstudio.ai/
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (Windows/Mac/Linux)
3. –ó–∞–ø—É—Å—Ç–∏—Ç—å LM Studio
```

### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ GUI

```
1. –û—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥–∫—É "Discover"
2. –ù–∞–π—Ç–∏ –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "llama-2-7b-chat")
3. –ù–∞–∂–∞—Ç—å "Download"
4. –î–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∫–∏
```

### –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```
1. –í–∫–ª–∞–¥–∫–∞ "Chat"
2. –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞
3. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
   - Temperature: 0.7 (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)
   - Max tokens: 2048 (–¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞)
   - Top P: 0.9
4. –ù–∞—á–∞—Ç—å —á–∞—Ç
```

### –®–∞–≥ 4: –í–∫–ª—é—á–µ–Ω–∏–µ API —Å–µ—Ä–≤–µ—Ä–∞

```
1. –í–∫–ª–∞–¥–∫–∞ "Local Server"
2. –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å
3. –ù–∞–∂–∞—Ç—å "Start Server"
4. API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:1234
```

---

## 1.6 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ –º–æ—â–Ω–æ—Å—Ç–∏:

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | RAM | GPU | –°–∫–æ—Ä–æ—Å—Ç—å |
|--------|--------|-----|-----|----------|
| phi-2 | 2.7B | 4 –ì–ë | –û–ø—Ü. | ‚ö°‚ö°‚ö° –ë—ã—Å—Ç—Ä–æ |
| llama2 | 7B | 8 –ì–ë | –û–ø—Ü. | ‚ö°‚ö° –°—Ä–µ–¥–Ω–µ |
| llama2:13b | 13B | 16 –ì–ë | –ñ–µ–ª–∞—Ç. | ‚ö° –ú–µ–¥–ª–µ–Ω–Ω–æ |
| llama2:70b | 70B | 64 –ì–ë | –¢—Ä–µ–±—É–µ—Ç—Å—è | üêå –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ |

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è:

```bash
# Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
nvidia-smi  # NVIDIA
rocm-smi    # AMD

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU
OLLAMA_GPU=0 ollama run llama2
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:

```json
{
  "model": "llama2",
  "prompt": "...",
  "options": {
    "num_ctx": 2048,        // –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–º–µ–Ω—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ)
    "num_gpu": 1,           // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU —Å–ª–æ–µ–≤
    "num_thread": 8,        // CPU –ø–æ—Ç–æ–∫–∏
    "temperature": 0.7,     // –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0-1)
    "top_p": 0.9,          // Nucleus sampling
    "repeat_penalty": 1.1   // –ò–∑–±–µ–≥–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
  }
}
```

---

## 1.7 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "system": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.",
  "prompt": "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤ Python?"
}'
```

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Modelfile):

```dockerfile
# Modelfile
FROM llama2

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM """
–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ Data Science.
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—à—å—Å—è –Ω–∞ Python, pandas, numpy.
–í—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—à—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞.
"""

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
PARAMETER temperature 0.5
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
```

```bash
# –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å
ollama create data-scientist -f Modelfile

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
ollama run data-scientist
```

### Embedding –º–æ–¥–µ–ª–∏ (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è):

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç—å embedding –º–æ–¥–µ–ª—å
ollama pull nomic-embed-text

# –ü–æ–ª—É—á–∏—Ç—å embedding
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "Python programming language"
}'

# –û—Ç–≤–µ—Ç: –º–∞—Å—Å–∏–≤ —á–∏—Å–µ–ª [0.1, -0.5, 0.3, ...]
# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
```

---

# –í–ê–†–ò–ê–ù–¢ 2: Desktop Server DIY ML Stack

## üéØ –î–ª—è –∫–æ–≥–æ: –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å –æ–ø—ã—Ç–æ–º Python

**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å –≥–∏–±–∫–∏–π ML backend —Å –ª—é–±—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏

---

## 2.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ —Å—Ç–µ–∫–∞

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ XAMPP

#### Windows:
```
1. –°–∫–∞—á–∞—Ç—å: https://www.apachefriends.org/download.html
2. –ó–∞–ø—É—Å—Ç–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫
3. –í—ã–±—Ä–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
   ‚úÖ Apache
   ‚úÖ MySQL
   ‚úÖ PHP
   ‚ùå Perl (–Ω–µ –Ω—É–∂–µ–Ω)
   ‚ùå FileZilla (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
4. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ C:\xampp
5. –ó–∞–ø—É—Å—Ç–∏—Ç—å XAMPP Control Panel
6. Start: Apache, MySQL
```

#### Linux (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ - –Ω–∞—Ç–∏–≤–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞):
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install apache2 mysql-server php libapache2-mod-php

# –í–∫–ª—é—á–∏—Ç—å —Å–µ—Ä–≤–∏—Å—ã
sudo systemctl start apache2
sudo systemctl start mysql
```

### –®–∞–≥ 2: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Python –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
python --version  # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å 3.9+

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
cd C:\xampp\htdocs  # –∏–ª–∏ /var/www/html
mkdir ml-backend
cd ml-backend
python -m venv venv

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è (Windows)
venv\Scripts\activate

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è (Linux/Mac)
source venv/bin/activate

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ pip
pip install --upgrade pip
```

### –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ ML –±–∏–±–ª–∏–æ—Ç–µ–∫

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install flask flask-cors
pip install numpy pandas scikit-learn
pip install torch torchvision torchaudio  # CPU version
# –î–ª—è GPU: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install transformers
pip install sentence-transformers
pip install spacy

# Computer Vision
pip install opencv-python
pip install pillow

# –£—Ç–∏–ª–∏—Ç—ã
pip install python-dotenv
pip install requests
pip install celery redis  # –î–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
```

---

## 2.2 –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ ML API

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:

```
ml-backend/
‚îú‚îÄ‚îÄ venv/                 # –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
‚îú‚îÄ‚îÄ models/               # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ uploads/              # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
‚îú‚îÄ‚îÄ app.py               # –ì–ª–∞–≤–Ω—ã–π Flask —Å–µ—Ä–≤–µ—Ä
‚îú‚îÄ‚îÄ config.py            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ requirements.txt     # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ services/
    ‚îú‚îÄ‚îÄ llm_service.py      # LLM —Å–µ—Ä–≤–∏—Å
    ‚îú‚îÄ‚îÄ vision_service.py   # Computer Vision
    ‚îî‚îÄ‚îÄ audio_service.py    # –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
```

### app.py - –ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª:

```python
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
from pathlib import Path

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Flask
app = Flask(__name__)
CORS(app)  # –†–∞–∑—Ä–µ—à–∏—Ç—å CORS –¥–ª—è Flutter

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPLOAD_FOLDER = Path('uploads')
UPLOAD_FOLDER.mkdir(exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100 –ú–ë

# –ò–º–ø–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–æ–≤
from services.llm_service import LLMService
from services.vision_service import VisionService

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
llm_service = LLMService()
vision_service = VisionService()

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return jsonify({
        'status': 'ok',
        'services': {
            'llm': llm_service.is_ready(),
            'vision': vision_service.is_ready()
        }
    })

@app.route('/api/llm/generate', methods=['POST'])
def generate_text():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        max_length = data.get('max_length', 200)

        logger.info(f"Generating text for prompt: {prompt[:50]}...")

        result = llm_service.generate(prompt, max_length)

        return jsonify({
            'success': True,
            'text': result['text'],
            'model': result['model'],
            'time': result['time']
        })

    except Exception as e:
        logger.error(f"Error in text generation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/vision/classify', methods=['POST'])
def classify_image():
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        if 'image' not in request.files:
            return jsonify({'success': False, 'error': 'No image provided'}), 400

        file = request.files['image']

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        filepath = UPLOAD_FOLDER / file.filename
        file.save(filepath)

        logger.info(f"Classifying image: {file.filename}")

        result = vision_service.classify(filepath)

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        filepath.unlink()

        return jsonify({
            'success': True,
            'predictions': result['predictions'],
            'model': result['model'],
            'time': result['time']
        })

    except Exception as e:
        logger.error(f"Error in image classification: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    logger.info("Starting ML Backend Server...")
    logger.info("Loading models...")

    # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    llm_service.load_model()
    vision_service.load_model()

    logger.info("‚úÖ All models loaded!")
    logger.info("üöÄ Server running on http://localhost:5000")

    app.run(
        host='0.0.0.0',
        port=5000,
        debug=True,
        threaded=True
    )
```

### services/llm_service.py - LLM —Å–µ—Ä–≤–∏—Å:

```python
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
import time
from pathlib import Path

class LLMService:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.model_name = "distilgpt2"  # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞—Ä—Ç–∞
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"Loading LLM model: {self.model_name} on {self.device}")

        # –ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏:
        # - "distilgpt2" - –±—ã—Å—Ç—Ä–∞—è, –º–∞–ª–µ–Ω—å–∫–∞—è (80 –ú–ë)
        # - "gpt2" - —Å—Ä–µ–¥–Ω—è—è (500 –ú–ë)
        # - "EleutherAI/gpt-neo-1.3B" - –±–æ–ª—å—à–∞—è (5 –ì–ë)

        self.pipeline = pipeline(
            "text-generation",
            model=self.model_name,
            device=0 if self.device == "cuda" else -1
        )

        print("‚úÖ LLM model loaded")

    def generate(self, prompt, max_length=200):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if self.pipeline is None:
            raise RuntimeError("Model not loaded")

        start_time = time.time()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        result = self.pipeline(
            prompt,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )

        elapsed_time = time.time() - start_time

        return {
            'text': result[0]['generated_text'],
            'model': self.model_name,
            'time': round(elapsed_time, 2)
        }

    def is_ready(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏"""
        return self.pipeline is not None
```

### services/vision_service.py - Vision —Å–µ—Ä–≤–∏—Å:

```python
from transformers import pipeline
import torch
import time
from PIL import Image

class VisionService:
    def __init__(self):
        self.pipeline = None
        self.model_name = "google/vit-base-patch16-224"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"Loading Vision model: {self.model_name} on {self.device}")

        self.pipeline = pipeline(
            "image-classification",
            model=self.model_name,
            device=0 if self.device == "cuda" else -1
        )

        print("‚úÖ Vision model loaded")

    def classify(self, image_path):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if self.pipeline is None:
            raise RuntimeError("Model not loaded")

        start_time = time.time()

        # –û—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(image_path)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        results = self.pipeline(image, top_k=5)

        elapsed_time = time.time() - start_time

        return {
            'predictions': [
                {
                    'label': r['label'],
                    'score': round(r['score'], 4)
                }
                for r in results
            ],
            'model': self.model_name,
            'time': round(elapsed_time, 2)
        }

    def is_ready(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏"""
        return self.pipeline is not None
```

---

## 2.3 –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π LLM:

```python
# services/llm_service.py

class LLMService:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
        self.model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
        # –ò–ª–∏ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é: "ai-forever/rugpt3large_based_on_gpt2"

    def generate_russian(self, prompt, max_length=200):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"""
        result = self.pipeline(
            prompt,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.8,
            top_p=0.9,
            repetition_penalty=1.2  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–æ–≤
        )

        return result[0]['generated_text']
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ Stable Diffusion:

```python
# services/image_generation_service.py

from diffusers import StableDiffusionPipeline
import torch

class ImageGenerationService:
    def __init__(self):
        self.model_name = "runwayml/stable-diffusion-v1-5"
        self.pipeline = None

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Stable Diffusion"""
        print(f"Loading Stable Diffusion: {self.model_name}")

        self.pipeline = StableDiffusionPipeline.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )

        if torch.cuda.is_available():
            self.pipeline = self.pipeline.to("cuda")

        print("‚úÖ Stable Diffusion loaded")

    def generate_image(self, prompt, negative_prompt="", num_steps=50):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        image = self.pipeline(
            prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_steps,
            guidance_scale=7.5
        ).images[0]

        return image
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—á–µ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:

```python
# services/audio_service.py

from transformers import pipeline
import torch

class AudioService:
    def __init__(self):
        self.model_name = "openai/whisper-base"
        self.pipeline = None

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Whisper"""
        print(f"Loading Whisper: {self.model_name}")

        self.pipeline = pipeline(
            "automatic-speech-recognition",
            model=self.model_name,
            device=0 if torch.cuda.is_available() else -1
        )

        print("‚úÖ Whisper loaded")

    def transcribe(self, audio_path):
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ"""
        result = self.pipeline(
            str(audio_path),
            return_timestamps=True
        )

        return {
            'text': result['text'],
            'chunks': result.get('chunks', [])
        }
```

---

## 2.4 –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö

### MySQL –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏:

```python
# database.py

import mysql.connector
from datetime import datetime
import json

class Database:
    def __init__(self):
        self.conn = mysql.connector.connect(
            host="localhost",
            user="root",
            password="",
            database="ml_backend"
        )
        self.cursor = self.conn.cursor()
        self.create_tables()

    def create_tables(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS requests (
                id INT AUTO_INCREMENT PRIMARY KEY,
                service VARCHAR(50),
                input_data TEXT,
                output_data TEXT,
                processing_time FLOAT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()

    def log_request(self, service, input_data, output_data, processing_time):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        self.cursor.execute("""
            INSERT INTO requests (service, input_data, output_data, processing_time)
            VALUES (%s, %s, %s, %s)
        """, (
            service,
            json.dumps(input_data),
            json.dumps(output_data),
            processing_time
        ))
        self.conn.commit()

    def get_history(self, service=None, limit=100):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏"""
        if service:
            self.cursor.execute("""
                SELECT * FROM requests
                WHERE service = %s
                ORDER BY created_at DESC
                LIMIT %s
            """, (service, limit))
        else:
            self.cursor.execute("""
                SELECT * FROM requests
                ORDER BY created_at DESC
                LIMIT %s
            """, (limit,))

        return self.cursor.fetchall()
```

---

## 2.5 –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å Celery

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Celery:

```python
# celery_app.py

from celery import Celery
import redis

# Celery –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
celery_app = Celery(
    'ml_backend',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)
```

### –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á:

```python
# tasks.py

from celery_app import celery_app
from services.llm_service import LLMService
import time

@celery_app.task(name='tasks.train_model')
def train_model(dataset_path, model_config):
    """–î–æ–ª–≥–∞—è –∑–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    # –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å —á–∞—Å—ã

    # –°–∏–º—É–ª—è—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    for epoch in range(model_config['epochs']):
        time.sleep(10)  # –°–∏–º—É–ª—è—Ü–∏—è —ç–ø–æ—Ö–∏

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        train_model.update_state(
            state='PROGRESS',
            meta={'current': epoch, 'total': model_config['epochs']}
        )

    return {'status': 'completed', 'model_path': '/models/trained_model.pt'}

@celery_app.task(name='tasks.process_large_dataset')
def process_large_dataset(file_path):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    import pandas as pd

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(file_path)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    result = df.groupby('category').agg({
        'value': ['sum', 'mean', 'count']
    })

    return result.to_dict()
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ API:

```python
# app.py

from tasks import train_model, process_large_dataset

@app.route('/api/train', methods=['POST'])
def start_training():
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ"""
    data = request.get_json()

    # –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏
    task = train_model.delay(
        dataset_path=data['dataset'],
        model_config=data['config']
    )

    return jsonify({
        'success': True,
        'task_id': task.id,
        'status_url': f'/api/task/{task.id}/status'
    })

@app.route('/api/task/<task_id>/status', methods=['GET'])
def task_status(task_id):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏"""
    task = celery_app.AsyncResult(task_id)

    if task.state == 'PENDING':
        response = {
            'state': task.state,
            'status': '–ó–∞–¥–∞—á–∞ –≤ –æ—á–µ—Ä–µ–¥–∏...'
        }
    elif task.state == 'PROGRESS':
        response = {
            'state': task.state,
            'current': task.info.get('current', 0),
            'total': task.info.get('total', 1),
            'status': f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {task.info.get('current', 0)}/{task.info.get('total', 1)}"
        }
    elif task.state == 'SUCCESS':
        response = {
            'state': task.state,
            'result': task.result,
            'status': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ!'
        }
    else:
        response = {
            'state': task.state,
            'status': str(task.info)
        }

    return jsonify(response)
```

---

## 2.6 –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### Prometheus –º–µ—Ç—Ä–∏–∫–∏:

```python
# metrics.py

from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
import time

# –ú–µ—Ç—Ä–∏–∫–∏
request_count = Counter(
    'ml_backend_requests_total',
    'Total number of requests',
    ['service', 'status']
)

request_duration = Histogram(
    'ml_backend_request_duration_seconds',
    'Request duration in seconds',
    ['service']
)

# Middleware –¥–ª—è –º–µ—Ç—Ä–∏–∫
@app.before_request
def before_request():
    request.start_time = time.time()

@app.after_request
def after_request(response):
    request_duration.labels(
        service=request.endpoint
    ).observe(time.time() - request.start_time)

    request_count.labels(
        service=request.endpoint,
        status=response.status_code
    ).inc()

    return response

# Endpoint –¥–ª—è –º–µ—Ç—Ä–∏–∫
@app.route('/metrics')
def metrics():
    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}
```

---

# –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° FLUTTER

## 3.1 Flutter –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama

```dart
// lib/services/ollama_service.dart

import 'package:dio/dio.dart';

class OllamaService {
  final Dio _dio;
  final String baseUrl;

  OllamaService({
    this.baseUrl = 'http://localhost:11434',
  }) : _dio = Dio(BaseOptions(baseUrl: baseUrl));

  /// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
  Future<String> generate({
    required String model,
    required String prompt,
    bool stream = false,
  }) async {
    try {
      final response = await _dio.post(
        '/api/generate',
        data: {
          'model': model,
          'prompt': prompt,
          'stream': stream,
        },
      );

      return response.data['response'];
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: $e');
    }
  }

  /// –ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
  Future<String> chat({
    required String model,
    required List<Map<String, String>> messages,
  }) async {
    try {
      final response = await _dio.post(
        '/api/chat',
        data: {
          'model': model,
          'messages': messages,
        },
      );

      return response.data['message']['content'];
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ —á–∞—Ç–∞: $e');
    }
  }

  /// Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
  Stream<String> generateStream({
    required String model,
    required String prompt,
  }) async* {
    try {
      final response = await _dio.post(
        '/api/generate',
        data: {
          'model': model,
          'prompt': prompt,
          'stream': true,
        },
        options: Options(responseType: ResponseType.stream),
      );

      await for (var chunk in response.data.stream) {
        final text = String.fromCharCodes(chunk);
        final lines = text.split('\n');

        for (var line in lines) {
          if (line.trim().isEmpty) continue;

          try {
            final json = jsonDecode(line);
            if (json['response'] != null) {
              yield json['response'];
            }
          } catch (_) {}
        }
      }
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ streaming: $e');
    }
  }

  /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
  Future<bool> isAvailable() async {
    try {
      await _dio.get('/api/tags');
      return true;
    } catch (e) {
      return false;
    }
  }

  /// –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
  Future<List<String>> listModels() async {
    try {
      final response = await _dio.get('/api/tags');
      final models = response.data['models'] as List;
      return models.map((m) => m['name'] as String).toList();
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: $e');
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Flutter:

```dart
// lib/screens/chat_screen.dart

import 'package:flutter/material.dart';
import '../services/ollama_service.dart';

class ChatScreen extends StatefulWidget {
  @override
  _ChatScreenState createState() => _ChatScreenState();
}

class _ChatScreenState extends State<ChatScreen> {
  final OllamaService _ollama = OllamaService();
  final TextEditingController _controller = TextEditingController();
  final List<Map<String, String>> _messages = [];
  bool _isLoading = false;

  @override
  void initState() {
    super.initState();
    _checkAvailability();
  }

  Future<void> _checkAvailability() async {
    final available = await _ollama.isAvailable();
    if (!available) {
      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä.')),
      );
    }
  }

  Future<void> _sendMessage() async {
    if (_controller.text.trim().isEmpty) return;

    final userMessage = _controller.text;
    _controller.clear();

    setState(() {
      _messages.add({'role': 'user', 'content': userMessage});
      _isLoading = true;
    });

    try {
      // –í–∞—Ä–∏–∞–Ω—Ç 1: –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
      final response = await _ollama.chat(
        model: 'llama2',
        messages: _messages,
      );

      setState(() {
        _messages.add({'role': 'assistant', 'content': response});
        _isLoading = false;
      });

      // –í–∞—Ä–∏–∞–Ω—Ç 2: Streaming (–±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ)
      // String assistantMessage = '';
      // setState(() {
      //   _messages.add({'role': 'assistant', 'content': ''});
      // });
      //
      // await for (var chunk in _ollama.generateStream(
      //   model: 'llama2',
      //   prompt: userMessage,
      // )) {
      //   assistantMessage += chunk;
      //   setState(() {
      //     _messages.last['content'] = assistantMessage;
      //   });
      // }
      //
      // setState(() {
      //   _isLoading = false;
      // });

    } catch (e) {
      setState(() {
        _isLoading = false;
      });

      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('–û—à–∏–±–∫–∞: $e')),
      );
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text('Ollama Chat'),
      ),
      body: Column(
        children: [
          // –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
          Expanded(
            child: ListView.builder(
              padding: EdgeInsets.all(16),
              itemCount: _messages.length,
              itemBuilder: (context, index) {
                final message = _messages[index];
                final isUser = message['role'] == 'user';

                return Align(
                  alignment: isUser ? Alignment.centerRight : Alignment.centerLeft,
                  child: Container(
                    margin: EdgeInsets.only(bottom: 8),
                    padding: EdgeInsets.all(12),
                    decoration: BoxDecoration(
                      color: isUser ? Colors.blue : Colors.grey[300],
                      borderRadius: BorderRadius.circular(12),
                    ),
                    child: Text(
                      message['content']!,
                      style: TextStyle(
                        color: isUser ? Colors.white : Colors.black,
                      ),
                    ),
                  ),
                );
              },
            ),
          ),

          // –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∑–∫–∏
          if (_isLoading)
            Padding(
              padding: EdgeInsets.all(8),
              child: CircularProgressIndicator(),
            ),

          // –ü–æ–ª–µ –≤–≤–æ–¥–∞
          Padding(
            padding: EdgeInsets.all(16),
            child: Row(
              children: [
                Expanded(
                  child: TextField(
                    controller: _controller,
                    decoration: InputDecoration(
                      hintText: '–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...',
                      border: OutlineInputBorder(),
                    ),
                    onSubmitted: (_) => _sendMessage(),
                  ),
                ),
                SizedBox(width: 8),
                IconButton(
                  icon: Icon(Icons.send),
                  onPressed: _sendMessage,
                ),
              ],
            ),
          ),
        ],
      ),
    );
  }
}
```

---

## 3.2 Flutter –∫–ª–∏–µ–Ω—Ç –¥–ª—è Desktop Server

```dart
// lib/services/ml_backend_service.dart

import 'package:dio/dio.dart';
import 'dart:io';

class MLBackendService {
  final Dio _dio;
  final String baseUrl;

  MLBackendService({
    this.baseUrl = 'http://localhost:5000',
  }) : _dio = Dio(BaseOptions(baseUrl: baseUrl));

  /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞
  Future<Map<String, dynamic>> healthCheck() async {
    final response = await _dio.get('/health');
    return response.data;
  }

  /// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM
  Future<String> generateText({
    required String prompt,
    int maxLength = 200,
  }) async {
    try {
      final response = await _dio.post(
        '/api/llm/generate',
        data: {
          'prompt': prompt,
          'max_length': maxLength,
        },
      );

      if (response.data['success']) {
        return response.data['text'];
      } else {
        throw Exception(response.data['error']);
      }
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: $e');
    }
  }

  /// –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
  Future<List<Map<String, dynamic>>> classifyImage(File imageFile) async {
    try {
      // –°–æ–∑–¥–∞–Ω–∏–µ FormData –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
      final formData = FormData.fromMap({
        'image': await MultipartFile.fromFile(
          imageFile.path,
          filename: imageFile.path.split('/').last,
        ),
      });

      final response = await _dio.post(
        '/api/vision/classify',
        data: formData,
      );

      if (response.data['success']) {
        return List<Map<String, dynamic>>.from(response.data['predictions']);
      } else {
        throw Exception(response.data['error']);
      }
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: $e');
    }
  }

  /// –ó–∞–ø—É—Å–∫ –¥–æ–ª–≥–æ–π –∑–∞–¥–∞—á–∏
  Future<String> startTraining({
    required String datasetPath,
    required Map<String, dynamic> config,
  }) async {
    try {
      final response = await _dio.post(
        '/api/train',
        data: {
          'dataset': datasetPath,
          'config': config,
        },
      );

      return response.data['task_id'];
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è: $e');
    }
  }

  /// –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏
  Future<Map<String, dynamic>> checkTaskStatus(String taskId) async {
    try {
      final response = await _dio.get('/api/task/$taskId/status');
      return response.data;
    } catch (e) {
      throw Exception('–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞: $e');
    }
  }

  /// Polling –∑–∞–¥–∞—á–∏ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
  Stream<Map<String, dynamic>> pollTask(String taskId) async* {
    while (true) {
      await Future.delayed(Duration(seconds: 2));

      final status = await checkTaskStatus(taskId);
      yield status;

      if (status['state'] == 'SUCCESS' || status['state'] == 'FAILURE') {
        break;
      }
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:

```dart
// lib/screens/image_classifier_screen.dart

import 'package:flutter/material.dart';
import 'package:image_picker/image_picker.dart';
import 'dart:io';
import '../services/ml_backend_service.dart';

class ImageClassifierScreen extends StatefulWidget {
  @override
  _ImageClassifierScreenState createState() => _ImageClassifierScreenState();
}

class _ImageClassifierScreenState extends State<ImageClassifierScreen> {
  final MLBackendService _mlService = MLBackendService();
  final ImagePicker _picker = ImagePicker();

  File? _image;
  List<Map<String, dynamic>>? _predictions;
  bool _isLoading = false;

  Future<void> _pickImage() async {
    final XFile? pickedFile = await _picker.pickImage(
      source: ImageSource.gallery,
    );

    if (pickedFile != null) {
      setState(() {
        _image = File(pickedFile.path);
        _predictions = null;
      });

      await _classifyImage();
    }
  }

  Future<void> _classifyImage() async {
    if (_image == null) return;

    setState(() {
      _isLoading = true;
    });

    try {
      final predictions = await _mlService.classifyImage(_image!);

      setState(() {
        _predictions = predictions;
        _isLoading = false;
      });
    } catch (e) {
      setState(() {
        _isLoading = false;
      });

      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('–û—à–∏–±–∫–∞: $e')),
      );
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text('Image Classifier'),
      ),
      body: SingleChildScrollView(
        padding: EdgeInsets.all(16),
        child: Column(
          crossAxisAlignment: CrossAxisAlignment.stretch,
          children: [
            // –ö–Ω–æ–ø–∫–∞ –≤—ã–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            ElevatedButton.icon(
              icon: Icon(Icons.image),
              label: Text('–í—ã–±—Ä–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'),
              onPressed: _pickImage,
            ),

            SizedBox(height: 16),

            // –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if (_image != null)
              Container(
                height: 300,
                decoration: BoxDecoration(
                  border: Border.all(color: Colors.grey),
                  borderRadius: BorderRadius.circular(8),
                ),
                child: ClipRRect(
                  borderRadius: BorderRadius.circular(8),
                  child: Image.file(
                    _image!,
                    fit: BoxFit.contain,
                  ),
                ),
              ),

            SizedBox(height: 16),

            // –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∑–∫–∏
            if (_isLoading)
              Center(child: CircularProgressIndicator()),

            // –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if (_predictions != null && !_isLoading)
              Card(
                child: Padding(
                  padding: EdgeInsets.all(16),
                  child: Column(
                    crossAxisAlignment: CrossAxisAlignment.start,
                    children: [
                      Text(
                        '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:',
                        style: Theme.of(context).textTheme.headline6,
                      ),
                      SizedBox(height: 8),
                      ..._predictions!.map((prediction) {
                        return ListTile(
                          title: Text(prediction['label']),
                          trailing: Text(
                            '${(prediction['score'] * 100).toStringAsFixed(1)}%',
                            style: TextStyle(fontWeight: FontWeight.bold),
                          ),
                          leading: CircularProgressIndicator(
                            value: prediction['score'],
                            backgroundColor: Colors.grey[300],
                          ),
                        );
                      }).toList(),
                    ],
                  ),
                ),
              ),
          ],
        ),
      ),
    );
  }
}
```

---

## 3.3 –£–º–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ backend

```dart
// lib/services/smart_backend_router.dart

import 'ollama_service.dart';
import 'ml_backend_service.dart';

enum BackendType {
  ollama,      // –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö LLM –∑–∞–¥–∞—á
  mlBackend,   // –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö ML –∑–∞–¥–∞—á
}

class SmartBackendRouter {
  final OllamaService _ollama = OllamaService();
  final MLBackendService _mlBackend = MLBackendService();

  /// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä backend
  Future<BackendType> selectBackend({
    required String taskType,
    int? dataSize,
  }) async {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
    final ollamaAvailable = await _ollama.isAvailable();
    final mlBackendAvailable = await _checkMLBackend();

    // –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞
    switch (taskType) {
      case 'text_generation':
      case 'chat':
        // –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama (–±—ã—Å—Ç—Ä–µ–µ)
        return ollamaAvailable ? BackendType.ollama : BackendType.mlBackend;

      case 'image_classification':
      case 'object_detection':
        // Computer Vision - —Ç–æ–ª—å–∫–æ ML Backend
        return BackendType.mlBackend;

      case 'complex_nlp':
        // –°–ª–æ–∂–Ω—ã–π NLP - ML Backend (–±–æ–ª—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—å)
        return BackendType.mlBackend;

      default:
        // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Ollama
        return ollamaAvailable ? BackendType.ollama : BackendType.mlBackend;
    }
  }

  Future<bool> _checkMLBackend() async {
    try {
      await _mlBackend.healthCheck();
      return true;
    } catch (e) {
      return false;
    }
  }

  /// –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
  Future<String> generateText(String prompt) async {
    final backend = await selectBackend(taskType: 'text_generation');

    switch (backend) {
      case BackendType.ollama:
        return await _ollama.generate(
          model: 'llama2',
          prompt: prompt,
        );

      case BackendType.mlBackend:
        return await _mlBackend.generateText(prompt: prompt);
    }
  }
}
```

---

–≠—Ç–æ –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥–∞
- Troubleshooting –∏ FAQ
- Best practices
- –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤?

---

# –°–†–ê–í–ù–ï–ù–ò–ï –ò –í–´–ë–û–†

## 4.1 –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **OLLAMA** –µ—Å–ª–∏:

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| **–û–ø—ã—Ç** | –ù–µ—Ç –æ–ø—ã—Ç–∞ —Å Python/ML |
| **–ó–∞–¥–∞—á–∏** | –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ (—á–∞—Ç, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è) |
| **–í—Ä–µ–º—è** | –ù—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∑–∞ 10 –º–∏–Ω—É—Ç |
| **–ú–æ–¥–µ–ª–∏** | –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Llama, Mistral, etc.) |
| **–ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è** | –ù–µ –Ω—É–∂–Ω–∞ –≥–ª—É–±–æ–∫–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è |
| **–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ** | –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ (–∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è) |
| **GUI** | –ù—É–∂–µ–Ω –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å |

**–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ–≤:**
- –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
- –ß–∞—Ç-–±–æ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü–æ–º–æ—â–Ω–∏–∫ –ø–æ –∫–æ–¥—É

---

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **DESKTOP SERVER** –µ—Å–ª–∏:

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| **–û–ø—ã—Ç** | –ó–Ω–∞–Ω–∏–µ Python, ML, API —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ |
| **–ó–∞–¥–∞—á–∏** | –ú–Ω–æ–∂–µ—Å—Ç–≤–æ ML –∑–∞–¥–∞—á (LLM + CV + Audio + Custom) |
| **–í—Ä–µ–º—è** | –ú–æ–∂–Ω–æ –ø–æ—Ç—Ä–∞—Ç–∏—Ç—å 1-2 –¥–Ω—è –Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫—É |
| **–ú–æ–¥–µ–ª–∏** | –ù—É–∂–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ fine-tuning |
| **–ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è** | –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ pipeline |
| **–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ** | –ì–æ—Ç–æ–≤—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å |
| **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** | –°–ª–æ–∂–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ |

**–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ–≤:**
- –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + –∞—É–¥–∏–æ)
- –°–∏—Å—Ç–µ–º–∞ —Å fine-tuned –º–æ–¥–µ–ª—è–º–∏
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è data science
- –ö–∞—Å—Ç–æ–º–Ω—ã–µ ML pipeline

---

## 4.2 –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

| –§—É–Ω–∫—Ü–∏—è | Ollama | Desktop Server |
|---------|--------|---------------|
| **–¢–µ–∫—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ |
| **–ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º** | ‚úÖ –î–∞ | ‚úÖ –î–∞ |
| **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ |
| **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (Stable Diffusion) |
| **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (Whisper) |
| **Fine-tuning –º–æ–¥–µ–ª–µ–π** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ |
| **–ö–∞—Å—Ç–æ–º–Ω—ã–µ pipeline** | ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ | ‚úÖ –ü–æ–ª–Ω–∞—è —Å–≤–æ–±–æ–¥–∞ |
| **–§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (Celery) |
| **–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö** | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ (MySQL/PostgreSQL) |
| **–ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** | ‚ö†Ô∏è –ë–∞–∑–æ–≤—ã–π | ‚úÖ –ü–æ–ª–Ω—ã–π (Prometheus) |
| **API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** | ‚úÖ OpenAI-like | ‚úÖ –ö–∞—Å—Ç–æ–º–Ω—ã–π |
| **–£—Å—Ç–∞–Ω–æ–≤–∫–∞** | ‚ö° 5 –º–∏–Ω—É—Ç | üêå 1-2 —á–∞—Å–∞ |
| **–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ** | üü¢ –ü—Ä–æ—Å—Ç–æ–µ | üî¥ –°–ª–æ–∂–Ω–æ–µ |

---

## 4.3 –°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç-–±–æ—Ç

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –õ–æ–∫–∞–ª—å–Ω—ã–π ChatGPT
- –ß–∞—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
- –ü—Ä–æ—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

–†–ï–®–ï–ù–ò–ï: OLLAMA ‚úÖ
–í—Ä–µ–º—è: 10 –º–∏–Ω—É—Ç
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üü¢ –õ–µ–≥–∫–æ
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π + —á–∞—Ç

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —Ñ–æ—Ç–æ
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ß–∞—Ç –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º

–†–ï–®–ï–ù–ò–ï: DESKTOP SERVER ‚úÖ
–í—Ä–µ–º—è: 2 –¥–Ω—è
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üî¥ –°–ª–æ–∂–Ω–æ
–ü—Ä–∏—á–∏–Ω–∞: –ù—É–∂–µ–Ω CV + LLM
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- –†–∞–±–æ—Ç–∞ –æ—Ñ–ª–∞–π–Ω

–†–ï–®–ï–ù–ò–ï: OLLAMA ‚úÖ
–í—Ä–µ–º—è: 15 –º–∏–Ω—É—Ç
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üü¢ –õ–µ–≥–∫–æ
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 4: Data Science –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞

```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –û–±—Ä–∞–±–æ—Ç–∫–∞ CSV/Excel
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- ML –∞–Ω–∞–ª–∏–∑
- Fine-tuned –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞

–†–ï–®–ï–ù–ò–ï: DESKTOP SERVER ‚úÖ
–í—Ä–µ–º—è: 1 –Ω–µ–¥–µ–ª—è
–°–ª–æ–∂–Ω–æ—Å—Ç—å: üî¥ –û—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ
–ü—Ä–∏—á–∏–Ω–∞: –ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π ML —Å—Ç–µ–∫
```

---

# TROUBLESHOOTING

## 5.1 –ü—Ä–æ–±–ª–µ–º—ã —Å Ollama

### –ü—Ä–æ–±–ª–µ–º–∞: Ollama –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–∞
systemctl status ollama  # Linux
ps aux | grep ollama     # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞

# –†–µ—à–µ–Ω–∏–µ 1: –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫
systemctl restart ollama  # Linux
# Windows: –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ Task Manager

# –†–µ—à–µ–Ω–∏–µ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä—Ç–æ–≤
netstat -an | grep 11434
# –ï—Å–ª–∏ –ø–æ—Ä—Ç –∑–∞–Ω—è—Ç - –∏–∑–º–µ–Ω–∏—Ç—å OLLAMA_HOST

# –†–µ—à–µ–Ω–∏–µ 3: –õ–æ–≥–∏
journalctl -u ollama -f  # Linux
# Windows: C:\Users\<user>\.ollama\logs\
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

```bash
# –û—à–∏–±–∫–∞: "model not found"
# –†–µ—à–µ–Ω–∏–µ: –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
ollama pull llama2

# –û—à–∏–±–∫–∞: "insufficient memory"
# –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
ollama pull phi-2  # –¢–æ–ª—å–∫–æ 2.7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –º–µ—Å—Ç–∞
df -h ~/.ollama/models  # Linux
dir %USERPROFILE%\.ollama\models  # Windows
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

```bash
# –ü—Ä–æ–±–ª–µ–º–∞: –ù–µ—Ç GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è
# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
nvidia-smi  # NVIDIA GPU
rocm-smi    # AMD GPU

# –†–µ—à–µ–Ω–∏–µ: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–æ–≤
# NVIDIA CUDA: https://developer.nvidia.com/cuda-downloads
# AMD ROCm: https://rocm.docs.amd.com/

# –ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å
# –†–µ—à–µ–Ω–∏–µ: –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
ollama run llama2  # 7B - –±—ã—Å—Ç—Ä–µ–µ
# –≤–º–µ—Å—Ç–æ
ollama run llama2:70b  # 70B - –º–µ–¥–ª–µ–Ω–Ω–æ
```

### –ü—Ä–æ–±–ª–µ–º–∞: Flutter –Ω–µ –º–æ–∂–µ—Ç –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è

```dart
// –û—à–∏–±–∫–∞: Connection refused
// –†–µ—à–µ–Ω–∏–µ 1: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
curl http://localhost:11434/api/tags

// –†–µ—à–µ–Ω–∏–µ 2: –ò–∑–º–µ–Ω–∏—Ç—å URL
// –ï—Å–ª–∏ –Ω–∞ –¥—Ä—É–≥–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ –≤ —Å–µ—Ç–∏:
OllamaService(baseUrl: 'http://192.168.1.100:11434')

// –†–µ—à–µ–Ω–∏–µ 3: –ù–∞—Å—Ç—Ä–æ–∏—Ç—å CORS (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
// Ollama –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–∑—Ä–µ—à–∞–µ—Ç CORS
```

---

## 5.2 –ü—Ä–æ–±–ª–µ–º—ã —Å Desktop Server

### –ü—Ä–æ–±–ª–µ–º–∞: XAMPP –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```
–û—à–∏–±–∫–∞: "Port 80 already in use"

–†–µ—à–µ–Ω–∏–µ 1: –ò–∑–º–µ–Ω–∏—Ç—å –ø–æ—Ä—Ç Apache
1. –û—Ç–∫—Ä—ã—Ç—å XAMPP Control Panel
2. Config -> Apache (httpd.conf)
3. –ù–∞–π—Ç–∏: Listen 80
4. –ò–∑–º–µ–Ω–∏—Ç—å: Listen 8080
5. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å Apache

–†–µ—à–µ–Ω–∏–µ 2: –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å
Windows:
  netstat -ano | findstr :80
  taskkill /PID <PID> /F

Linux:
  sudo lsof -i :80
  sudo kill <PID>
```

### –ü—Ä–æ–±–ª–µ–º–∞: Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è

```bash
# –û—à–∏–±–∫–∞: "No module named 'torch'"
# –†–µ—à–µ–Ω–∏–µ: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ

# 1. –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate      # Windows

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏—é
which python  # –î–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å venv/bin/python

# 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt

# –û—à–∏–±–∫–∞: "Could not find a version that satisfies torch"
# –†–µ—à–µ–Ω–∏–µ: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π URL
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

```python
# –û—à–∏–±–∫–∞: "OutOfMemoryError"
# –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ CPU

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# –í–º–µ—Å—Ç–æ –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏:
# model_name = "EleutherAI/gpt-neo-2.7B"

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞–ª–µ–Ω—å–∫—É—é:
model_name = "distilgpt2"

# –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ CPU:
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,  # –í–º–µ—Å—Ç–æ float16
    device_map="cpu"  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU
)

# –û—à–∏–±–∫–∞: "Connection timeout"
# –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    cache_dir="./models",  # –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞
    local_files_only=True   # –ù–µ —Å–∫–∞—á–∏–≤–∞—Ç—å –∑–∞–Ω–æ–≤–æ
)
```

### –ü—Ä–æ–±–ª–µ–º–∞: Flask —Å–µ—Ä–≤–µ—Ä –ø–∞–¥–∞–µ—Ç

```bash
# –û—à–∏–±–∫–∞: "Address already in use"
# –†–µ—à–µ–Ω–∏–µ: –ò–∑–º–µ–Ω–∏—Ç—å –ø–æ—Ä—Ç

# –í app.py:
app.run(port=5001)  # –í–º–µ—Å—Ç–æ 5000

# –ò–ª–∏ —É–±–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ –ø–æ—Ä—Ç—É 5000:
# Linux/Mac:
lsof -ti:5000 | xargs kill -9

# Windows:
netstat -ano | findstr :5000
taskkill /PID <PID> /F

# –û—à–∏–±–∫–∞: "Werkzeug crashed"
# –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å production —Å–µ—Ä–≤–µ—Ä
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

### –ü—Ä–æ–±–ª–µ–º–∞: Flutter –Ω–µ –≤–∏–¥–∏—Ç API

```dart
// –û—à–∏–±–∫–∞: "Connection refused"
// –û—Ç–ª–∞–¥–∫–∞:

// 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API
curl http://localhost:5000/health

// 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å CORS
// –í app.py –¥–æ–±–∞–≤–∏—Ç—å:
from flask_cors import CORS
CORS(app, origins=['*'])  # –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

// 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å firewall
// Windows: Settings -> Firewall -> Allow app
// Linux: sudo ufw allow 5000

// 4. –ï—Å–ª–∏ Flutter –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å IP:
MLBackendService(baseUrl: 'http://192.168.1.100:5000')
```

---

## 5.3 –ü—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

```python
# –ü—Ä–æ–±–ª–µ–º–∞: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 30+ —Å–µ–∫—É–Ω–¥
# –†–µ—à–µ–Ω–∏–µ: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

# ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–æ
result = pipeline(
    prompt,
    max_length=2048,  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
    num_beams=5,      # Beam search –º–µ–¥–ª–µ–Ω–Ω—ã–π
)

# ‚úÖ –ë—ã—Å—Ç—Ä–æ
result = pipeline(
    prompt,
    max_length=200,    # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
    do_sample=True,    # Sampling –±—ã—Å—Ç—Ä–µ–µ beam search
    top_k=50,
    top_p=0.95,
    num_return_sequences=1
)
```

### –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM

```python
# –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 16+ –ì–ë RAM
# –†–µ—à–µ–Ω–∏–µ: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ 2 —Ä–∞–∑–∞)
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)

# –ò–ª–∏ 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—É–º–µ–Ω—å—à–∞–µ—Ç –≤ 4 —Ä–∞–∑–∞!)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

---

# BEST PRACTICES

## 6.1 –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

### Ollama:

```bash
# 1. –ù–µ –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –Ω–∞—Ä—É–∂—É –±–µ–∑ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: localhost only ‚úÖ

# 2. –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∏–∑ —Å–µ—Ç–∏ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å nginx proxy
server {
    listen 80;
    server_name ollama.local;

    location / {
        proxy_pass http://localhost:11434;
        
        # Basic auth
        auth_basic "Restricted";
        auth_basic_user_file /etc/nginx/.htpasswd;
    }
}

# 3. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞ –ø–æ IP
# –í nginx:
allow 192.168.1.0/24;
deny all;
```

### Desktop Server:

```python
# 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å API –∫–ª—é—á–∏
from flask import request, abort

API_KEYS = {
    "flutter-app-key-123": "mobile_app",
    "web-app-key-456": "web_client"
}

@app.before_request
def check_api_key():
    api_key = request.headers.get('X-API-Key')
    if api_key not in API_KEYS:
        abort(401, "Invalid API key")

# 2. –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
from pydantic import BaseModel, validator

class GenerateRequest(BaseModel):
    prompt: str
    max_length: int = 200
    
    @validator('prompt')
    def prompt_not_empty(cls, v):
        if not v.strip():
            raise ValueError('Prompt cannot be empty')
        return v
    
    @validator('max_length')
    def max_length_limit(cls, v):
        if v > 2048:
            raise ValueError('Max length cannot exceed 2048')
        return v

# 3. Rate limiting
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["100 per hour"]
)

@app.route('/api/llm/generate')
@limiter.limit("10 per minute")
def generate_text():
    # ...
    pass
```

---

## 6.2 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:

```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Redis –¥–ª—è –∫—ç—à–∞
import redis
import json
import hashlib

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def generate_with_cache(prompt, max_length):
    # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    cache_key = hashlib.md5(
        f"{prompt}:{max_length}".encode()
    ).hexdigest()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached = redis_client.get(cache_key)
    if cached:
        print("‚úÖ Cache hit!")
        return json.loads(cached)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ—Å–ª–∏ –Ω–µ—Ç –≤ –∫—ç—à–µ
    result = llm_service.generate(prompt, max_length)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à (TTL 1 —á–∞—Å)
    redis_client.setex(
        cache_key,
        3600,  # 1 —á–∞—Å
        json.dumps(result)
    )
    
    return result
```

### –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π:

```python
# app.py

# ‚ùå –ü–ª–æ—Ö–æ: –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ
@app.route('/api/generate')
def generate():
    model = load_model()  # –ú–µ–¥–ª–µ–Ω–Ω–æ!
    result = model.generate(...)
    return result

# ‚úÖ –•–æ—Ä–æ—à–æ: –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
llm_service = None
vision_service = None

@app.before_first_request
def init_models():
    global llm_service, vision_service
    
    print("Loading models...")
    llm_service = LLMService()
    llm_service.load_model()
    
    vision_service = VisionService()
    vision_service.load_model()
    print("‚úÖ Models loaded!")

@app.route('/api/generate')
def generate():
    # –ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
    result = llm_service.generate(...)
    return result
```

### Batch processing:

```python
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=4)

@app.route('/api/batch-generate', methods=['POST'])
def batch_generate():
    prompts = request.json['prompts']  # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    futures = [
        executor.submit(llm_service.generate, prompt)
        for prompt in prompts
    ]
    
    results = [future.result() for future in futures]
    
    return jsonify({'results': results})
```

---

## 6.3 –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤:

```python
import logging
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml_backend.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

@app.route('/api/llm/generate')
def generate_text():
    start_time = datetime.now()
    
    try:
        prompt = request.json['prompt']
        
        logger.info(f"Generate request: {prompt[:50]}...")
        
        result = llm_service.generate(prompt)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"Generation completed in {elapsed:.2f}s")
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Generation failed: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500
```

### –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```python
from prometheus_client import Counter, Histogram, Gauge

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
requests_total = Counter(
    'ml_requests_total',
    'Total requests',
    ['endpoint', 'status']
)

request_duration = Histogram(
    'ml_request_duration_seconds',
    'Request duration',
    ['endpoint']
)

models_loaded = Gauge(
    'ml_models_loaded',
    'Number of loaded models'
)

@app.before_request
def start_timer():
    request.start_time = time.time()

@app.after_request
def record_metrics(response):
    duration = time.time() - request.start_time
    
    request_duration.labels(
        endpoint=request.endpoint
    ).observe(duration)
    
    requests_total.labels(
        endpoint=request.endpoint,
        status=response.status_code
    ).inc()
    
    return response
```

---

# –ü–†–ò–ú–ï–†–´ –†–ï–ê–õ–¨–ù–´–• –ü–†–û–ï–ö–¢–û–í

## 7.1 –ü—Ä–æ–µ–∫—Ç: –õ–æ–∫–∞–ª—å–Ω—ã–π AI –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç (Ollama)

### –û–ø–∏—Å–∞–Ω–∏–µ:
–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, –∫–æ–¥–æ–º –∏ –æ–±—â–µ–Ω–∏—è.

### –°—Ç–µ–∫:
- Frontend: Flutter (Mobile + Desktop)
- Backend: Ollama (llama2)
- –•—Ä–∞–Ω–∏–ª–∏—â–µ: SharedPreferences

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:

```
Flutter App
‚îú‚îÄ‚îÄ Chat Screen
‚îÇ   ‚îî‚îÄ‚îÄ Ollama API (localhost:11434)
‚îú‚îÄ‚îÄ Document Analyzer
‚îÇ   ‚îî‚îÄ‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ PDF/TXT ‚Üí Ollama Summary
‚îî‚îÄ‚îÄ Code Helper
    ‚îî‚îÄ‚îÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è/–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ–ª–∞–π–Ω
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ª—é–±–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (10 –º–∏–Ω—É—Ç)
- ‚ùå –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏

---

## 7.2 –ü—Ä–æ–µ–∫—Ç: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ (Desktop Server)

### –û–ø–∏—Å–∞–Ω–∏–µ:
–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞—É–¥–∏–æ —Å –µ–¥–∏–Ω—ã–º API.

### –°—Ç–µ–∫:
- Frontend: Flutter Web + Mobile
- Backend: Flask + Python ML Stack
- –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: PostgreSQL
- –û—á–µ—Ä–µ–¥–∏: Celery + Redis
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: Prometheus + Grafana

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:

```
Flutter Clients (Web + Mobile)
        ‚Üì
    nginx (Load Balancer)
        ‚Üì
   Flask API Servers (3 instances)
        ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì       ‚Üì        ‚Üì         ‚Üì
  LLM    Vision   Audio    Custom ML
  GPT-2   ViT    Whisper   Sklearn
    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
   PostgreSQL (–∏—Å—Ç–æ—Ä–∏—è)
   Redis (–∫—ç—à + –æ—á–µ—Ä–µ–¥–∏)
   
   Celery Workers (—Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏)
```

### –°–µ—Ä–≤–∏—Å—ã:

**1. Text Service:**
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (GPT-2)
- –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è (BART)
- –ü–µ—Ä–µ–≤–æ–¥ (Helsinki-NLP)
- Sentiment Analysis

**2. Vision Service:**
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (ViT)
- Object Detection (YOLO)
- Face Recognition (FaceNet)
- Image Generation (Stable Diffusion)

**3. Audio Service:**
- Speech-to-Text (Whisper)
- Text-to-Speech (TTS)
- Audio Classification

**4. Custom ML Service:**
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
- Anomaly Detection
- Time Series Forecasting

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- ‚úÖ –í—Å–µ —Ç–∏–ø—ã ML –∑–∞–¥–∞—á
- ‚úÖ –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
- ‚úÖ –ü–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- ‚ùå –°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (1 –Ω–µ–¥–µ–ª—è)

---

## 7.3 –ü—Ä–æ–µ–∫—Ç: Edge AI –¥–ª—è IoT (Hybrid)

### –û–ø–∏—Å–∞–Ω–∏–µ:
–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å IoT —É—Å—Ç—Ä–æ–π—Å—Ç–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ AI.

### –°—Ç–µ–∫:
- Frontend: Flutter Mobile
- Local Backend: Ollama (–±—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏)
- Heavy Backend: Desktop Server (—Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏)
- Edge Devices: Raspberry Pi + TensorFlow Lite

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:

```
IoT Sensors ‚Üí Raspberry Pi (TFLite)
                ‚Üì
         Edge Processing
                ‚Üì
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚Üì                 ‚Üì
  Ollama            Desktop Server
 (quick)              (heavy)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
        Flutter Dashboard
```

### Workflow:

1. **–ë—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏** ‚Üí Ollama
   - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –ü—Ä–æ—Å—Ç—ã–µ –∞–ª–µ—Ä—Ç—ã

2. **–°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏** ‚Üí Desktop Server
   - –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
   - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
   - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

3. **Real-time** ‚Üí Edge (TFLite)
   - –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è
   - –†–∞–±–æ—Ç–∞ –±–µ–∑ —Å–µ—Ç–∏

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- ‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –æ—Ñ–ª–∞–π–Ω
- ‚úÖ Real-time –æ–±—Ä–∞–±–æ—Ç–∫–∞
- ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å

---

## –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É:

**–î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:**
‚Üí –ù–∞—á–Ω–∏—Ç–µ —Å **Ollama**
- –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- –ú–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤
- –°–æ–æ–±—â–µ—Å—Ç–≤–æ –ø–æ–¥–¥–µ—Ä–∂–∫–∏

**–î–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤:**
‚Üí **Ollama** –¥–ª—è MVP
‚Üí –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ **Desktop Server** –ø—Ä–∏ —Ä–æ—Å—Ç–µ

**–î–ª—è production:**
‚Üí **Desktop Server** + –æ–±–ª–∞–∫–æ
- –õ–æ–∫–∞–ª—å–Ω–æ: –±—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏
- –û–±–ª–∞–∫–æ: —Ç—è–∂–µ–ª—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

**–î–ª—è enterprise:**
‚Üí –ü–æ–ª–Ω—ã–π **Desktop Server** —Å—Ç–µ–∫
- –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
- –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã:

### –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
- Ollama: https://ollama.ai/docs
- Transformers: https://huggingface.co/docs/transformers
- Flask: https://flask.palletsprojects.com/
- Flutter HTTP: https://pub.dev/packages/http

### –°–æ–æ–±—â–µ—Å—Ç–≤–∞:
- Ollama Discord: https://discord.gg/ollama
- Hugging Face Forum: https://discuss.huggingface.co/
- r/LocalLLaMA: https://reddit.com/r/LocalLLaMA

### –ú–æ–¥–µ–ª–∏:
- Ollama Library: https://ollama.ai/library
- Hugging Face Hub: https://huggingface.co/models
- Model Zoo: https://modelzoo.co/

---

**–ê–≤—Ç–æ—Ä –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏:** AI/ML Integration Team  
**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** 2026-01-08  
**–õ–∏—Ü–µ–Ω–∑–∏—è:** Open Source (MIT)
