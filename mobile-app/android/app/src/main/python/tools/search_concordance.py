#!/usr/bin/env python3
"""
Advanced Concordance Search - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ
–§—É–Ω–∫—Ü–∏–∏:
- Fuzzy search —Å Levenshtein distance
- Regex search
- Boolean operators (AND, OR, NOT)
- Wildcard search (*, ?)
- Phrase search ("...")
- KWIC (Key Word In Context)
- Context highlighting
- Export results (JSON, TXT, CSV)
- Search statistics

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: grep, ack, ag, ripgrep, Elasticsearch
"""

import json
import sys
from pathlib import Path
import re
from collections import Counter, defaultdict
import math


class QueryParser:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""

    def __init__(self):
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been'
        }

    def tokenize(self, query):
        """
        –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å

        Args:
            query: —Å—Ç—Ä–æ–∫–∞ –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            list: —Ç–æ–∫–µ–Ω—ã
        """
        # –£–¥–∞–ª–∏—Ç—å —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å wildcards
        tokens = re.findall(r'\w+[\*\?]*|\*|\?', query.lower())
        return tokens

    def remove_stop_words(self, tokens):
        """
        –£–¥–∞–ª–∏—Ç—å stop words

        Args:
            tokens: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤

        Returns:
            list: —Ç–æ–∫–µ–Ω—ã –±–µ–∑ stop words
        """
        return [t for t in tokens if t not in self.stop_words]

    def apply_stemming(self, word):
        """
        –ü—Ä–æ—Å—Ç–æ–π —Å—Ç–µ–º–º–∏–Ω–≥ (—É–¥–∞–ª–µ–Ω–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏–π)

        Args:
            word: —Å–ª–æ–≤–æ

        Returns:
            str: –æ—Å–Ω–æ–≤–∞ —Å–ª–æ–≤–∞
        """
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
        if word.endswith('ing'):
            return word[:-3]
        elif word.endswith('ed'):
            return word[:-2]
        elif word.endswith('s') and len(word) > 3:
            return word[:-1]
        elif word.endswith('ly'):
            return word[:-2]

        return word

    def expand_query(self, query, synonyms=None):
        """
        –†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏

        Args:
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            synonyms: —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤

        Returns:
            list: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        """
        if not synonyms:
            # –ë–∞–∑–æ–≤—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            synonyms = {
                'docker': ['container', 'containerization'],
                'kubernetes': ['k8s', 'orchestration'],
                'python': ['py', 'python3'],
                'javascript': ['js', 'ecmascript'],
                'database': ['db', 'storage']
            }

        tokens = self.tokenize(query)
        expanded = set(tokens)

        for token in tokens:
            if token in synonyms:
                expanded.update(synonyms[token])

        return list(expanded)

    def parse_boolean_query(self, query):
        """
        –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å boolean –∑–∞–ø—Ä–æ—Å –≤ AST

        Args:
            query: –∑–∞–ø—Ä–æ—Å —Å AND/OR/NOT

        Returns:
            dict: AST –∑–∞–ø—Ä–æ—Å–∞
        """
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è AND/OR/NOT
        query = query.strip()

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
        if ' AND ' in query.upper():
            parts = re.split(r'\s+AND\s+', query, maxsplit=1, flags=re.IGNORECASE)
            return {
                'op': 'AND',
                'left': self.parse_boolean_query(parts[0]),
                'right': self.parse_boolean_query(parts[1])
            }
        elif ' OR ' in query.upper():
            parts = re.split(r'\s+OR\s+', query, maxsplit=1, flags=re.IGNORECASE)
            return {
                'op': 'OR',
                'left': self.parse_boolean_query(parts[0]),
                'right': self.parse_boolean_query(parts[1])
            }
        elif query.upper().startswith('NOT '):
            return {
                'op': 'NOT',
                'term': self.parse_boolean_query(query[4:])
            }
        else:
            return {
                'op': 'TERM',
                'value': query.strip().lower()
            }

    def optimize_query(self, query):
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å

        Args:
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            str: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å
        tokens = self.tokenize(query)

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        tokens = list(dict.fromkeys(tokens))

        # –£–¥–∞–ª–∏—Ç—å stop words (–µ—Å–ª–∏ –Ω–µ boolean –∑–∞–ø—Ä–æ—Å)
        if not any(op in query.upper() for op in [' AND ', ' OR ', ' NOT ']):
            tokens = self.remove_stop_words(tokens)

        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å—Ç–µ–º–º–∏–Ω–≥
        tokens = [self.apply_stemming(t) for t in tokens]

        return ' '.join(tokens)

    def suggest_corrections(self, query, concordance_words, max_suggestions=5):
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫

        Args:
            query: –∑–∞–ø—Ä–æ—Å
            concordance_words: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–∞
            max_suggestions: –º–∞–∫—Å–∏–º—É–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

        Returns:
            list: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        """
        from difflib import get_close_matches

        tokens = self.tokenize(query)
        suggestions = []

        for token in tokens:
            if token not in concordance_words:
                matches = get_close_matches(token, concordance_words, n=max_suggestions, cutoff=0.6)
                if matches:
                    suggestions.append({
                        'original': token,
                        'suggestions': matches
                    })

        return suggestions


class SearchRanker:
    """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, concordance, all_articles=None):
        self.concordance = concordance
        self.all_articles = all_articles or []

        # –í—ã—á–∏—Å–ª–∏—Ç—å IDF
        self.idf_scores = self._calculate_idf()

    def _calculate_idf(self):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å IDF (Inverse Document Frequency) –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

        Returns:
            dict: IDF scores
        """
        if not self.concordance:
            return {}

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        all_files = set()
        for entries in self.concordance.values():
            for entry in entries:
                all_files.add(entry['file'])

        total_docs = len(all_files)

        idf_scores = {}

        for word, entries in self.concordance.items():
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Å–ª–æ–≤–æ
            docs_with_word = len(set(entry['file'] for entry in entries))

            # IDF = log(N / df)
            idf = math.log(total_docs / (1 + docs_with_word))
            idf_scores[word] = idf

        return idf_scores

    def calculate_tf_idf(self, word, file_path):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ

        Args:
            word: —Å–ª–æ–≤–æ
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            float: TF-IDF score
        """
        if word not in self.concordance:
            return 0.0

        # TF: —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        term_freq = sum(1 for entry in self.concordance[word] if entry['file'] == file_path)

        # IDF
        idf = self.idf_scores.get(word, 0.0)

        return term_freq * idf

    def calculate_bm25(self, query_terms, document, k1=1.5, b=0.75):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å BM25 score –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞

        BM25 - –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–µ—Ä—Å–∏—è TF-IDF.

        Args:
            query_terms: —Ç–µ—Ä–º–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞
            document: –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É
            k1: –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–∞—Å—ã—â–µ–Ω–∏—è TF
            b: –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω—ã

        Returns:
            float: BM25 score
        """
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
        avg_doc_length = 1000  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É

        # –î–ª–∏–Ω–∞ —Ç–µ–∫—É—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤)
        doc_length = sum(
            len(entries)
            for entries in self.concordance.values()
            if any(e['file'] == document for e in entries)
        )

        score = 0.0

        for term in query_terms:
            if term not in self.concordance:
                continue

            # TF –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            tf = sum(1 for entry in self.concordance[term] if entry['file'] == document)

            # IDF
            idf = self.idf_scores.get(term, 0.0)

            # BM25 —Ñ–æ—Ä–º—É–ª–∞
            numerator = tf * (k1 + 1)
            denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_length))

            score += idf * (numerator / denominator)

        return score

    def rank_results(self, results, query_terms, method='tfidf'):
        """
        –†–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞

        Args:
            results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            query_terms: —Ç–µ—Ä–º–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞
            method: –º–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è (tfidf, bm25, frequency)

        Returns:
            list: —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        scored_results = []

        for word, entry in results:
            if method == 'tfidf':
                score = self.calculate_tf_idf(word, entry['file'])
            elif method == 'bm25':
                score = self.calculate_bm25(query_terms, entry['file'])
            elif method == 'frequency':
                # –ü—Ä–æ—Å—Ç–∞—è —á–∞—Å—Ç–æ—Ç–∞
                score = sum(1 for e in self.concordance.get(word, []) if e['file'] == entry['file'])
            else:
                score = 0.0

            scored_results.append((score, word, entry))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ score (descending)
        scored_results.sort(key=lambda x: -x[0])

        # –í–µ—Ä–Ω—É—Ç—å –±–µ–∑ score
        return [(word, entry) for score, word, entry in scored_results]


class SearchIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""

    def __init__(self):
        self.inverted_index = defaultdict(set)
        self.ngram_index = defaultdict(set)
        self.word_positions = defaultdict(list)

    def build_inverted_index(self, concordance):
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å

        Args:
            concordance: –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å

        Returns:
            dict: –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        """
        self.inverted_index = defaultdict(set)

        for word, entries in concordance.items():
            for entry in entries:
                self.inverted_index[word].add(entry['file'])

        return dict(self.inverted_index)

    def build_ngram_index(self, concordance, n=3):
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å n-gram –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ fuzzy search

        Args:
            concordance: –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å
            n: —Ä–∞–∑–º–µ—Ä n-gram

        Returns:
            dict: n-gram –∏–Ω–¥–µ–∫—Å
        """
        self.ngram_index = defaultdict(set)

        for word in concordance.keys():
            # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å n-grams
            ngrams = self._generate_ngrams(word, n)

            for ngram in ngrams:
                self.ngram_index[ngram].add(word)

        return dict(self.ngram_index)

    def _generate_ngrams(self, word, n):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å n-grams –¥–ª—è —Å–ª–æ–≤–∞

        Args:
            word: —Å–ª–æ–≤–æ
            n: —Ä–∞–∑–º–µ—Ä n-gram

        Returns:
            list: n-grams
        """
        word = f'${word}$'  # –î–æ–±–∞–≤–∏—Ç—å –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞
        ngrams = []

        for i in range(len(word) - n + 1):
            ngrams.append(word[i:i+n])

        return ngrams

    def fuzzy_search_with_ngrams(self, query, min_similarity=0.5):
        """
        Fuzzy search –∏—Å–ø–æ–ª—å–∑—É—è n-gram –∏–Ω–¥–µ–∫—Å

        Args:
            query: –∑–∞–ø—Ä–æ—Å
            min_similarity: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ö–æ–∂–µ—Å—Ç—å

        Returns:
            list: –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
        """
        if not self.ngram_index:
            return []

        query_ngrams = set(self._generate_ngrams(query, 3))

        candidates = defaultdict(int)

        # –ù–∞–π—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ n-grams
        for ngram in query_ngrams:
            if ngram in self.ngram_index:
                for word in self.ngram_index[ngram]:
                    candidates[word] += 1

        # –í—ã—á–∏—Å–ª–∏—Ç—å Jaccard similarity
        results = []

        for word, ngram_matches in candidates.items():
            word_ngrams = set(self._generate_ngrams(word, 3))
            similarity = len(query_ngrams & word_ngrams) / len(query_ngrams | word_ngrams)

            if similarity >= min_similarity:
                results.append((word, similarity))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ similarity
        results.sort(key=lambda x: -x[1])

        return results

    def build_position_index(self, concordance):
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è phrase search

        Args:
            concordance: –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å

        Returns:
            dict: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        """
        self.word_positions = defaultdict(list)

        for word, entries in concordance.items():
            for entry in entries:
                self.word_positions[word].append({
                    'file': entry['file'],
                    'line': entry['line'],
                    'context': entry['context']
                })

        return dict(self.word_positions)

    def phrase_search(self, phrase):
        """
        –ü–æ–∏—Å–∫ —Ñ—Ä–∞–∑—ã (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤)

        Args:
            phrase: —Ñ—Ä–∞–∑–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞

        Returns:
            list: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        words = phrase.lower().split()

        if not words:
            return []

        # –ù–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –≤—Å–µ —Å–ª–æ–≤–∞
        if words[0] not in self.word_positions:
            return []

        results = []

        # –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞
        for pos in self.word_positions[words[0]]:
            context = pos['context'].lower()

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ context –≤—Å—é —Ñ—Ä–∞–∑—É
            if phrase.lower() in context:
                results.append(pos)

        return results


class SearchVisualizer:
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""

    def __init__(self, results, query):
        self.results = results
        self.query = query

    def generate_html_results(self):
        """
        –°–æ–∑–¥–∞—Ç—å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            str: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        total_results = len(self.results)

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ñ–∞–π–ª–∞–º
        by_file = defaultdict(list)
        for word, entry in self.results:
            by_file[entry['file']].append((word, entry))

        # –¢–æ–ø —Ñ–∞–π–ª–æ–≤
        top_files = sorted(by_file.items(), key=lambda x: -len(x[1]))[:10]

        # –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤
        word_freq = Counter(word for word, _ in self.results)
        top_words = word_freq.most_common(10)

        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üîç Search Results: {self.query}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}

        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}

        .container {{
            max-width: 1200px;
            margin: 0 auto;
        }}

        h1 {{
            color: white;
            text-align: center;
            margin-bottom: 20px;
            font-size: 2.5em;
            text-shadow: 0 2px 10px rgba(0,0,0,0.3);
        }}

        .query-box {{
            background: white;
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            margin-bottom: 20px;
            text-align: center;
        }}

        .query {{
            font-size: 1.5em;
            font-weight: bold;
            color: #667eea;
        }}

        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 20px;
        }}

        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            text-align: center;
        }}

        .stat-value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #667eea;
        }}

        .stat-label {{
            color: #666;
            margin-top: 10px;
        }}

        .results-section {{
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            margin-bottom: 20px;
        }}

        .section-title {{
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
        }}

        .result-item {{
            padding: 15px;
            border-left: 4px solid #667eea;
            background: #f8f9fa;
            margin-bottom: 10px;
            border-radius: 5px;
        }}

        .file-path {{
            font-weight: 600;
            color: #667eea;
            margin-bottom: 5px;
        }}

        .context {{
            color: #333;
            line-height: 1.6;
        }}

        .highlight {{
            background: yellow;
            font-weight: bold;
            padding: 2px 4px;
        }}

        .word-cloud {{
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            justify-content: center;
        }}

        .word-tag {{
            background: #667eea;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: calc(12px + var(--size) * 8px);
        }}

        .file-list {{
            list-style: none;
        }}

        .file-list li {{
            padding: 10px;
            border-bottom: 1px solid #eee;
            display: flex;
            justify-content: space-between;
        }}

        .file-list li:last-child {{
            border-bottom: none;
        }}

        .badge {{
            background: #764ba2;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üîç Search Results</h1>

        <div class="query-box">
            <div class="query">"{self.query}"</div>
        </div>

        <div class="stats">
            <div class="stat-card">
                <div class="stat-value">{total_results}</div>
                <div class="stat-label">–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{len(by_file)}</div>
                <div class="stat-label">–§–∞–π–ª–æ–≤</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{len(word_freq)}</div>
                <div class="stat-label">–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤</div>
            </div>
        </div>

        <div class="results-section">
            <div class="section-title">‚òÅÔ∏è –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞</div>
            <div class="word-cloud">
                {"".join(f'<span class="word-tag" style="--size: {min(count/max(word_freq.values()), 1)}">{word} ({count})</span>' for word, count in top_words)}
            </div>
        </div>

        <div class="results-section">
            <div class="section-title">üìÅ –¢–æ–ø —Ñ–∞–π–ª–æ–≤</div>
            <ul class="file-list">
                {"".join(f'<li><span>{file_path}</span><span class="badge">{len(items)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π</span></li>' for file_path, items in top_files)}
            </ul>
        </div>

        <div class="results-section">
            <div class="section-title">üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–µ—Ä–≤—ã–µ 20)</div>
            {"".join(f'''
            <div class="result-item">
                <div class="file-path">{entry['file']}:{entry['line']}</div>
                <div class="context">{entry['context'].replace(word, f'<span class="highlight">{word}</span>')}</div>
            </div>
            ''' for word, entry in self.results[:20])}
        </div>
    </div>
</body>
</html>"""

        return html

    def generate_word_cloud_data(self):
        """
        –°–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è word cloud

        Returns:
            dict: –¥–∞–Ω–Ω—ã–µ –¥–ª—è word cloud
        """
        word_freq = Counter(word for word, _ in self.results)

        return {
            'words': [
                {'text': word, 'size': count}
                for word, count in word_freq.most_common(50)
            ]
        }

    def export_to_markdown(self):
        """
        –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Markdown

        Returns:
            str: Markdown –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        lines = []

        lines.append(f"# üîç Search Results: {self.query}\n\n")
        lines.append(f"**Total Results:** {len(self.results)}\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ñ–∞–π–ª–∞–º
        by_file = defaultdict(list)
        for word, entry in self.results:
            by_file[entry['file']].append((word, entry))

        lines.append("## üìÅ By Files\n\n")

        for file_path, items in sorted(by_file.items(), key=lambda x: -len(x[1]))[:20]:
            lines.append(f"### {file_path} ({len(items)} matches)\n\n")

            for word, entry in items[:5]:
                lines.append(f"- **Line {entry['line']}**: {entry['context']}\n")

            if len(items) > 5:
                lines.append(f"\n_...and {len(items) - 5} more matches_\n")

            lines.append("\n")

        return ''.join(lines)


class AdvancedConcordanceSearch:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ"""

    def __init__(self, concordance_file):
        self.concordance_file = concordance_file
        self.concordance = None
        self.load_concordance()

    def load_concordance(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å"""
        if not self.concordance_file.exists():
            print("‚ùå –ö–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞:")
            print("   python tools/build_concordance.py")
            return False

        with open(self.concordance_file, 'r', encoding='utf-8') as f:
            self.concordance = json.load(f)

        return True

    def levenshtein_distance(self, s1, s2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞"""
        if len(s1) < len(s2):
            return self.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)

        for i, c1 in enumerate(s1):
            current_row = [i + 1]

            for j, c2 in enumerate(s2):
                # –°—Ç–æ–∏–º–æ—Å—Ç—å –≤—Å—Ç–∞–≤–∫–∏, —É–¥–∞–ª–µ–Ω–∏—è, –∑–∞–º–µ–Ω—ã
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)

                current_row.append(min(insertions, deletions, substitutions))

            previous_row = current_row

        return previous_row[-1]

    def fuzzy_search(self, word, max_distance=2):
        """–ù–µ—á—ë—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å Levenshtein distance"""
        if not self.concordance:
            return []

        word_lower = word.lower()
        matches = []

        for concordance_word in self.concordance.keys():
            distance = self.levenshtein_distance(word_lower, concordance_word)

            if distance <= max_distance:
                matches.append((concordance_word, distance))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ distance
        matches.sort(key=lambda x: x[1])

        return matches

    def regex_search(self, pattern):
        """–ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é"""
        if not self.concordance:
            return []

        try:
            regex = re.compile(pattern, re.IGNORECASE)
        except re.error as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ regex: {e}")
            return []

        matches = []

        for word, entries in self.concordance.items():
            if regex.search(word):
                matches.append((word, entries))

        return matches

    def wildcard_search(self, pattern):
        """–ü–æ–∏—Å–∫ —Å wildcards (* –∏ ?)"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å wildcard –≤ regex
        regex_pattern = pattern.replace('*', '.*').replace('?', '.')
        regex_pattern = '^' + regex_pattern + '$'

        return self.regex_search(regex_pattern)

    def boolean_search(self, query):
        """Boolean –ø–æ–∏—Å–∫ (AND, OR, NOT)"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è: —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ AND/OR/NOT
        # –ü—Ä–∏–º–µ—Ä: "docker AND python" –∏–ª–∏ "docker OR kubernetes"

        if ' AND ' in query.upper():
            words = [w.strip().lower() for w in re.split(r'\s+AND\s+', query, flags=re.IGNORECASE)]
            return self._search_and(words)

        elif ' OR ' in query.upper():
            words = [w.strip().lower() for w in re.split(r'\s+OR\s+', query, flags=re.IGNORECASE)]
            return self._search_or(words)

        elif ' NOT ' in query.upper():
            parts = re.split(r'\s+NOT\s+', query, flags=re.IGNORECASE)
            if len(parts) == 2:
                include = parts[0].strip().lower()
                exclude = parts[1].strip().lower()
                return self._search_not(include, exclude)

        # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
        return self.exact_search(query)

    def _search_and(self, words):
        """–ü–æ–∏—Å–∫ —Å AND - –≤—Å–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å"""
        if not self.concordance:
            return []

        # –ù–∞–π—Ç–∏ –æ–±—â–∏–µ —Ñ–∞–π–ª—ã –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤
        file_sets = []

        for word in words:
            if word in self.concordance:
                files = set(entry['file'] for entry in self.concordance[word])
                file_sets.append(files)

        if not file_sets:
            return []

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–Ω–æ–∂–µ—Å—Ç–≤
        common_files = file_sets[0]
        for file_set in file_sets[1:]:
            common_files &= file_set

        # –°–æ–±—Ä–∞—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ –æ–±—â–∏—Ö —Ñ–∞–π–ª–æ–≤
        results = []
        for word in words:
            if word in self.concordance:
                for entry in self.concordance[word]:
                    if entry['file'] in common_files:
                        results.append((word, entry))

        return results

    def _search_or(self, words):
        """–ü–æ–∏—Å–∫ —Å OR - –ª—é–±–æ–µ —Å–ª–æ–≤–æ"""
        if not self.concordance:
            return []

        results = []

        for word in words:
            if word in self.concordance:
                for entry in self.concordance[word]:
                    results.append((word, entry))

        return results

    def _search_not(self, include_word, exclude_word):
        """–ü–æ–∏—Å–∫ —Å NOT - –∏—Å–∫–ª—é—á–∏—Ç—å —Å–ª–æ–≤–æ"""
        if not self.concordance:
            return []

        # –§–∞–π–ª—ã —Å exclude_word
        exclude_files = set()
        if exclude_word in self.concordance:
            exclude_files = set(entry['file'] for entry in self.concordance[exclude_word])

        # –ò—Å–∫–∞—Ç—å include_word, –Ω–æ –Ω–µ –≤ exclude_files
        results = []
        if include_word in self.concordance:
            for entry in self.concordance[include_word]:
                if entry['file'] not in exclude_files:
                    results.append((include_word, entry))

        return results

    def exact_search(self, word):
        """–¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å–ª–æ–≤–∞"""
        if not self.concordance:
            return []

        word_lower = word.lower()

        if word_lower not in self.concordance:
            return []

        entries = self.concordance[word_lower]
        return [(word_lower, entry) for entry in entries]

    def highlight_context(self, context, word):
        """–ü–æ–¥—Å–≤–µ—Ç–∏—Ç—å —Å–ª–æ–≤–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ANSI escape codes –¥–ª—è —Ü–≤–µ—Ç–∞
        highlighted = re.sub(
            f'({re.escape(word)})',
            r'\033[1;31m\1\033[0m',  # –ö—Ä–∞—Å–Ω—ã–π —Ü–≤–µ—Ç
            context,
            flags=re.IGNORECASE
        )
        return highlighted

    def kwic_display(self, results, context_width=40):
        """KWIC (Key Word In Context) –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
        print("\n" + "=" * 80)
        print("KWIC Display".center(80))
        print("=" * 80 + "\n")

        for word, entry in results[:50]:
            context = entry['context']

            # –ù–∞–π—Ç–∏ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            match = re.search(re.escape(word), context, re.IGNORECASE)

            if match:
                start = match.start()
                end = match.end()

                # –í—ã—Ä–µ–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞
                left_context = context[max(0, start - context_width):start]
                keyword = context[start:end]
                right_context = context[end:min(len(context), end + context_width)]

                # –í—ã—Ä–æ–≤–Ω—è—Ç—å
                print(f"{left_context:>{context_width}} ", end='')
                print(f"\033[1;31m{keyword}\033[0m", end='')
                print(f" {right_context:<{context_width}}")
                print(f"  ‚Üí {entry['file']}:{entry['line']}\n")

    def generate_statistics(self, results):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        if not results:
            return

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã
        files = Counter(entry['file'] for _, entry in results)

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≤–∞
        words = Counter(word for word, _ in results)

        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞:\n")
        print(f"   –í—Å–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(results)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(files)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(words)}\n")

        print("   –¢–æ–ø-5 —Ñ–∞–π–ª–æ–≤:")
        for file, count in files.most_common(5):
            print(f"      {file}: {count}")

        if len(words) > 1:
            print("\n   –¢–æ–ø-5 —Å–ª–æ–≤:")
            for word, count in words.most_common(5):
                print(f"      {word}: {count}")

    def export_results(self, results, output_format='txt', output_file=None):
        """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not results:
            print("   –ù–µ—á–µ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å")
            return

        if output_format == 'json':
            data = [
                {'word': word, **entry}
                for word, entry in results
            ]

            output = json.dumps(data, ensure_ascii=False, indent=2)

        elif output_format == 'csv':
            lines = ['word,file,line,context']
            for word, entry in results:
                context = entry['context'].replace('"', '""')
                lines.append(f'"{word}","{entry["file"]}",{entry["line"]},"{context}"')

            output = '\n'.join(lines)

        else:  # txt
            lines = []
            for word, entry in results:
                lines.append(f"Word: {word}")
                lines.append(f"File: {entry['file']}:{entry['line']}")
                lines.append(f"Context: {entry['context']}")
                lines.append("")

            output = '\n'.join(lines)

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(output)
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_file}")
        else:
            print(output)

    def search(self, query, mode='exact', max_results=50, display='list', export=None):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫"""
        if not self.concordance:
            return

        # –í—ã–±—Ä–∞—Ç—å —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
        if mode == 'fuzzy':
            matches = self.fuzzy_search(query)
            results = []
            for word, distance in matches:
                for entry in self.concordance[word]:
                    results.append((word, entry))
                    if len(results) >= max_results:
                        break

        elif mode == 'regex':
            matches = self.regex_search(query)
            results = []
            for word, entries in matches:
                for entry in entries:
                    results.append((word, entry))

        elif mode == 'wildcard':
            matches = self.wildcard_search(query)
            results = []
            for word, entries in matches:
                for entry in entries:
                    results.append((word, entry))

        elif mode == 'boolean':
            results = self.boolean_search(query)

        else:  # exact
            results = self.exact_search(query)

        if not results:
            print(f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è '{query}'")

            # –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
            fuzzy_matches = self.fuzzy_search(query, max_distance=2)
            if fuzzy_matches:
                print(f"\n–ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞:")
                for word, distance in fuzzy_matches[:10]:
                    print(f"  - {word} (distance: {distance})")

            return

        print(f"\nüìñ –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n")

        # –û—Ç–æ–±—Ä–∞–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if display == 'kwic':
            self.kwic_display(results)
        else:
            for i, (word, entry) in enumerate(results[:max_results], 1):
                print(f"{i}. {entry['file']}:{entry['line']}")
                highlighted = self.highlight_context(entry['context'], word)
                print(f"   {highlighted}\n")

            if len(results) > max_results:
                print(f"   ...–∏ –µ—â—ë {len(results) - max_results} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.generate_statistics(results)

        # –≠–∫—Å–ø–æ—Ä—Ç
        if export:
            self.export_results(results, export_format=export['format'], output_file=export.get('file'))


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='üîç Advanced Concordance Search - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s docker                        # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
  %(prog)s 'docker*' -m wildcard         # Wildcard search
  %(prog)s 'doc.*' -m regex              # Regex search
  %(prog)s 'docker AND python' -m boolean # Boolean search
  %(prog)s —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ -m fuzzy          # Fuzzy search
  %(prog)s docker -d kwic                # KWIC display
  %(prog)s docker --html                 # HTML visualization
  %(prog)s docker --rank bm25            # Ranked results with BM25
  %(prog)s docker --optimize             # Optimize query
  %(prog)s docker --expand               # Expand with synonyms
  %(prog)s "docker container" --phrase   # Phrase search
  %(prog)s docker --suggest              # Suggest corrections
  %(prog)s docker --all                  # All features

–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
  ‚Ä¢ Query Optimization: stemming, stop words removal
  ‚Ä¢ Query Expansion: synonym expansion
  ‚Ä¢ Advanced Ranking: TF-IDF, BM25
  ‚Ä¢ Fast Indexing: inverted index, n-gram index
  ‚Ä¢ HTML Visualization: word clouds, interactive results
        """
    )

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('query', nargs='?', help='–°–ª–æ–≤–æ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞')
    parser.add_argument('-m', '--mode', choices=['exact', 'fuzzy', 'regex', 'wildcard', 'boolean'],
                       default='exact', help='–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞')
    parser.add_argument('-d', '--display', choices=['list', 'kwic'], default='list',
                       help='–§–æ—Ä–º–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è')
    parser.add_argument('-n', '--max-results', type=int, default=50,
                       help='–ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50)')

    # –ù–æ–≤—ã–µ –∞–Ω–∞–ª–∏–∑—ã
    parser.add_argument('--html', action='store_true',
                       help='üé® –°–æ–∑–¥–∞—Ç—å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--rank', choices=['tfidf', 'bm25', 'frequency'],
                       help='üìä –ú–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--optimize', action='store_true',
                       help='‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å (stemming, stop words)')
    parser.add_argument('--expand', action='store_true',
                       help='üîÑ –†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏')
    parser.add_argument('--phrase', action='store_true',
                       help='üîç Phrase search (–ø–æ–∏—Å–∫ —Ñ—Ä–∞–∑—ã)')
    parser.add_argument('--suggest', action='store_true',
                       help='üí° –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫')
    parser.add_argument('--build-index', action='store_true',
                       help='üèóÔ∏è –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--ngram-fuzzy', action='store_true',
                       help='üéØ Fuzzy search —Å n-gram –∏–Ω–¥–µ–∫—Å–æ–º (–±—ã—Å—Ç—Ä–µ–µ)')

    # –≠–∫—Å–ø–æ—Ä—Ç
    parser.add_argument('-e', '--export', choices=['txt', 'json', 'csv', 'md'],
                       help='üíæ –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞')
    parser.add_argument('-o', '--output', help='üìÅ –§–∞–π–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞')

    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ
    parser.add_argument('--all', action='store_true',
                       help='üéØ –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã')

    args = parser.parse_args()

    if not args.query:
        parser.print_help()
        return

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent
    concordance_file = root_dir / "concordance.json"

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å searcher
    searcher = AdvancedConcordanceSearch(concordance_file)

    if not searcher.concordance:
        return

    print(f"üîç –ü–æ–∏—Å–∫: '{args.query}'")
    print(f"üìö –ö–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å: {len(searcher.concordance)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n")

    # --all –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏
    if args.all:
        args.optimize = True
        args.expand = True
        args.html = True
        args.rank = 'bm25'
        args.suggest = True
        args.build_index = True

    # Query Parser
    query_parser = QueryParser()

    # Optimize query
    original_query = args.query
    if args.optimize:
        optimized = query_parser.optimize_query(args.query)
        print(f"‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{optimized}'")
        args.query = optimized

    # Expand query
    expanded_terms = []
    if args.expand:
        expanded_terms = query_parser.expand_query(args.query)
        print(f"üîÑ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {', '.join(expanded_terms)}")

    # Suggest corrections
    if args.suggest:
        suggestions = query_parser.suggest_corrections(args.query, list(searcher.concordance.keys()))
        if suggestions:
            print("\nüí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é:")
            for sug in suggestions:
                print(f"   '{sug['original']}' ‚Üí {', '.join(sug['suggestions'][:3])}")
        print()

    # Build indexes
    indexer = None
    if args.build_index or args.ngram_fuzzy:
        print("üèóÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")
        indexer = SearchIndexer()
        indexer.build_inverted_index(searcher.concordance)
        indexer.build_ngram_index(searcher.concordance)
        indexer.build_position_index(searcher.concordance)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã\n")

    # Phrase search
    if args.phrase and indexer:
        print(f"üîç Phrase search: '{original_query}'")
        results_list = indexer.phrase_search(original_query)

        if results_list:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(results_list)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n")
            for i, result in enumerate(results_list[:args.max_results], 1):
                print(f"{i}. {result['file']}:{result['line']}")
                print(f"   {result['context']}\n")
        else:
            print("‚ùå –§—Ä–∞–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n")

        return

    # N-gram fuzzy search
    if args.ngram_fuzzy and indexer:
        print(f"üéØ N-gram fuzzy search: '{args.query}'")
        similar_words = indexer.fuzzy_search_with_ngrams(args.query)

        if similar_words:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤: {len(similar_words)}\n")
            results = []
            for word, similarity in similar_words[:20]:
                print(f"   {word} (similarity: {similarity:.2f})")
                if word in searcher.concordance:
                    for entry in searcher.concordance[word]:
                        results.append((word, entry))

            print(f"\nüìä –í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\n")
        else:
            print("‚ùå –ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n")

        return

    # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫
    if args.mode == 'fuzzy':
        matches = searcher.fuzzy_search(args.query)
        results = []
        for word, distance in matches:
            for entry in searcher.concordance[word]:
                results.append((word, entry))
                if len(results) >= args.max_results * 2:  # –ü–æ–ª—É—á–∏—Ç—å –±–æ–ª—å—à–µ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
                    break

    elif args.mode == 'regex':
        matches = searcher.regex_search(args.query)
        results = []
        for word, entries in matches:
            for entry in entries:
                results.append((word, entry))

    elif args.mode == 'wildcard':
        matches = searcher.wildcard_search(args.query)
        results = []
        for word, entries in matches:
            for entry in entries:
                results.append((word, entry))

    elif args.mode == 'boolean':
        results = searcher.boolean_search(args.query)

    else:  # exact
        results = searcher.exact_search(args.query)

        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        if args.expand and not results:
            for term in expanded_terms:
                term_results = searcher.exact_search(term)
                results.extend(term_results)

    if not results:
        print(f"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è '{args.query}'")

        # Fuzzy matches –∫–∞–∫ fallback
        fuzzy_matches = searcher.fuzzy_search(args.query, max_distance=2)
        if fuzzy_matches:
            print(f"\nüí° –ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞:")
            for word, distance in fuzzy_matches[:10]:
                print(f"   {word} (distance: {distance})")

        return

    print(f"üìñ –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n")

    # Ranking
    if args.rank:
        print(f"üìä –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º: {args.rank.upper()}")
        ranker = SearchRanker(searcher.concordance)
        query_terms = query_parser.tokenize(args.query)
        results = ranker.rank_results(results, query_terms, method=args.rank)
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω—ã\n")

    # HTML visualization
    if args.html:
        print("üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
        visualizer = SearchVisualizer(results, original_query)
        html_content = visualizer.generate_html_results()

        html_file = root_dir / f"search_results_{original_query.replace(' ', '_')}.html"
        html_file.write_text(html_content, encoding='utf-8')
        print(f"‚úÖ HTML: {html_file}\n")

    # Display results
    if args.display == 'kwic':
        searcher.kwic_display(results)
    else:
        for i, (word, entry) in enumerate(results[:args.max_results], 1):
            print(f"{i}. {entry['file']}:{entry['line']}")
            highlighted = searcher.highlight_context(entry['context'], word)
            print(f"   {highlighted}\n")

        if len(results) > args.max_results:
            print(f"   ...–∏ –µ—â—ë {len(results) - args.max_results} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")

    # Statistics
    searcher.generate_statistics(results)

    # Export
    if args.export:
        if args.export == 'md':
            # Markdown export —á–µ—Ä–µ–∑ visualizer
            visualizer = SearchVisualizer(results, original_query)
            md_content = visualizer.export_to_markdown()

            if args.output:
                Path(args.output).write_text(md_content, encoding='utf-8')
                print(f"\n‚úÖ Markdown —ç–∫—Å–ø–æ—Ä—Ç: {args.output}")
            else:
                print("\n" + md_content)
        else:
            export_config = {
                'format': args.export,
                'file': args.output
            }
            searcher.export_results(results, output_format=args.export, output_file=args.output)


if __name__ == "__main__":
    main()
