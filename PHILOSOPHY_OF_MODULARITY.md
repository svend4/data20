# ğŸ§© Ğ¤Ğ˜Ğ›ĞĞ¡ĞĞ¤Ğ˜Ğ¯ ĞœĞĞ”Ğ£Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ˜: ĞÑ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² Ğ´Ğ¾ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

## ğŸ“‹ Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ

1. [Ğ¤Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²](#Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ-Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²)
2. [ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹ Ğ¸ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ](#ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹-Ğ¸-Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ)
3. [Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹ Flutter: Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸](#Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹-flutter)
4. [AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ RAG](#ai-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹-Ğ¸-rag)
5. [Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ: Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸](#ĞµĞ´Ğ¸Ğ½Ğ°Ñ-Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ)
6. [Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ AI Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ](#Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ-ai-Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ)
7. [ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°](#Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°)
8. [Ğ‘ÑƒĞ´ÑƒÑ‰ĞµĞµ: ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ AI Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²](#Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ)

---

# 1. Ğ¤Ğ˜Ğ›ĞĞ¡ĞĞ¤Ğ˜Ğ¯ ĞœĞ˜ĞšĞ ĞĞ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ĞĞ’

## 1.1 Ğ¡ÑƒÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸

### ĞœĞ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚ vs ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹

```
ĞœĞĞĞĞ›Ğ˜Ğ¢ (ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                     â”‚
â”‚    ĞĞ“Ğ ĞĞœĞĞĞ• ĞŸĞ Ğ˜Ğ›ĞĞ–Ğ•ĞĞ˜Ğ•              â”‚
â”‚    - Ğ’ÑĞµ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼                    â”‚
â”‚    - ĞĞ´Ğ¸Ğ½ ĞºĞ¾Ğ´                       â”‚
â”‚    - ĞĞ´Ğ½Ğ° Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…               â”‚
â”‚    - ĞĞ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ                   â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹:
âŒ Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
âŒ ĞĞ´Ğ¸Ğ½ Ğ±Ğ°Ğ³ = Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²ÑÑ‘
âŒ Ğ”Ğ¾Ğ»Ğ³Ğ¸Ğ¹ Ğ´ĞµĞ¿Ğ»Ğ¾Ğ¹
âŒ Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ°


ĞœĞ˜ĞšĞ ĞĞ¡Ğ•Ğ Ğ’Ğ˜Ğ¡Ğ« (ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Auth   â”‚  â”‚ Users  â”‚  â”‚ Orders â”‚  â”‚ Paymentâ”‚
â”‚ Serviceâ”‚  â”‚ Serviceâ”‚  â”‚ Serviceâ”‚  â”‚ Serviceâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚           â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            API Gateway / Message Bus

ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:
âœ… ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
âœ… Ğ˜Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
âœ… Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¿Ğ»Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
âœ… Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ° Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹
```

---

## 1.2 Flask Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ½Ğ° Python

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹:

```python
# SERVICE 1: Authentication Microservice (Port 5001)
# auth_service.py

from flask import Flask, request, jsonify
import jwt
import redis

app = Flask(__name__)
redis_client = redis.Redis(host='redis', port=6379)

@app.route('/auth/login', methods=['POST'])
def login():
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ·Ğ° Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ"""
    username = request.json['username']
    password = request.json['password']

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° credentials
    if validate_credentials(username, password):
        token = jwt.encode({'user': username}, SECRET)

        # ĞšÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Redis
        redis_client.setex(f"session:{username}", 3600, token)

        return jsonify({'token': token}), 200

    return jsonify({'error': 'Invalid credentials'}), 401

@app.route('/auth/verify', methods=['POST'])
def verify_token():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°"""
    token = request.json['token']
    try:
        payload = jwt.decode(token, SECRET)
        return jsonify({'valid': True, 'user': payload['user']}), 200
    except:
        return jsonify({'valid': False}), 401

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)


# SERVICE 2: User Management Microservice (Port 5002)
# user_service.py

from flask import Flask, request, jsonify
import pymongo

app = Flask(__name__)
db = pymongo.MongoClient('mongodb://mongo:27017')['users_db']

@app.route('/users', methods=['GET'])
def get_users():
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ·Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸"""
    users = list(db.users.find({}, {'_id': 0}))
    return jsonify(users), 200

@app.route('/users/<user_id>', methods=['GET'])
def get_user(user_id):
    user = db.users.find_one({'id': user_id}, {'_id': 0})
    if user:
        return jsonify(user), 200
    return jsonify({'error': 'User not found'}), 404

@app.route('/users', methods=['POST'])
def create_user():
    # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Auth Service
    token = request.headers.get('Authorization')

    # ĞœĞµĞ¶ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ
    auth_response = requests.post('http://auth-service:5001/auth/verify',
                                  json={'token': token})

    if auth_response.status_code != 200:
        return jsonify({'error': 'Unauthorized'}), 401

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    user_data = request.json
    db.users.insert_one(user_data)

    return jsonify({'status': 'created'}), 201

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5002)


# SERVICE 3: ML Inference Microservice (Port 5003)
# ml_service.py

from flask import Flask, request, jsonify
from transformers import pipeline

app = Flask(__name__)

# ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ²Ğ¾Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
llm_pipeline = pipeline("text-generation", model="gpt2")
classifier_pipeline = pipeline("sentiment-analysis")

@app.route('/ml/generate', methods=['POST'])
def generate_text():
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ·Ğ° ML Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ"""
    prompt = request.json['prompt']
    result = llm_pipeline(prompt, max_length=100)
    return jsonify({'text': result[0]['generated_text']}), 200

@app.route('/ml/analyze-sentiment', methods=['POST'])
def analyze_sentiment():
    """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¹ - Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ endpoint"""
    text = request.json['text']
    result = classifier_pipeline(text)
    return jsonify(result), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5003)


# SERVICE 4: Data Processing Microservice (Port 5004)
# data_service.py

from flask import Flask, request, jsonify
import pandas as pd
from celery import Celery

app = Flask(__name__)
celery_app = Celery('data_service', broker='redis://redis:6379/0')

@celery_app.task
def process_large_dataset(file_path):
    """Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    df = pd.read_csv(file_path)
    result = df.groupby('category').agg({
        'value': ['sum', 'mean', 'count']
    })
    return result.to_dict()

@app.route('/data/process', methods=['POST'])
def start_processing():
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ·Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    file_path = request.json['file_path']

    # Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸
    task = process_large_dataset.delay(file_path)

    return jsonify({
        'task_id': task.id,
        'status': 'processing'
    }), 202

@app.route('/data/status/<task_id>', methods=['GET'])
def get_status(task_id):
    task = celery_app.AsyncResult(task_id)
    return jsonify({
        'status': task.state,
        'result': task.result if task.ready() else None
    }), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5004)
```

### ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²:

1. **Single Responsibility** - Ğ¾Ğ´Ğ¸Ğ½ ÑĞµÑ€Ğ²Ğ¸Ñ = Ğ¾Ğ´Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
2. **ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ** - ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾
3. **Ğ˜Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…** - ÑĞ²Ğ¾Ñ Ğ‘Ğ” Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
4. **ĞšĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· API** - REST, gRPC, Ğ¸Ğ»Ğ¸ message queue
5. **Decentralized** - Ğ½ĞµÑ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°

---

# 2. ĞšĞĞĞ¢Ğ•Ğ™ĞĞ•Ğ Ğ« Ğ˜ ĞĞ ĞšĞ•Ğ¡Ğ¢Ğ ĞĞ¦Ğ˜Ğ¯

## 2.1 Docker: Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ²

### ĞÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ°Ğ¼

```
Ğ’Ğ˜Ğ Ğ¢Ğ£ĞĞ›Ğ¬ĞĞ«Ğ• ĞœĞĞ¨Ğ˜ĞĞ« (ÑÑ‚Ğ°Ñ€Ğ¾Ğµ):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Host OS (Linux)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Hypervisor (VMware)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Guest OS â”‚  Guest OS â”‚  Guest OS   â”‚
â”‚  (Linux)  â”‚  (Windows)â”‚  (Linux)    â”‚
â”‚           â”‚           â”‚             â”‚
â”‚   App A   â”‚   App B   â”‚   App C     â”‚
â”‚  4 Ğ“Ğ‘ RAM â”‚  6 Ğ“Ğ‘ RAM â”‚  4 Ğ“Ğ‘ RAM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: ĞšĞ°Ğ¶Ğ´Ğ°Ñ VM = Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ ĞĞ¡ = Ñ‚ÑĞ¶ĞµĞ»Ğ¾


DOCKER ĞšĞĞĞ¢Ğ•Ğ™ĞĞ•Ğ Ğ« (Ğ½Ğ¾Ğ²Ğ¾Ğµ):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Host OS (Linux)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Docker Engine                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚Container Aâ”‚Container Bâ”‚Container C  â”‚
â”‚           â”‚           â”‚             â”‚
â”‚   App A   â”‚   App B   â”‚   App C     â”‚
â”‚  + libs   â”‚  + libs   â”‚  + libs     â”‚
â”‚           â”‚           â”‚             â”‚
â”‚  50 ĞœĞ‘    â”‚  80 ĞœĞ‘    â”‚  50 ĞœĞ‘      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾: ĞĞ±Ñ‰ĞµĞµ ÑĞ´Ñ€Ğ¾ ĞĞ¡ = Ğ»ĞµĞ³ĞºĞ¾
```

### Dockerfile Ğ´Ğ»Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°:

```dockerfile
# Dockerfile Ğ´Ğ»Ñ ML Service
FROM python:3.11-slim

# ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
LABEL maintainer="team@example.com"
LABEL service="ml-inference"
LABEL version="1.0"

# Ğ Ğ°Ğ±Ğ¾Ñ‡Ğ°Ñ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ
WORKDIR /app

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°
COPY ml_service.py .
COPY models/ ./models/

# ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
ENV MODEL_PATH=/app/models
ENV PORT=5003

# Healthcheck
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:5003/health || exit 1

# ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ€Ñ‚Ğ°
EXPOSE 5003

# ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
CMD ["python", "ml_service.py"]
```

### Docker Compose - Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ²:

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
  auth-service:
    build: ./auth_service
    ports:
      - "5001:5001"
    environment:
      - REDIS_URL=redis://redis:6379
      - SECRET_KEY=${SECRET_KEY}
    depends_on:
      - redis
    networks:
      - microservices-net
    restart: always
    deploy:
      replicas: 2  # Ğ”Ğ²Ğ° ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
  user-service:
    build: ./user_service
    ports:
      - "5002:5002"
    environment:
      - MONGO_URL=mongodb://mongo:27017
    depends_on:
      - mongo
    networks:
      - microservices-net
    restart: always

  # ML ÑĞµÑ€Ğ²Ğ¸Ñ
  ml-service:
    build: ./ml_service
    ports:
      - "5003:5003"
    environment:
      - MODEL_PATH=/models
    volumes:
      - ./models:/models:ro  # Read-only Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼
    networks:
      - microservices-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]  # GPU Ğ´Ğ»Ñ ML

  # Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
  data-service:
    build: ./data_service
    ports:
      - "5004:5004"
    environment:
      - CELERY_BROKER=redis://redis:6379/0
    depends_on:
      - redis
    networks:
      - microservices-net
    restart: always

  # Redis (ĞºÑÑˆ Ğ¸ message broker)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - microservices-net
    restart: always

  # MongoDB (Ğ±Ğ°Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹)
  mongo:
    image: mongo:6
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db
    networks:
      - microservices-net
    restart: always

  # API Gateway (ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°)
  api-gateway:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - auth-service
      - user-service
      - ml-service
      - data-service
    networks:
      - microservices-net
    restart: always

networks:
  microservices-net:
    driver: bridge

volumes:
  redis-data:
  mongo-data:
```

---

## 2.2 Kubernetes: Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°

### Ğ¤Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Kubernetes:

```
Kubernetes = ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹

ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸:
1. Pod - Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ (1+ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ²)
2. Service - ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Pods
3. Deployment - Ğ´ĞµĞºĞ»Ğ°Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Pods
4. ConfigMap/Secret - ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
5. Volume - Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
6. Namespace - Ğ¸Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²
```

### Kubernetes Ğ¼Ğ°Ğ½Ğ¸Ñ„ĞµÑÑ‚ Ğ´Ğ»Ñ ML Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°:

```yaml
# ml-service-deployment.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: ai-microservices
---
# Deployment - ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ğ¼Ğ¸
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-service
  namespace: ai-microservices
  labels:
    app: ml-service
    tier: backend
spec:
  replicas: 3  # 3 ĞºĞ¾Ğ¿Ğ¸Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
  selector:
    matchLabels:
      app: ml-service
  strategy:
    type: RollingUpdate  # ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: ml-service
        version: v1.0
    spec:
      containers:
      - name: ml-inference
        image: myregistry/ml-service:1.0
        ports:
        - containerPort: 5003
          name: http
        env:
        - name: MODEL_PATH
          value: /models
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: ml-config
              key: log_level
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        volumeMounts:
        - name: models-volume
          mountPath: /models
          readOnly: true
        livenessProbe:  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¶Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸
          httpGet:
            path: /health
            port: 5003
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
          httpGet:
            path: /ready
            port: 5003
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: models-volume
        persistentVolumeClaim:
          claimName: ml-models-pvc
---
# Service - Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞµÑ€Ğ²Ğ¸ÑÑƒ
apiVersion: v1
kind: Service
metadata:
  name: ml-service
  namespace: ai-microservices
spec:
  selector:
    app: ml-service
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5003
  type: ClusterIP  # Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿
---
# HorizontalPodAutoscaler - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-service-hpa
  namespace: ai-microservices
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
---
# ConfigMap - ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-config
  namespace: ai-microservices
data:
  log_level: "INFO"
  model_version: "v1.0"
  max_batch_size: "32"
---
# PersistentVolumeClaim - Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ml-models-pvc
  namespace: ai-microservices
spec:
  accessModes:
    - ReadOnlyMany  # ĞœĞ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ĞµĞ»ĞµĞ¹
  resources:
    requests:
      storage: 100Gi
  storageClassName: fast-ssd
```

### ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ² Kubernetes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Ingress Controller                      â”‚   â”‚
â”‚  â”‚         (nginx / Traefik / Istio)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚             API Gateway Service                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚     â”‚      â”‚      â”‚      â”‚      â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Auth  â”‚ â”‚User  â”‚ â”‚ML   â”‚ â”‚Dataâ”‚ â”‚Imageâ”‚ â”‚Audio â”‚         â”‚
â”‚  â”‚Pods  â”‚ â”‚Pods  â”‚ â”‚Pods â”‚ â”‚Podsâ”‚ â”‚Pods â”‚ â”‚Pods  â”‚         â”‚
â”‚  â”‚(x3)  â”‚ â”‚(x3)  â”‚ â”‚(x5) â”‚ â”‚(x2)â”‚ â”‚(x2) â”‚ â”‚(x2)  â”‚         â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜         â”‚
â”‚     â”‚        â”‚        â”‚       â”‚      â”‚        â”‚             â”‚
â”‚  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚            Message Bus (Kafka / RabbitMQ)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Redis   â”‚  â”‚ MongoDB  â”‚  â”‚PostgreSQLâ”‚  â”‚ ML Modelsâ”‚    â”‚
â”‚  â”‚  Cluster â”‚  â”‚ Cluster  â”‚  â”‚ Cluster  â”‚  â”‚  Storage â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:
âœ… ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (HPA)
âœ… Self-healing (Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº)
âœ… Load balancing (Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸)
âœ… Rolling updates (Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ)
âœ… Service discovery (Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¾Ğ¸ÑĞº ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²)
âœ… Resource management (ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸)
```

---

# 3. Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« FLUTTER: Ğ˜Ğ•Ğ ĞĞ Ğ¥Ğ˜Ğ¯ ĞšĞĞœĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ˜

## 3.1 Ğ¤Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²

### Ğ’ÑĞµ ĞµÑÑ‚ÑŒ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ² Flutter:

```dart
// Ğ¤Ğ£ĞĞ”ĞĞœĞ•ĞĞ¢ĞĞ›Ğ¬ĞĞ«Ğ™ ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸ FLUTTER:
// "Everything is a Widget"

// Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚ - ÑÑ‚Ğ¾:
// 1. ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ UI
// 2. ĞĞµĞ¸Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğ¹ (immutable) Ğ¾Ğ±ÑŠĞµĞºÑ‚
// 3. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ (ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ/ÑƒĞ½Ğ¸Ñ‡Ñ‚Ğ¾Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾)
// 4. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²
```

---

## 3.2 Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ½Ğ°Ğ½Ğ¾ Ğ´Ğ¾ Ğ¼ĞµĞ³Ğ°

### ĞĞĞĞ-Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« (Atomic Widgets)

Ğ¡Ğ°Ğ¼Ñ‹Ğµ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ, Ğ½ĞµĞ´ĞµĞ»Ğ¸Ğ¼Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹:

```dart
// Text - Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°
Text('Hello')

// Icon - Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ğ¸ĞºĞ¾Ğ½ĞºĞ¸
Icon(Icons.star)

// Image - Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
Image.asset('logo.png')

// Divider - Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ñ
Divider()

// SizedBox - Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°
SizedBox(width: 10, height: 10)

// CircularProgressIndicator - Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°
CircularProgressIndicator()
```

### ĞŸĞ˜ĞšĞ-Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« (Basic Widgets)

ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ½Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²:

```dart
// IconWithLabel - Ğ¿Ğ¸ĞºĞ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class IconWithLabel extends StatelessWidget {
  final IconData icon;
  final String label;

  IconWithLabel({required this.icon, required this.label});

  @override
  Widget build(BuildContext context) {
    return Column(
      children: [
        Icon(icon),      // Ğ½Ğ°Ğ½Ğ¾
        SizedBox(height: 4),  // Ğ½Ğ°Ğ½Ğ¾
        Text(label),     // Ğ½Ğ°Ğ½Ğ¾
      ],
    );
  }
}

// Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
IconWithLabel(icon: Icons.home, label: 'Home')
```

### ĞœĞ˜ĞšĞ Ğ-Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« (Component Widgets)

ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹:

```dart
// UserAvatar - Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class UserAvatar extends StatelessWidget {
  final String imageUrl;
  final double size;
  final VoidCallback? onTap;

  UserAvatar({
    required this.imageUrl,
    this.size = 40,
    this.onTap,
  });

  @override
  Widget build(BuildContext context) {
    return GestureDetector(
      onTap: onTap,
      child: Container(
        width: size,
        height: size,
        decoration: BoxDecoration(
          shape: BoxShape.circle,
          image: DecorationImage(
            image: NetworkImage(imageUrl),
            fit: BoxFit.cover,
          ),
          border: Border.all(color: Colors.blue, width: 2),
        ),
      ),
    );
  }
}

// ChatBubble - Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class ChatBubble extends StatelessWidget {
  final String message;
  final bool isMe;
  final DateTime timestamp;

  ChatBubble({
    required this.message,
    required this.isMe,
    required this.timestamp,
  });

  @override
  Widget build(BuildContext context) {
    return Align(
      alignment: isMe ? Alignment.centerRight : Alignment.centerLeft,
      child: Container(
        margin: EdgeInsets.symmetric(vertical: 4, horizontal: 8),
        padding: EdgeInsets.all(12),
        decoration: BoxDecoration(
          color: isMe ? Colors.blue : Colors.grey[300],
          borderRadius: BorderRadius.circular(16),
        ),
        child: Column(
          crossAxisAlignment: CrossAxisAlignment.start,
          children: [
            Text(
              message,
              style: TextStyle(
                color: isMe ? Colors.white : Colors.black,
              ),
            ),
            SizedBox(height: 4),
            Text(
              _formatTime(timestamp),
              style: TextStyle(
                fontSize: 10,
                color: isMe ? Colors.white70 : Colors.black54,
              ),
            ),
          ],
        ),
      ),
    );
  }

  String _formatTime(DateTime time) {
    return '${time.hour}:${time.minute.toString().padLeft(2, '0')}';
  }
}
```

### ĞœĞ˜Ğ”Ğ˜-Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« (Feature Widgets)

Ğ—Ğ°ĞºĞ¾Ğ½Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸:

```dart
// ChatMessageList - Ğ¼Ğ¸Ğ´Ğ¸-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class ChatMessageList extends StatefulWidget {
  final List<Message> messages;
  final ScrollController scrollController;

  ChatMessageList({
    required this.messages,
    required this.scrollController,
  });

  @override
  _ChatMessageListState createState() => _ChatMessageListState();
}

class _ChatMessageListState extends State<ChatMessageList> {
  @override
  Widget build(BuildContext context) {
    return ListView.builder(
      controller: widget.scrollController,
      itemCount: widget.messages.length,
      itemBuilder: (context, index) {
        final message = widget.messages[index];

        return ChatBubble(  // Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
          message: message.text,
          isMe: message.isMe,
          timestamp: message.timestamp,
        );
      },
    );
  }
}

// UserProfile - Ğ¼Ğ¸Ğ´Ğ¸-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class UserProfile extends StatelessWidget {
  final User user;

  UserProfile({required this.user});

  @override
  Widget build(BuildContext context) {
    return Card(
      child: Padding(
        padding: EdgeInsets.all(16),
        child: Column(
          children: [
            UserAvatar(  // Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
              imageUrl: user.avatarUrl,
              size: 80,
            ),
            SizedBox(height: 16),
            Text(
              user.name,
              style: Theme.of(context).textTheme.headline6,
            ),
            SizedBox(height: 8),
            Text(
              user.bio,
              style: Theme.of(context).textTheme.bodyText2,
              textAlign: TextAlign.center,
            ),
            SizedBox(height: 16),
            Row(
              mainAxisAlignment: MainAxisAlignment.spaceEvenly,
              children: [
                _buildStatColumn('Posts', user.postsCount),
                _buildStatColumn('Followers', user.followersCount),
                _buildStatColumn('Following', user.followingCount),
              ],
            ),
          ],
        ),
      ),
    );
  }

  Widget _buildStatColumn(String label, int count) {
    return Column(
      children: [
        Text(
          count.toString(),
          style: TextStyle(
            fontSize: 20,
            fontWeight: FontWeight.bold,
          ),
        ),
        Text(label),
      ],
    );
  }
}
```

### ĞœĞ•Ğ“Ğ-Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« (Screen Widgets)

Ğ¦ĞµĞ»Ñ‹Ğµ ÑĞºÑ€Ğ°Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ:

```dart
// ChatScreen - Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class ChatScreen extends StatefulWidget {
  final String chatId;

  ChatScreen({required this.chatId});

  @override
  _ChatScreenState createState() => _ChatScreenState();
}

class _ChatScreenState extends State<ChatScreen> {
  final TextEditingController _messageController = TextEditingController();
  final ScrollController _scrollController = ScrollController();
  List<Message> _messages = [];

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Row(
          children: [
            UserAvatar(  // Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
              imageUrl: 'https://example.com/avatar.jpg',
              size: 32,
            ),
            SizedBox(width: 12),
            Text('Chat with AI'),
          ],
        ),
      ),
      body: Column(
        children: [
          // Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ - Ğ¼Ğ¸Ğ´Ğ¸-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
          Expanded(
            child: ChatMessageList(
              messages: _messages,
              scrollController: _scrollController,
            ),
          ),

          // ĞŸĞ¾Ğ»Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ° - Ğ¼Ğ¸Ğ´Ğ¸-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
          MessageInput(
            controller: _messageController,
            onSend: _sendMessage,
          ),
        ],
      ),
    );
  }

  void _sendMessage(String text) {
    setState(() {
      _messages.add(Message(
        text: text,
        isMe: true,
        timestamp: DateTime.now(),
      ));
    });

    // ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ½Ğ° AI backend
    _getAIResponse(text);
  }

  Future<void> _getAIResponse(String prompt) async {
    // Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ° AI
    final response = await AIService.generate(prompt);

    setState(() {
      _messages.add(Message(
        text: response,
        isMe: false,
        timestamp: DateTime.now(),
      ));
    });
  }
}
```

### Ğ“Ğ˜Ğ“Ğ-Ğ’Ğ˜Ğ”Ğ–Ğ•Ğ¢Ğ« (App Widgets)

Ğ¦ĞµĞ»Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ:

```dart
// DataScienceApp - Ğ³Ğ¸Ğ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
class DataScienceApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Data Science Platform',
      theme: ThemeData.dark(),
      home: MainNavigation(),  // Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
      routes: {
        '/chat': (context) => ChatScreen(chatId: 'main'),
        '/data': (context) => DataAnalysisScreen(),
        '/ml': (context) => MLTrainingScreen(),
        '/viz': (context) => VisualizationScreen(),
      },
    );
  }
}

// MainNavigation - Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸
class MainNavigation extends StatefulWidget {
  @override
  _MainNavigationState createState() => _MainNavigationState();
}

class _MainNavigationState extends State<MainNavigation> {
  int _currentIndex = 0;

  final List<Widget> _screens = [
    HomeScreen(),           // Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
    ChatScreen(chatId: '1'),    // Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
    DataAnalysisScreen(),   // Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
    ProfileScreen(),        // Ğ¼ĞµĞ³Ğ°-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚
  ];

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: _screens[_currentIndex],
      bottomNavigationBar: BottomNavigationBar(
        currentIndex: _currentIndex,
        onTap: (index) => setState(() => _currentIndex = index),
        items: [
          BottomNavigationBarItem(
            icon: Icon(Icons.home),
            label: 'Home',
          ),
          BottomNavigationBarItem(
            icon: Icon(Icons.chat),
            label: 'Chat',
          ),
          BottomNavigationBarItem(
            icon: Icon(Icons.analytics),
            label: 'Data',
          ),
          BottomNavigationBarItem(
            icon: Icon(Icons.person),
            label: 'Profile',
          ),
        ],
      ),
    );
  }
}
```

---

## 3.3 ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ vs ĞĞ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

Flutter Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ **ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ**, Ğ° Ğ½Ğµ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:

```dart
// âŒ ĞŸĞ›ĞĞ¥Ğ: ĞĞ°ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (ĞºĞ°Ğº Ğ² Java/OOP)
class MyButton extends CustomButton {
  // Ğ–ĞµÑÑ‚ĞºĞ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ, ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ
}

// âœ… Ğ¥ĞĞ ĞĞ¨Ğ: ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ (Flutter way)
class MyButton extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return GestureDetector(  // ĞĞ±Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹
      onTap: _onTap,
      child: Container(
        decoration: BoxDecoration(/* ... */),
        child: Text(_label),  // ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²
      ),
    );
  }
}
```

**ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒ Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸:**
- Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚ = Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ
- ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ² = Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
- Props/State = API ÑĞµÑ€Ğ²Ğ¸ÑĞ°
- Build method = Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°

---

# 4. AI ĞĞ“Ğ•ĞĞ¢Ğ« Ğ˜ RAG

## 4.1 Fine-tuning vs RAG

### Fine-tuning (Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸):

```python
# Fine-tuning - Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸

from transformers import AutoModelForCausalLM, Trainer, TrainingArguments

# 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
model = AutoModelForCausalLM.from_pretrained("gpt2")

# 2. ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
train_dataset = load_custom_dataset()

# 3. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ²)
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir='./fine-tuned-model',
        num_train_epochs=3,
        per_device_train_batch_size=4,
    ),
    train_dataset=train_dataset,
)

trainer.train()

# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: ĞĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸
model.save_pretrained('./my-specialized-model')
```

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ fine-tuning:**
- âŒ Ğ”Ğ¾Ñ€Ğ¾Ğ³Ğ¾ (GPU, Ğ²Ñ€ĞµĞ¼Ñ)
- âŒ ĞÑƒĞ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- âŒ Ğ Ğ¸ÑĞº catastrophic forgetting
- âŒ Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ

### RAG (Retrieval-Augmented Generation):

```python
# RAG - Ğ½Ğµ Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA

# 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ‘Ğ•Ğ— Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)
llm = HuggingFacePipeline.from_model_id(
    model_id="gpt2",
    task="text-generation",
)

# 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (Vector Database)
embeddings = HuggingFaceEmbeddings()

documents = [
    "Python - ÑÑ‚Ğ¾ ÑĞ·Ñ‹Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
    "Flutter - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ UI",
    "Docker - ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
    # ... Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
]

vector_store = FAISS.from_texts(documents, embeddings)

# 3. RAG chain - Ğ¿Ğ¾Ğ¸ÑĞº + Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
)

# 4. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
question = "Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Flutter?"

# RAG Ğ´ĞµĞ»Ğ°ĞµÑ‚:
# Ğ¨Ğ°Ğ³ 1: ĞŸĞ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
# Ğ¨Ğ°Ğ³ 2: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
# Ğ¨Ğ°Ğ³ 3: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
answer = qa_chain.run(question)
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° RAG:**
- âœ… Ğ”ĞµÑˆĞµĞ²Ğ¾ (Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ)
- âœ… Ğ›ĞµĞ³ĞºĞ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ (Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚)
- âœ… ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (Ğ²Ğ¸Ğ´Ğ½Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸)
- âœ… Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ (Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹)

---

## 4.2 AI ĞĞ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹

### ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²:

```python
# ĞĞ“Ğ•ĞĞ¢ = ĞœĞ˜ĞšĞ ĞĞ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ Ñ AI Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹

from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent
from langchain.prompts import StringPromptTemplate

# ĞĞ“Ğ•ĞĞ¢ 1: ĞŸĞ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
class SearchAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ"""

    def __init__(self):
        self.name = "search-agent"
        self.port = 6001

    def search(self, query: str) -> str:
        # ĞŸĞ¾Ğ¸ÑĞº Ğ² Google/Bing
        results = google_search(query)
        return results

# ĞĞ“Ğ•ĞĞ¢ 2: ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
class DataAnalysisAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""

    def __init__(self):
        self.name = "data-agent"
        self.port = 6002

    def analyze(self, data: pd.DataFrame) -> dict:
        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
        return {
            'mean': data.mean(),
            'std': data.std(),
            'correlation': data.corr(),
        }

# ĞĞ“Ğ•ĞĞ¢ 3: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°
class CodeGenerationAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°"""

    def __init__(self):
        self.name = "code-agent"
        self.port = 6003
        self.model = AutoModelForCausalLM.from_pretrained("codellama")

    def generate_code(self, description: str, language: str) -> str:
        prompt = f"Generate {language} code for: {description}"
        code = self.model.generate(prompt)
        return code

# ĞĞ“Ğ•ĞĞ¢ 4: Ğ¡ÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
class SummarizationAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°"""

    def __init__(self):
        self.name = "summary-agent"
        self.port = 6004
        self.pipeline = pipeline("summarization")

    def summarize(self, text: str, max_length: int = 100) -> str:
        summary = self.pipeline(text, max_length=max_length)
        return summary[0]['summary_text']

# ĞœĞ•Ğ¢Ğ-ĞĞ“Ğ•ĞĞ¢: ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ Ğ²ÑĞµÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
class MasterAgent:
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸"""

    def __init__(self):
        self.agents = {
            'search': SearchAgent(),
            'data': DataAnalysisAgent(),
            'code': CodeGenerationAgent(),
            'summary': SummarizationAgent(),
        }

    def route_task(self, task_description: str) -> str:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ°ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¾Ñ‚Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ"""

        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM
        agent_name = self.determine_agent(task_description)

        # ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğº Ğ½ÑƒĞ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ
        agent = self.agents[agent_name]
        result = agent.execute(task_description)

        return result

    def determine_agent(self, task: str) -> str:
        """LLM Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ¸Ğ¿ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸"""
        prompt = f"""
        Task: {task}

        Which agent should handle this?
        - search: for finding information online
        - data: for analyzing datasets
        - code: for generating code
        - summary: for summarizing text

        Answer with just the agent name:
        """

        response = llm(prompt)
        return response.strip()
```

---

## 4.3 RAG ĞºĞ°Ğº Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ

```python
# RAG Microservice Architecture

from flask import Flask, request, jsonify
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
import chromadb

app = Flask(__name__)

# Ğ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ 1: Vector Store (Ğ‘Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹)
class VectorStoreService:
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²"""

    def __init__(self):
        self.client = chromadb.Client()
        self.collection = self.client.create_collection("knowledge_base")
        self.embeddings = HuggingFaceEmbeddings()

    def add_document(self, text: str, metadata: dict):
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² Ğ±Ğ°Ğ·Ñƒ"""
        embedding = self.embeddings.embed_query(text)
        self.collection.add(
            embeddings=[embedding],
            documents=[text],
            metadatas=[metadata],
            ids=[metadata['id']]
        )

    def search(self, query: str, k: int = 5):
        """ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
        query_embedding = self.embeddings.embed_query(query)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        return results

# Ğ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ 2: Retrieval (ĞŸĞ¾Ğ¸ÑĞº)
class RetrievalService:
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸"""

    def __init__(self, vector_store: VectorStoreService):
        self.vector_store = vector_store

    def retrieve(self, query: str, k: int = 3):
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
        results = self.vector_store.search(query, k=k)

        # Ğ Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        ranked_results = self.rerank(results, query)

        return ranked_results

    def rerank(self, results, query):
        """ĞŸĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸"""
        # Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        return sorted(results, key=lambda x: x['score'], reverse=True)

# Ğ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ 3: Generation (Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ)
class GenerationService:
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²"""

    def __init__(self):
        self.llm = HuggingFacePipeline.from_model_id("gpt2")

    def generate(self, query: str, context: list[str]) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°"""

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
        prompt = self.build_prompt(query, context)

        # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ
        response = self.llm(prompt)

        return response

    def build_prompt(self, query: str, context: list[str]) -> str:
        """ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼"""
        context_text = "\n\n".join(context)

        prompt = f"""
        Context:
        {context_text}

        Question: {query}

        Answer based on the context above:
        """

        return prompt

# Ğ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ 4: RAG Orchestrator (ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€)
class RAGOrchestrator:
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ, ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ RAG pipeline"""

    def __init__(self):
        self.vector_store = VectorStoreService()
        self.retriever = RetrievalService(self.vector_store)
        self.generator = GenerationService()

    def query(self, question: str) -> dict:
        """ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ RAG pipeline"""

        # Ğ¨Ğ°Ğ³ 1: Retrieval - Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
        relevant_docs = self.retriever.retrieve(question, k=3)

        # Ğ¨Ğ°Ğ³ 2: Augmentation - Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
        context = [doc['text'] for doc in relevant_docs]

        # Ğ¨Ğ°Ğ³ 3: Generation - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
        answer = self.generator.generate(question, context)

        return {
            'answer': answer,
            'sources': relevant_docs,
            'confidence': self.calculate_confidence(relevant_docs)
        }

    def calculate_confidence(self, docs):
        """Ğ Ğ°ÑÑ‡ĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ"""
        # ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ similarity scores
        avg_score = sum(doc['score'] for doc in docs) / len(docs)
        return avg_score

# Flask API Ğ´Ğ»Ñ RAG ÑĞµÑ€Ğ²Ğ¸ÑĞ°
rag = RAGOrchestrator()

@app.route('/rag/query', methods=['POST'])
def rag_query():
    """Endpoint Ğ´Ğ»Ñ RAG Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²"""
    data = request.json
    question = data['question']

    result = rag.query(question)

    return jsonify(result)

@app.route('/rag/add_document', methods=['POST'])
def add_document():
    """Endpoint Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹"""
    data = request.json

    rag.vector_store.add_document(
        text=data['text'],
        metadata=data['metadata']
    )

    return jsonify({'status': 'added'})

if __name__ == '__main__':
    app.run(port=7000)
```

---

# 5. Ğ•Ğ”Ğ˜ĞĞĞ¯ Ğ¤Ğ˜Ğ›ĞĞ¡ĞĞ¤Ğ˜Ğ¯: ĞŸĞĞ ĞĞ›Ğ›Ğ•Ğ›Ğ˜ Ğ˜ ĞĞĞĞ›ĞĞ“Ğ˜Ğ˜

## 5.1 Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

Ğ’ÑĞµ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ - **Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹**, **ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹**, **Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹ Flutter**, **AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹**, **RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹** - Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ:

### ğŸ¯ ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸

```
Ğ‘ĞĞ›Ğ¬Ğ¨ĞĞ¯ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ = ĞšĞĞœĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ¯ ĞœĞĞ›Ğ«Ğ¥ ĞĞ•Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞ«Ğ¥ ĞœĞĞ”Ğ£Ğ›Ğ•Ğ™

Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ:
1. Ğ˜Ğ¼ĞµĞµÑ‚ Ñ‡ĞµÑ‚ĞºÑƒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ (Single Responsibility)
2. Ğ¡Ğ»Ğ°Ğ±Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ (Loose Coupling)
3. Ğ˜Ğ¼ĞµĞµÑ‚ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ (Clear Interface)
4. ĞœĞ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ¼ĞµĞ½ĞµĞ½/Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ (Replaceability)
5. ĞœĞ¾Ğ¶ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ (Independent Scaling)
```

## 5.2 Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ĞµĞ¹

| ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ | ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ | ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ | Flutter Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚ | AI ĞĞ³ĞµĞ½Ñ‚ | RAG ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ |
|-----------|-------------|-----------|----------------|----------|---------------|
| **Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°** | Flask API | Docker Image | Widget ĞºĞ»Ğ°ÑÑ | Agent Instance | RAG Module |
| **Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ** | REST Endpoint | Exposed Ports | Widget API | Message Protocol | Query/Response |
| **Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ** | Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… | Volume | State/StatelessWidget | Agent Memory | Vector Store |
| **Ğ¡Ğ²ÑĞ·ÑŒ** | HTTP/gRPC | Network | Widget Tree | Message Queue | API Calls |
| **ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ** | API Gateway | Kubernetes | Widget Composition | Agent Orchestrator | RAG Pipeline |
| **ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ** | Load Balancer | Pod Replicas | Widget Rebuilds | Agent Pool | Distributed Retrieval |
| **Ğ˜Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ** | Process/Network | Namespace/Cgroups | Widget Scope | Agent Context | Knowledge Domain |
| **ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ** | Service Mesh | Docker Compose | build() method | Agent Hierarchy | Multi-stage RAG |

## 5.3 Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸

### ğŸ“¦ ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ â†” ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ â†” Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚ â†” AI ĞĞ³ĞµĞ½Ñ‚

#### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ 1: Auth Service Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ñ…

**ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ (Flask API):**
```python
# auth_service.py
@app.route('/auth/login', methods=['POST'])
def login():
    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ğ½Ğ°
    return jsonify({'token': token})
```

**ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ (Docker):**
```dockerfile
# Dockerfile
FROM python:3.11
COPY auth_service.py .
CMD ["python", "auth_service.py"]
```

**Flutter Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚:**
```dart
// LoginWidget
class LoginWidget extends StatefulWidget {
  @override
  Widget build(BuildContext context) {
    return TextField(/* login form */);
  }
}
```

**AI ĞĞ³ĞµĞ½Ñ‚:**
```python
# AuthAgent
class AuthAgent:
    def handle_message(self, msg):
        if msg['type'] == 'login_request':
            return self.process_login(msg['data'])
```

**RAG ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚:**
```python
# AuthKnowledgeRetriever
class AuthKnowledgeRAG:
    def retrieve_auth_context(self, query):
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        return self.vector_store.search(query)
```

### ğŸ”„ ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…

#### ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ:
```
E-commerce App
â”œâ”€â”€ Auth Service (Ğ¿Ğ¾Ñ€Ñ‚ 5001)
â”œâ”€â”€ Product Service (Ğ¿Ğ¾Ñ€Ñ‚ 5002)
â”œâ”€â”€ Cart Service (Ğ¿Ğ¾Ñ€Ñ‚ 5003)
â””â”€â”€ Payment Service (Ğ¿Ğ¾Ñ€Ñ‚ 5004)
```

#### ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ (docker-compose.yml):
```yaml
services:
  auth:
    image: auth-service
  products:
    image: product-service
  cart:
    image: cart-service
  payment:
    image: payment-service
```

#### Flutter ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ:
```dart
ShoppingApp
â”œâ”€â”€ LoginScreen
â”‚   â”œâ”€â”€ LoginForm
â”‚   â”‚   â”œâ”€â”€ EmailField
â”‚   â”‚   â””â”€â”€ PasswordField
â”‚   â””â”€â”€ LoginButton
â”œâ”€â”€ ProductListScreen
â”‚   â””â”€â”€ ProductCard (Ã—N)
â””â”€â”€ CartScreen
    â””â”€â”€ CartItem (Ã—N)
```

#### AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ:
```
E-commerce AI System
â”œâ”€â”€ CustomerServiceAgent
â”œâ”€â”€ RecommendationAgent
â”œâ”€â”€ InventoryAgent
â””â”€â”€ FraudDetectionAgent
```

#### RAG ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ:
```
Knowledge System
â”œâ”€â”€ ProductRAG (Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹)
â”œâ”€â”€ PolicyRAG (Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸)
â”œâ”€â”€ TechnicalRAG (Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ)
â””â”€â”€ FAQRAG (Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹)
```

## 5.4 Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### ğŸ¨ ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 1: Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** ĞœĞ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ° Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞµ

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ñ…:**

1. **ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹:** Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚ Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹
2. **ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹:** ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğµ
3. **Flutter:** Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ UI Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²
4. **AI:** Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
5. **RAG:** Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹

### ğŸ¨ ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 2: Ğ˜Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»ÑÑ†Ğ¸Ñ

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**

| ĞŸĞ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° | ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»ÑÑ†Ğ¸Ğ¸ |
|-----------|----------------------|
| ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ | Private Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, ÑĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ·Ğ° API |
| ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ | Internal ports, Ğ½Ğµ exposed Ğ½Ğ°Ñ€ÑƒĞ¶Ñƒ |
| Flutter | Private Ğ¿Ğ¾Ğ»Ñ (_variableName) |
| AI Ğ°Ğ³ĞµĞ½Ñ‚ | Internal state, ÑĞºÑ€Ñ‹Ñ‚ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² |
| RAG | Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ embeddings, Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ query API |

### ğŸ¨ ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 3: Ğ¡Ğ»Ğ°Ğ±Ğ°Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ

**ĞšĞ°Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ:**

```python
# âŒ ĞŸĞ›ĞĞ¥Ğ: Ğ¢ĞµÑĞ½Ğ°Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ
class OrderService:
    def create_order(self):
        user = UserDatabase.query(user_id)  # ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ‘Ğ” Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
        payment = PaymentService().charge()  # ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ²

# âœ… Ğ¥ĞĞ ĞĞ¨Ğ: Ğ¡Ğ»Ğ°Ğ±Ğ°Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ
class OrderService:
    def create_order(self):
        user = self.user_api_client.get_user(user_id)  # Ğ§ĞµÑ€ĞµĞ· API
        payment = self.message_queue.send('payment.charge', data)  # Ğ§ĞµÑ€ĞµĞ· Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ
```

Ğ¢Ğ° Ğ¶Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ° ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼:
- **ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹:** Ğ¡Ğ²ÑĞ·ÑŒ Ñ‡ĞµÑ€ĞµĞ· network, Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· shared volumes
- **Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹:** Ğ¡Ğ²ÑĞ·ÑŒ Ñ‡ĞµÑ€ĞµĞ· callbacks Ğ¸ state management, Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
- **AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹:** Ğ¡Ğ²ÑĞ·ÑŒ Ñ‡ĞµÑ€ĞµĞ· message passing, Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· shared memory
- **RAG:** Ğ¡Ğ²ÑĞ·ÑŒ Ñ‡ĞµÑ€ĞµĞ· API, Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº vector store

### ğŸ¨ ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 4: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

**Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞ·Ğ´Ğµ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾:**

```
1 ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ â†’ N ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² + Load Balancer
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:**

| ĞŸĞ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° | ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ |
|-----------|-----------------|
| ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ | 1 Flask app â†’ 3 Flask app instances + Nginx |
| ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ | 1 Pod â†’ 3 Pod replicas + Kubernetes Service |
| Flutter | 1 ListView item â†’ ListView Ñ 1000 ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² |
| AI Ğ°Ğ³ĞµĞ½Ñ‚ | 1 Worker agent â†’ Pool Ğ¸Ğ· 10 worker agents |
| RAG | 1 Vector store â†’ Distributed vector store (Qdrant cluster) |

## 5.5 Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: Ğ¾Ñ‚ UI Ğ´Ğ¾ AI

### ğŸ—ï¸ ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑÑ‚ĞµĞº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUTTER FRONTEND                          â”‚
â”‚  (Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹: Screens â†’ Forms â†’ Buttons)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API GATEWAY                                â”‚
â”‚  (Nginx/Kong - Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)                      â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚          â”‚          â”‚          â”‚
      â”‚ Docker   â”‚ Docker   â”‚ Docker   â”‚ Docker
      â”‚ Network  â”‚ Network  â”‚ Network  â”‚ Network
      â”‚          â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auth    â”‚ â”‚ Product â”‚ â”‚  Order   â”‚ â”‚  Payment  â”‚
â”‚ Service  â”‚ â”‚ Service â”‚ â”‚ Service  â”‚ â”‚  Service  â”‚
â”‚ (Flask)  â”‚ â”‚ (Flask) â”‚ â”‚ (Flask)  â”‚ â”‚  (Flask)  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚          â”‚          â”‚           â”‚
      â”‚ Kafka    â”‚ Kafka    â”‚ Kafka     â”‚ Kafka
      â”‚ Events   â”‚ Events   â”‚ Events    â”‚ Events
      â”‚          â”‚          â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI AGENT LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚Customer  â”‚  â”‚Recommend â”‚  â”‚  Fraud   â”‚                â”‚
â”‚  â”‚Service   â”‚  â”‚ation     â”‚  â”‚Detection â”‚                â”‚
â”‚  â”‚Agent     â”‚  â”‚Agent     â”‚  â”‚Agent     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â”‚ Query       â”‚ Query       â”‚ Query
        â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG LAYER                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚Product   â”‚  â”‚Policy    â”‚  â”‚Tech Doc  â”‚                â”‚
â”‚  â”‚Knowledge â”‚  â”‚Knowledge â”‚  â”‚Knowledge â”‚                â”‚
â”‚  â”‚RAG       â”‚  â”‚RAG       â”‚  â”‚RAG       â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼
   [Vector DB]   [Vector DB]   [Vector DB]
   (Qdrant)      (Qdrant)      (Qdrant)
```

### ğŸ’¡ ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ

**Ğ’ÑĞµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ´Ğ½Ñƒ Ğ¸ Ñ‚Ñƒ Ğ¶Ğµ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ:**

1. **ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:** ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°
2. **ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ:** Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²
3. **Ğ˜Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ:** ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² ÑĞ²Ğ¾ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ
4. **Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹:** Ğ¡Ğ²ÑĞ·ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ API
5. **ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ:** Ğ›ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
6. **Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ:** Ğ›ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°

---

# 6. Ğ ĞĞ¡ĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞĞ«Ğ• AI Ğ’Ğ«Ğ§Ğ˜Ğ¡Ğ›Ğ•ĞĞ˜Ğ¯

## 6.1 Ğ—Ğ°Ñ‡ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ AI Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ?

### ğŸ¯ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ AI:

1. **ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸:**
   - GPT-3 (175B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) = ~350 GB RAM
   - LLaMA 70B = ~140 GB RAM
   - ĞĞ´Ğ½Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ° Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ

2. **Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:**
   - BERT Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 1 GPU = 4 Ğ´Ğ½Ñ
   - BERT Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 64 GPU = 1.5 Ñ‡Ğ°ÑĞ°

3. **ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ inference:**
   - 1 ÑĞµÑ€Ğ²ĞµÑ€ = 10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²/ÑĞµĞº
   - 10 ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² = 100 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²/ÑĞµĞº

4. **ĞÑ‚ĞºĞ°Ğ·Ğ¾ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ:**
   - 1 ÑĞµÑ€Ğ²ĞµÑ€ ÑƒĞ¿Ğ°Ğ» = ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°
   - 10 ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ², 1 ÑƒĞ¿Ğ°Ğ» = 90% Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸

### âœ… Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ: Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ

```
Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ AI = AI Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ + ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° + ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹
```

## 6.2 Ğ¢Ğ¸Ğ¿Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ AI Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹

### ğŸ”€ Ğ¢Ğ¸Ğ¿ 1: Data Parallelism (Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼)

**Ğ˜Ğ´ĞµÑ:** ĞĞ´Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ñ…

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚         â”‚         â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Model  â”‚ â”‚Model  â”‚ â”‚Model  â”‚
   â”‚Copy 1 â”‚ â”‚Copy 2 â”‚ â”‚Copy 3 â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚        â”‚         â”‚
   [Batch] [Batch]  [Batch]
    1-100  101-200  201-300
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ´Ğ° (PyTorch):**
```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°
dist.init_process_group(backend='nccl')

# ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° GPU
model = MyNeuralNetwork().cuda()

# ĞĞ±Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² DistributedDataParallel
model = DistributedDataParallel(model)

# ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ - ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¹ batch
for epoch in range(num_epochs):
    for batch in dataloader:
        loss = model(batch)
        loss.backward()
        optimizer.step()
```

**ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°:**
```python
# training_worker_service.py (Flask)
@app.route('/train_batch', methods=['POST'])
def train_batch():
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ batch"""
    batch_data = request.json['batch']
    batch_labels = request.json['labels']

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    model = load_model()

    # ĞĞ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ° batch
    loss = model.train_step(batch_data, batch_labels)

    # Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°
    return jsonify({
        'gradients': model.get_gradients(),
        'loss': loss
    })

# Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ 4 ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ° ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°
# Ğ½Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ… 8001, 8002, 8003, 8004
```

**Docker Compose:**
```yaml
# docker-compose-training.yml
services:
  worker1:
    image: training-worker:latest
    environment:
      - RANK=0
      - WORLD_SIZE=4
    ports: ["8001:8000"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1

  worker2:
    image: training-worker:latest
    environment:
      - RANK=1
      - WORLD_SIZE=4
    ports: ["8002:8000"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1

  # worker3 Ğ¸ worker4 Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾

  coordinator:
    image: training-coordinator:latest
    ports: ["9000:9000"]
    depends_on:
      - worker1
      - worker2
      - worker3
      - worker4
```

### ğŸ”€ Ğ¢Ğ¸Ğ¿ 2: Model Parallelism (Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)

**Ğ˜Ğ´ĞµÑ:** Ğ Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ñ…

```
          Input
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Server 1:     â”‚
    â”‚  Layers 1-10   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Server 2:     â”‚
    â”‚  Layers 11-20  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Server 3:     â”‚
    â”‚  Layers 21-30  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
          Output
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: GPT-3 Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° 3 ÑĞµÑ€Ğ²ĞµÑ€Ğ°**

```python
# layer_service_1.py (Flask Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ - ÑĞ»Ğ¾Ğ¸ 1-10)
@app.route('/forward', methods=['POST'])
def forward_layers_1_10():
    """ĞŸĞµÑ€Ğ²Ñ‹Ğµ 10 ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    input_data = request.json['input']

    # ĞŸÑ€Ğ¾Ğ³Ğ½Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ¸ 1-10
    hidden_state = self.model.layers_1_10(input_data)

    # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞµÑ€Ğ²ĞµÑ€
    response = requests.post(
        'http://layer-service-2:8002/forward',
        json={'input': hidden_state}
    )

    return response.json()

# layer_service_2.py (ÑĞ»Ğ¾Ğ¸ 11-20)
@app.route('/forward', methods=['POST'])
def forward_layers_11_20():
    """Ğ¡Ğ»Ğ¾Ğ¸ 11-20"""
    input_data = request.json['input']
    hidden_state = self.model.layers_11_20(input_data)

    # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞµÑ€Ğ²ĞµÑ€
    response = requests.post(
        'http://layer-service-3:8003/forward',
        json={'input': hidden_state}
    )
    return response.json()

# layer_service_3.py (ÑĞ»Ğ¾Ğ¸ 21-30)
@app.route('/forward', methods=['POST'])
def forward_layers_21_30():
    """ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸"""
    input_data = request.json['input']
    output = self.model.layers_21_30(input_data)

    return jsonify({'output': output})
```


**Kubernetes Ğ´ĞµĞ¿Ğ»Ğ¾Ğ¹Ğ¼ĞµĞ½Ñ‚:**
```yaml
# model-parallelism-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: gpt-pipeline
spec:
  selector:
    app: gpt-layer-1
  ports:
    - port: 80
      targetPort: 8001
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt-layer-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpt-layer-1
  template:
    metadata:
      labels:
        app: gpt-layer-1
    spec:
      containers:
      - name: layer-service
        image: gpt-layer-1:latest
        resources:
          limits:
            nvidia.com/gpu: 1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt-layer-2
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: layer-service
        image: gpt-layer-2:latest
# Ğ˜ Ñ‚Ğ°Ğº Ğ´Ğ°Ğ»ĞµĞµ Ğ´Ğ»Ñ layer-3
```

### ğŸ”€ Ğ¢Ğ¸Ğ¿ 3: Pipeline Parallelism (ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼)

**Ğ˜Ğ´ĞµÑ:** ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ° + Ğ±Ğ°Ñ‚Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ¼

```
Ğ’Ñ€ĞµĞ¼Ñ â†’

Batch 1:  [Server1] â†’ [Server2] â†’ [Server3] â†’ Output
Batch 2:           [Server1] â†’ [Server2] â†’ [Server3] â†’ Output
Batch 3:                    [Server1] â†’ [Server2] â†’ [Server3] â†’ Output
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ´Ğ° (PyTorch):**
```python
from torch.distributed.pipeline.sync import Pipe

# Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 3 Ñ‡Ğ°ÑÑ‚Ğ¸
model_part1 = nn.Sequential(layers[0:10])
model_part2 = nn.Sequential(layers[10:20])
model_part3 = nn.Sequential(layers[20:30])

# Ğ Ğ°Ğ·Ğ¼ĞµÑÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… GPU
model_part1 = model_part1.to('cuda:0')
model_part2 = model_part2.to('cuda:1')
model_part3 = model_part3.to('cuda:2')

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ pipeline
model = Pipe(
    nn.Sequential(model_part1, model_part2, model_part3),
    chunks=8  # Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ batch Ğ½Ğ° 8 micro-batches
)

# ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ pipeline
for batch in dataloader:
    output = model(batch)
    loss = criterion(output, labels)
    loss.backward()
```

### ğŸ”€ Ğ¢Ğ¸Ğ¿ 4: Distributed Inference (Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ inference)

**Ğ˜Ğ´ĞµÑ:** ĞœĞ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²

```
                Load Balancer
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
    â”‚Model 1â”‚    â”‚Model 2  â”‚   â”‚Model 3â”‚
    â”‚(GPU 1)â”‚    â”‚(GPU 2)  â”‚   â”‚(GPU 3)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°:**

```python
# inference_worker.py
@app.route('/predict', methods=['POST'])
def predict():
    """ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ inference"""
    input_text = request.json['text']

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (ĞºĞµÑˆĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸)
    model = get_model()

    # Inference
    result = model.generate(input_text, max_length=100)

    return jsonify({'result': result})

# Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ… 8001, 8002, 8003
```

**Kubernetes Ñ Ğ°Ğ²Ñ‚Ğ¾ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³Ğ¾Ğ¼:**
```yaml
# inference-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference
spec:
  replicas: 3  # ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ 3 Ñ€ĞµĞ¿Ğ»Ğ¸Ğº
  selector:
    matchLabels:
      app: llm-inference
  template:
    metadata:
      labels:
        app: llm-inference
    spec:
      containers:
      - name: inference-worker
        image: llm-inference:latest
        resources:
          requests:
            memory: "16Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "32Gi"
            nvidia.com/gpu: 1
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-inference-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference
  minReplicas: 3
  maxReplicas: 10  # ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ 10 Ñ€ĞµĞ¿Ğ»Ğ¸Ğº
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**Nginx Load Balancer:**
```nginx
# nginx.conf
upstream llm_inference {
    least_conn;  # Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ
    server inference-1:8001;
    server inference-2:8002;
    server inference-3:8003;
}

server {
    listen 80;

    location /api/predict {
        proxy_pass http://llm_inference;
        proxy_next_upstream error timeout invalid_header http_500;
        proxy_connect_timeout 5s;
    }
}
```

## 6.3 Distributed RAG: Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹

### ğŸ—‚ï¸ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Ğ¦ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ RAG Ğ½Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹:**
- ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² â†’ Ğ¾Ğ´Ğ½Ğ° vector DB Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ
- Ğ Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ â†’ Ğ½ÑƒĞ¶Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
- Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ â†’ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ

### âœ… Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ: Distributed RAG Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   RAG Gateway    â”‚
                    â”‚ (Query Router)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Product    â”‚  â”‚   Policy    â”‚  â”‚  Technical  â”‚
     â”‚  RAG Node   â”‚  â”‚  RAG Node   â”‚  â”‚   RAG Node  â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                â”‚                â”‚
     [Vector DB 1]    [Vector DB 2]    [Vector DB 3]
      (10M docs)       (5M docs)         (15M docs)
```

**ĞšĞ¾Ğ´ RAG Gateway (Flask):**

```python
# rag_gateway.py
class RAGGateway:
    def __init__(self):
        # Ğ ĞµĞµÑÑ‚Ñ€ RAG Ğ½Ğ¾Ğ´Ğ¾Ğ²
        self.rag_nodes = {
            'products': 'http://product-rag:8001',
            'policies': 'http://policy-rag:8002',
            'technical': 'http://technical-rag:8003',
        }

        # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        self.query_classifier = QueryClassifier()

    def route_query(self, query: str):
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, Ğº ĞºĞ°ĞºĞ¾Ğ¼Ñƒ RAG Ğ½Ğ¾Ğ´Ñƒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"""
        query_type = self.query_classifier.classify(query)

        # ĞœĞ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²
        # ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: "ĞšĞ°ĞºĞ¾Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° X?"
        # â†’ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ¸ 'products' Ğ¸ 'policies'

        if query_type == 'product':
            return ['products']
        elif query_type == 'policy':
            return ['policies']
        elif query_type == 'technical':
            return ['technical']
        elif query_type == 'product_policy':
            return ['products', 'policies']
        else:
            # ĞĞµ ÑƒĞ²ĞµÑ€ĞµĞ½Ñ‹ - ÑĞ¿Ñ€Ğ¾ÑĞ¸Ğ¼ Ğ²ÑĞµ Ğ½Ğ¾Ğ´Ñ‹
            return list(self.rag_nodes.keys())

@app.route('/rag/query', methods=['POST'])
def query():
    """Endpoint Ğ´Ğ»Ñ distributed RAG Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²"""
    query = request.json['query']

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ RAG Ğ½Ğ¾Ğ´Ñ‹
    target_nodes = rag_gateway.route_query(query)

    # ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ½Ğ¾Ğ´Ğ°Ğ¼
    results = []
    with ThreadPoolExecutor(max_workers=len(target_nodes)) as executor:
        futures = []
        for node_name in target_nodes:
            node_url = rag_gateway.rag_nodes[node_name]
            future = executor.submit(
                requests.post,
                f"{node_url}/retrieve",
                json={'query': query}
            )
            futures.append((node_name, future))

        # Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        for node_name, future in futures:
            response = future.result()
            results.extend(response.json()['documents'])

    # Ğ Ğµ-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    results = rerank_results(query, results)

    # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    answer = generate_answer(query, results[:5])

    return jsonify({
        'answer': answer,
        'sources': results[:5]
    })
```


## 6.4 Multi-Agent Systems: Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ AI Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚

### ğŸ¤– ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ AI:**
- ĞĞ´Ğ¸Ğ½ LLM Ğ¿Ñ‹Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ²ÑÑ‘
- ĞĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾
- Ğ¢Ñ€ÑƒĞ´Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ: Multi-Agent System**

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚Agent Orchestratorâ”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                  â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚Research â”‚      â”‚  Code       â”‚    â”‚  Writing  â”‚
    â”‚ Agent   â”‚      â”‚  Agent      â”‚    â”‚  Agent    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
    [RAG: Docs]      [RAG: Code]       [RAG: Style]
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: Software Development Multi-Agent System**

```python
# orchestrator_service.py (Flask Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ)

class AgentOrchestrator:
    def __init__(self):
        # Ğ ĞµĞµÑÑ‚Ñ€ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
        self.agents = {
            'requirements': 'http://requirements-agent:8001',
            'architect': 'http://architect-agent:8002',
            'coder': 'http://coder-agent:8003',
            'tester': 'http://tester-agent:8004',
            'reviewer': 'http://reviewer-agent:8005',
        }

        # Message queue Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸
        self.queue = RedisQueue()

    def process_task(self, task_description: str):
        """Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸"""

        # 1. Requirements Agent Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ
        requirements = self.call_agent('requirements', {
            'task': task_description
        })

        # 2. Architect Agent Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ
        architecture = self.call_agent('architect', {
            'requirements': requirements
        })

        # 3. Coder Agent Ğ¿Ğ¸ÑˆĞµÑ‚ ĞºĞ¾Ğ´ (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾)
        code_tasks = architecture['modules']
        code_results = []
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(self.call_agent, 'coder', {'module': module})
                for module in code_tasks
            ]
            code_results = [f.result() for f in futures]

        # 4. Tester Agent Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚
        test_results = self.call_agent('tester', {
            'code': code_results
        })

        # 5. Reviewer Agent Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾
        review = self.call_agent('reviewer', {
            'code': code_results,
            'tests': test_results
        })

        return {
            'requirements': requirements,
            'architecture': architecture,
            'code': code_results,
            'tests': test_results,
            'review': review
        }

    def call_agent(self, agent_name: str, data: dict):
        """Ğ’Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· HTTP API"""
        url = self.agents[agent_name]
        response = requests.post(f"{url}/process", json=data)
        return response.json()

@app.route('/task', methods=['POST'])
def process_task():
    """Endpoint Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· multi-agent ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ"""
    task = request.json['task']
    result = orchestrator.process_task(task)
    return jsonify(result)
```

**ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ - Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ:**

```python
# coder_agent_service.py
class CoderAgent:
    def __init__(self):
        # Ğ¡Ğ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ RAG Ğ±Ğ°Ğ·Ğ° Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ°
        self.code_rag = CodeRAG()

        # LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°
        self.llm = LLMClient(model='codellama-34b')

    def process(self, task):
        """Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ"""
        module_spec = task['module']

        # ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² RAG
        examples = self.code_rag.find_similar(module_spec)

        # Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´
        code = self.llm.generate(
            prompt=f"Implement module: {module_spec}\nExamples:\n{examples}"
        )

        return {'code': code, 'module': module_spec}

@app.route('/process', methods=['POST'])
def process():
    data = request.json
    result = coder_agent.process(data)
    return jsonify(result)

if __name__ == '__main__':
    app.run(port=8003)
```

**Docker Compose Ğ´Ğ»Ñ Ğ²ÑĞµĞ¹ multi-agent ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹:**

```yaml
# multi-agent-system.yaml
services:
  orchestrator:
    image: agent-orchestrator:latest
    ports: ["9000:9000"]
    depends_on:
      - requirements-agent
      - architect-agent
      - coder-agent
      - tester-agent
      - reviewer-agent
    environment:
      - REDIS_URL=redis://redis:6379

  requirements-agent:
    image: requirements-agent:latest
    ports: ["8001:8001"]
    deploy:
      replicas: 2

  architect-agent:
    image: architect-agent:latest
    ports: ["8002:8002"]
    deploy:
      replicas: 2

  coder-agent:
    image: coder-agent:latest
    ports: ["8003:8003"]
    deploy:
      replicas: 5  # ĞœĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1

  tester-agent:
    image: tester-agent:latest
    ports: ["8004:8004"]
    deploy:
      replicas: 3

  reviewer-agent:
    image: reviewer-agent:latest
    ports: ["8005:8005"]
    deploy:
      replicas: 2

  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
```

## 6.5 Federated Learning: Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

### ğŸ”’ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

**Ğ¡Ñ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹:** ĞœĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· 100 Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†

âŒ **ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´:**
1. Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾ Ğ¼ĞµÑÑ‚Ğ¾
2. ĞĞ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
3. **ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** ĞĞ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²

âœ… **Federated Learning:**
1. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ğµ
2. Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ½Ğ° Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµÑ€Ğ²ĞµÑ€
3. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğµ Ğ¿Ğ¾ĞºĞ¸Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ñƒ

### ğŸ“Š ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Federated Learning

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Central Server     â”‚
                  â”‚  (Aggregator)       â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                  ĞĞ±Ğ¼ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚Hospital â”‚         â”‚Hospital â”‚        â”‚Hospital â”‚
    â”‚   1     â”‚         â”‚   2     â”‚        â”‚   3     â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
    [Local     ]        [Local     ]       [Local     ]
    [Data      ]        [Data      ]       [Data      ]
    [Never shared]      [Never shared]     [Never shared]
```

**ĞšĞ¾Ğ´ Federated Learning ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹:**

```python
# central_server.py (Flask)
class FederatedServer:
    def __init__(self):
        # Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        self.global_model = NeuralNetwork()

        # Ğ’ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        self.global_weights = self.global_model.get_weights()

        # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        self.clients = []

    def aggregate_weights(self, client_weights_list):
        """Ğ¤ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ (FedAvg)"""
        # Ğ£ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ‚ÑŒ Ğ²ĞµÑĞ° Ğ¾Ñ‚ Ğ²ÑĞµÑ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        avg_weights = []
        for layer_idx in range(len(client_weights_list[0])):
            # Ğ£ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ‚ÑŒ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
            layer_weights = [
                client_weights[layer_idx]
                for client_weights in client_weights_list
            ]
            avg_layer = np.mean(layer_weights, axis=0)
            avg_weights.append(avg_layer)

        return avg_weights

@app.route('/get_global_model', methods=['GET'])
def get_global_model():
    """ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ"""
    return jsonify({'weights': server.global_weights.tolist()})

@app.route('/submit_update', methods=['POST'])
def submit_update():
    """ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°"""
    client_id = request.json['client_id']
    client_weights = request.json['weights']

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²ĞµÑĞ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°
    server.clients.append({
        'id': client_id,
        'weights': np.array(client_weights)
    })

    # Ğ•ÑĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ²ĞµÑĞ° Ğ¾Ñ‚ Ğ²ÑĞµÑ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
    if len(server.clients) >= NUM_CLIENTS:
        # ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
        all_weights = [c['weights'] for c in server.clients]
        server.global_weights = server.aggregate_weights(all_weights)

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        server.global_model.set_weights(server.global_weights)

        # ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑƒĞ½Ğ´Ğ°
        server.clients = []

        return jsonify({'status': 'aggregated', 'round_complete': True})

    return jsonify({'status': 'received', 'round_complete': False})
```

**ĞšĞ»Ğ¸ĞµĞ½Ñ‚ (Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ğ°):**

```python
# hospital_client.py (Flask)
class HospitalClient:
    def __init__(self, client_id, central_server_url):
        self.client_id = client_id
        self.server_url = central_server_url

        # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ!)
        self.local_data = load_local_patient_data()

        # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¿Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        self.model = NeuralNetwork()

    def train_local(self, epochs=5):
        """ĞĞ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        # Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        response = requests.get(f"{self.server_url}/get_global_model")
        global_weights = np.array(response.json()['weights'])
        self.model.set_weights(global_weights)

        # ĞĞ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        for epoch in range(epochs):
            for batch in self.local_data:
                loss = self.model.train_step(batch)

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°
        updated_weights = self.model.get_weights()

        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²ĞµÑĞ° (ĞĞ• Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ!)
        requests.post(
            f"{self.server_url}/submit_update",
            json={
                'client_id': self.client_id,
                'weights': updated_weights.tolist()
            }
        )

@app.route('/start_training', methods=['POST'])
def start_training():
    """ĞĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ"""
    client.train_local(epochs=5)
    return jsonify({'status': 'training_complete'})

if __name__ == '__main__':
    # ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ğ° Ğ½Ğ° ÑĞ²Ğ¾ĞµĞ¼ Ğ¿Ğ¾Ñ€Ñ‚Ñƒ
    app.run(port=8000 + HOSPITAL_ID)
```

**Docker Compose Ğ´Ğ»Ñ Federated Learning:**

```yaml
# federated-learning.yaml
services:
  central-server:
    image: federated-server:latest
    ports: ["9000:9000"]

  hospital-1:
    image: hospital-client:latest
    environment:
      - CLIENT_ID=hospital_1
      - CENTRAL_SERVER=http://central-server:9000
    volumes:
      - ./hospital1_data:/data  # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    ports: ["8001:8001"]

  hospital-2:
    image: hospital-client:latest
    environment:
      - CLIENT_ID=hospital_2
      - CENTRAL_SERVER=http://central-server:9000
    volumes:
      - ./hospital2_data:/data
    ports: ["8002:8002"]

  hospital-3:
    image: hospital-client:latest
    environment:
      - CLIENT_ID=hospital_3
      - CENTRAL_SERVER=http://central-server:9000
    volumes:
      - ./hospital3_data:/data
    ports: ["8003:8003"]
```

## 6.6 Ğ˜Ñ‚Ğ¾Ğ³Ğ¸: Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ AI Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹

### ğŸ’¡ ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸:

1. **Data Parallelism** = ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ° (N Ñ€ĞµĞ¿Ğ»Ğ¸Ğº)
2. **Model Parallelism** = Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² (pipeline)
3. **Distributed Inference** = Load balancing Ğ¼ĞµĞ¶Ğ´Ñƒ N ĞºĞ¾Ğ¿Ğ¸ÑĞ¼Ğ¸
4. **Distributed RAG** = Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ RAG Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹
5. **Multi-Agent** = ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… AI Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
6. **Federated Learning** = Ğ”ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

### ğŸ† Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½:

```
AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° = ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° + ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹ + ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
```

---


# 7. ĞŸĞ ĞĞšĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ ĞĞ Ğ¥Ğ˜Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ: ĞŸĞĞ›ĞĞ«Ğ™ ĞŸĞ Ğ˜ĞœĞ•Ğ 

## 7.1 Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°: AI-Powered Data Science Platform

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Data Science Ñ:

1. **Flutter Frontend** (Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ)
2. **ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°** (Flask APIs)
3. **ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ** (Docker + Kubernetes)
4. **AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹** (ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸)
5. **Distributed RAG** (Ğ±Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹)
6. **Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ** (training cluster)

### ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FLUTTER APP (Layer 1)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Dashboard â”‚  â”‚  Chat    â”‚  â”‚Analytics â”‚  â”‚Training  â”‚   â”‚
â”‚  â”‚Screen    â”‚  â”‚  Screen  â”‚  â”‚Screen    â”‚  â”‚Screen    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTPS/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API GATEWAY (Nginx/Kong)                       â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚        â”‚        â”‚        â”‚        â”‚        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚Auth  â”‚ â”‚Data â”‚ â”‚ML    â”‚ â”‚Chat â”‚ â”‚Train â”‚ â”‚Viz   â”‚
â”‚API   â”‚ â”‚API  â”‚ â”‚API   â”‚ â”‚API  â”‚ â”‚API   â”‚ â”‚API   â”‚
â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚       â”‚        â”‚       â”‚        â”‚       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Message Bus (Kafka) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Data    â”‚     â”‚Code     â”‚     â”‚Research â”‚
â”‚Agent   â”‚     â”‚Agent    â”‚     â”‚Agent    â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚               â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Distributed RAG     â”‚
         â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
         â”‚  â”‚Vec1â”‚ â”‚Vec2â”‚ â”‚Vec3â”‚
         â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 7.2 Flutter Frontend (Layer 1)

### main.dart - Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°

```dart
// main.dart
import 'package:flutter/material.dart';
import 'package:provider/provider.dart';
import 'services/api_service.dart';
import 'services/auth_service.dart';
import 'screens/dashboard_screen.dart';
import 'screens/chat_screen.dart';
import 'screens/analytics_screen.dart';

void main() {
  runApp(
    MultiProvider(
      providers: [
        Provider<ApiService>(
          create: (_) => ApiService(baseUrl: 'https://api.dsplatform.com'),
        ),
        ChangeNotifierProvider<AuthService>(
          create: (context) => AuthService(context.read<ApiService>()),
        ),
      ],
      child: DataSciencePlatformApp(),
    ),
  );
}

class DataSciencePlatformApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'AI Data Science Platform',
      theme: ThemeData.dark(),
      home: MainNavigation(),
      routes: {
        '/dashboard': (context) => DashboardScreen(),
        '/chat': (context) => ChatScreen(),
        '/analytics': (context) => AnalyticsScreen(),
        '/training': (context) => TrainingScreen(),
      },
    );
  }
}
```

### ApiService - Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸

```dart
// services/api_service.dart
import 'package:dio/dio.dart';

class ApiService {
  final Dio _dio;
  final String baseUrl;

  ApiService({required this.baseUrl})
      : _dio = Dio(BaseOptions(
          baseUrl: baseUrl,
          connectTimeout: Duration(seconds: 10),
          receiveTimeout: Duration(seconds: 30),
        ));

  // Ğ’Ñ‹Ğ·Ğ¾Ğ² Auth Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°
  Future<String> login(String username, String password) async {
    final response = await _dio.post('/auth/login', data: {
      'username': username,
      'password': password,
    });
    return response.data['token'];
  }

  // Ğ’Ñ‹Ğ·Ğ¾Ğ² ML Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ°
  Future<Map<String, dynamic>> trainModel(Map<String, dynamic> config) async {
    final response = await _dio.post('/ml/train', data: config);
    return response.data;
  }

  // Ğ’Ñ‹Ğ·Ğ¾Ğ² Chat AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
  Future<String> chatWithAI(String message) async {
    final response = await _dio.post('/chat/message', data: {
      'message': message,
    });
    return response.data['response'];
  }

  // Ğ’Ñ‹Ğ·Ğ¾Ğ² Data API
  Future<List<dynamic>> loadDataset(String datasetId) async {
    final response = await _dio.get('/data/datasets/$datasetId');
    return response.data['data'];
  }

  // Ğ’Ñ‹Ğ·Ğ¾Ğ² Analytics API
  Future<Map<String, dynamic>> getAnalytics(String datasetId) async {
    final response = await _dio.get('/analytics/summary/$datasetId');
    return response.data;
  }
}
```

### ChatScreen - Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼

```dart
// screens/chat_screen.dart
class ChatScreen extends StatefulWidget {
  @override
  _ChatScreenState createState() => _ChatScreenState();
}

class _ChatScreenState extends State<ChatScreen> {
  final TextEditingController _messageController = TextEditingController();
  final List<ChatMessage> _messages = [];
  late ApiService _apiService;

  @override
  void initState() {
    super.initState();
    _apiService = context.read<ApiService>();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(title: Text('AI Data Science Assistant')),
      body: Column(
        children: [
          Expanded(
            child: ListView.builder(
              itemCount: _messages.length,
              itemBuilder: (context, index) {
                final message = _messages[index];
                return ChatBubble(
                  message: message.text,
                  isMe: message.isUser,
                  timestamp: message.timestamp,
                );
              },
            ),
          ),
          MessageInputBar(
            controller: _messageController,
            onSend: _sendMessage,
          ),
        ],
      ),
    );
  }

  Future<void> _sendMessage(String text) async {
    // Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    setState(() {
      _messages.add(ChatMessage(
        text: text,
        isUser: true,
        timestamp: DateTime.now(),
      ));
    });

    // Ğ’Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ AI Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ
    try {
      final response = await _apiService.chatWithAI(text);

      // Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ AI
      setState(() {
        _messages.add(ChatMessage(
          text: response,
          isUser: false,
          timestamp: DateTime.now(),
        ));
      });
    } catch (e) {
      _showError('Failed to get AI response: $e');
    }
  }
}
```

## 7.3 Backend ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ (Layer 2)

### Auth Service (Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸)

```python
# services/auth_service/app.py
from flask import Flask, request, jsonify
import jwt
import redis
from functools import wraps

app = Flask(__name__)
redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)

SECRET_KEY = "your-secret-key"

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = request.headers.get('Authorization')
        if not token:
            return jsonify({'error': 'Token required'}), 401
        try:
            data = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
            current_user = data['user']
        except:
            return jsonify({'error': 'Invalid token'}), 401
        return f(current_user, *args, **kwargs)
    return decorated

@app.route('/auth/login', methods=['POST'])
def login():
    """ĞÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ"""
    data = request.json
    username = data['username']
    password = data['password']

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° credentials (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾)
    if verify_credentials(username, password):
        token = jwt.encode({'user': username}, SECRET_KEY, algorithm="HS256")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² Redis
        redis_client.setex(f"session:{username}", 3600, token)

        return jsonify({'token': token, 'user': username}), 200

    return jsonify({'error': 'Invalid credentials'}), 401

@app.route('/auth/verify', methods=['POST'])
@token_required
def verify(current_user):
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°"""
    return jsonify({'valid': True, 'user': current_user}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)
```

### ML Service (Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)

```python
# services/ml_service/app.py
from flask import Flask, request, jsonify
from celery import Celery
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import joblib

app = Flask(__name__)

# Celery Ğ´Ğ»Ñ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
celery_app = Celery('ml_service',
                   broker='redis://redis:6379/0',
                   backend='redis://redis:6379/0')

@celery_app.task
def train_model_async(dataset_id, config):
    """Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    df = load_dataset(dataset_id)

    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ°
    X = df.drop('target', axis=1)
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
    model = RandomForestClassifier(**config)
    model.fit(X_train, y_train)

    # ĞÑ†ĞµĞ½ĞºĞ°
    score = model.score(X_test, y_test)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
    model_path = f'/models/{dataset_id}_model.joblib'
    joblib.dump(model, model_path)

    return {
        'status': 'success',
        'accuracy': score,
        'model_path': model_path
    }

@app.route('/ml/train', methods=['POST'])
def train():
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    data = request.json
    dataset_id = data['dataset_id']
    config = data.get('config', {})

    # Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ñ„Ğ¾Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ
    task = train_model_async.delay(dataset_id, config)

    return jsonify({
        'task_id': task.id,
        'status': 'training_started'
    }), 202

@app.route('/ml/predict', methods=['POST'])
def predict():
    """ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    data = request.json
    model_id = data['model_id']
    features = data['features']

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    model = joblib.load(f'/models/{model_id}_model.joblib')

    # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
    prediction = model.predict([features])

    return jsonify({
        'prediction': prediction.tolist(),
        'model_id': model_id
    }), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5003)
```

### Chat Service - Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸

```python
# services/chat_service/app.py
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

# URL AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
RESEARCH_AGENT_URL = 'http://research-agent:6001'
CODE_AGENT_URL = 'http://code-agent:6002'
DATA_AGENT_URL = 'http://data-agent:6003'

class ChatOrchestrator:
    """ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²"""

    def route_message(self, message: str) -> str:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ, ĞºĞ°ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ"""

        # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
        message_lower = message.lower()

        if any(word in message_lower for word in ['code', 'python', 'function']):
            return self.call_code_agent(message)

        elif any(word in message_lower for word in ['data', 'dataset', 'analyze']):
            return self.call_data_agent(message)

        else:
            return self.call_research_agent(message)

    def call_code_agent(self, message: str) -> str:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°"""
        response = requests.post(
            f"{CODE_AGENT_URL}/generate",
            json={'prompt': message}
        )
        return response.json()['code']

    def call_data_agent(self, message: str) -> str:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        response = requests.post(
            f"{DATA_AGENT_URL}/analyze",
            json={'query': message}
        )
        return response.json()['analysis']

    def call_research_agent(self, message: str) -> str:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹"""
        response = requests.post(
            f"{RESEARCH_AGENT_URL}/research",
            json={'query': message}
        )
        return response.json()['answer']

orchestrator = ChatOrchestrator()

@app.route('/chat/message', methods=['POST'])
def chat():
    """Endpoint Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° Ñ AI"""
    data = request.json
    message = data['message']

    # ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ
    response = orchestrator.route_message(message)

    return jsonify({'response': response}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5004)
```


## 7.4 AI ĞĞ³ĞµĞ½Ñ‚Ñ‹ (Layer 3)

### Code Generation Agent

```python
# agents/code_agent/app.py
from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

app = Flask(__name__)

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°
model_name = "Salesforce/codegen-350M-mono"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# RAG Ğ±Ğ°Ğ·Ğ° Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ°
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()
code_examples_db = FAISS.load_local("./code_examples_vectorstore", embeddings)

class CodeAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°"""

    def generate(self, prompt: str) -> str:
        """Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°"""

        # 1. ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² RAG
        similar_examples = code_examples_db.similarity_search(prompt, k=3)

        # 2. ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
        context = "\n\n".join([doc.page_content for doc in similar_examples])

        # 3. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
        full_prompt = f"""
        Here are some examples of similar code:

        {context}

        Now, generate code for: {prompt}

        Code:
        """

        # 4. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ
        inputs = tokenizer(full_prompt, return_tensors="pt")
        outputs = model.generate(
            **inputs,
            max_length=512,
            temperature=0.7,
            top_p=0.95
        )

        code = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return code

code_agent = CodeAgent()

@app.route('/generate', methods=['POST'])
def generate():
    """Endpoint Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°"""
    data = request.json
    prompt = data['prompt']

    code = code_agent.generate(prompt)

    return jsonify({'code': code}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=6002)
```

### Data Analysis Agent

```python
# agents/data_agent/app.py
from flask import Flask, request, jsonify
import pandas as pd
from langchain.chains import LLMChain
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate

app = Flask(__name__)

# LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
llm = HuggingFacePipeline.from_model_id(
    model_id="google/flan-t5-base",
    task="text2text-generation",
)

class DataAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""

    def __init__(self):
        self.llm = llm

    def analyze(self, query: str, dataset_id: str) -> dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°"""

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚
        df = self.load_dataset(dataset_id)

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
        stats = df.describe().to_dict()

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ LLM
        prompt_template = PromptTemplate(
            input_variables=["query", "stats"],
            template="""
            Given this dataset statistics:
            {stats}

            Answer the following question about the data:
            {query}

            Analysis:
            """
        )

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ
        chain = LLMChain(llm=self.llm, prompt=prompt_template)

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
        analysis = chain.run(query=query, stats=str(stats))

        return {
            'analysis': analysis,
            'statistics': stats,
            'dataset_shape': df.shape
        }

    def load_dataset(self, dataset_id: str) -> pd.DataFrame:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚"""
        # Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ
        return pd.read_csv(f'/data/{dataset_id}.csv')

data_agent = DataAgent()

@app.route('/analyze', methods=['POST'])
def analyze():
    """Endpoint Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    data = request.json
    query = data['query']
    dataset_id = data.get('dataset_id', 'default')

    result = data_agent.analyze(query, dataset_id)

    return jsonify(result), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=6003)
```

## 7.5 Distributed RAG (Layer 4)

### RAG Gateway

```python
# rag/gateway/app.py
from flask import Flask, request, jsonify
import requests
from concurrent.futures import ThreadPoolExecutor

app = Flask(__name__)

class RAGGateway:
    """Gateway Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""

    def __init__(self):
        # Ğ ĞµĞµÑÑ‚Ñ€ RAG Ğ½Ğ¾Ğ´Ğ¾Ğ²
        self.nodes = {
            'code': 'http://code-rag:7001',
            'data': 'http://data-rag:7002',
            'ml': 'http://ml-rag:7003',
            'docs': 'http://docs-rag:7004',
        }

    def query(self, question: str, domains: list = None) -> dict:
        """Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ"""

        # Ğ•ÑĞ»Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ
        if not domains:
            domains = list(self.nodes.keys())

        # ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ½Ğ¾Ğ´Ğ°Ğ¼
        results = []
        with ThreadPoolExecutor(max_workers=len(domains)) as executor:
            futures = []
            for domain in domains:
                node_url = self.nodes[domain]
                future = executor.submit(
                    self._query_node,
                    node_url,
                    question
                )
                futures.append((domain, future))

            # Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
            for domain, future in futures:
                try:
                    result = future.result(timeout=10)
                    results.append({
                        'domain': domain,
                        'documents': result
                    })
                except Exception as e:
                    print(f"Error querying {domain}: {e}")

        # Ğ Ğµ-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
        reranked = self._rerank_results(question, results)

        return {
            'question': question,
            'sources': reranked[:5],
            'domains_queried': domains
        }

    def _query_node(self, node_url: str, question: str) -> list:
        """Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ RAG Ğ½Ğ¾Ğ´Ñƒ"""
        response = requests.post(
            f"{node_url}/retrieve",
            json={'query': question},
            timeout=10
        )
        return response.json()['documents']

    def _rerank_results(self, question: str, results: list) -> list:
        """Ğ Ğµ-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ€Ğµ-Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ score
        all_docs = []
        for result in results:
            for doc in result['documents']:
                doc['domain'] = result['domain']
                all_docs.append(doc)

        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        sorted_docs = sorted(all_docs, key=lambda x: x['score'], reverse=True)

        return sorted_docs

rag_gateway = RAGGateway()

@app.route('/rag/query', methods=['POST'])
def query():
    """Endpoint Ğ´Ğ»Ñ RAG Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²"""
    data = request.json
    question = data['query']
    domains = data.get('domains', None)

    result = rag_gateway.query(question, domains)

    return jsonify(result), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=7000)
```

## 7.6 Docker Compose - Ğ²ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ============== INFRASTRUCTURE ==============

  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    networks: [platform-net]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: platform_db
      POSTGRES_USER: platform
      POSTGRES_PASSWORD: platform_pass
    volumes: [postgres-data:/var/lib/postgresql/data]
    networks: [platform-net]

  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on: [zookeeper]
    networks: [platform-net]

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks: [platform-net]

  # ============== API GATEWAY ==============

  nginx:
    image: nginx:alpine
    ports: ["80:80", "443:443"]
    volumes: ["./nginx.conf:/etc/nginx/nginx.conf:ro"]
    depends_on:
      - auth-service
      - ml-service
      - chat-service
    networks: [platform-net]

  # ============== ĞœĞ˜ĞšĞ ĞĞ¡Ğ•Ğ Ğ’Ğ˜Ğ¡Ğ« ==============

  auth-service:
    build: ./services/auth_service
    ports: ["5001:5001"]
    environment:
      REDIS_URL: redis://redis:6379
      SECRET_KEY: ${SECRET_KEY}
    depends_on: [redis]
    networks: [platform-net]
    deploy:
      replicas: 2

  data-service:
    build: ./services/data_service
    ports: ["5002:5002"]
    environment:
      POSTGRES_URL: postgresql://platform:platform_pass@postgres:5432/platform_db
    depends_on: [postgres]
    networks: [platform-net]
    deploy:
      replicas: 3

  ml-service:
    build: ./services/ml_service
    ports: ["5003:5003"]
    environment:
      CELERY_BROKER: redis://redis:6379/0
    depends_on: [redis]
    networks: [platform-net]
    volumes: ["./models:/models"]
    deploy:
      replicas: 2
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  chat-service:
    build: ./services/chat_service
    ports: ["5004:5004"]
    depends_on:
      - research-agent
      - code-agent
      - data-agent
    networks: [platform-net]
    deploy:
      replicas: 3

  # ============== AI ĞĞ“Ğ•ĞĞ¢Ğ« ==============

  research-agent:
    build: ./agents/research_agent
    ports: ["6001:6001"]
    networks: [platform-net]
    deploy:
      replicas: 2

  code-agent:
    build: ./agents/code_agent
    ports: ["6002:6002"]
    volumes: ["./code_examples_vectorstore:/app/code_examples_vectorstore:ro"]
    networks: [platform-net]
    deploy:
      replicas: 2
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1

  data-agent:
    build: ./agents/data_agent
    ports: ["6003:6003"]
    volumes: ["./data:/data:ro"]
    networks: [platform-net]
    deploy:
      replicas: 2

  # ============== DISTRIBUTED RAG ==============

  rag-gateway:
    build: ./rag/gateway
    ports: ["7000:7000"]
    depends_on:
      - code-rag
      - data-rag
      - ml-rag
      - docs-rag
    networks: [platform-net]
    deploy:
      replicas: 3

  code-rag:
    build: ./rag/nodes/code
    ports: ["7001:7001"]
    volumes: ["./vectorstores/code:/vectorstore"]
    networks: [platform-net]

  data-rag:
    build: ./rag/nodes/data
    ports: ["7002:7002"]
    volumes: ["./vectorstores/data:/vectorstore"]
    networks: [platform-net]

  ml-rag:
    build: ./rag/nodes/ml
    ports: ["7003:7003"]
    volumes: ["./vectorstores/ml:/vectorstore"]
    networks: [platform-net]

  docs-rag:
    build: ./rag/nodes/docs
    ports: ["7004:7004"]
    volumes: ["./vectorstores/docs:/vectorstore"]
    networks: [platform-net]

networks:
  platform-net:
    driver: bridge

volumes:
  postgres-data:
  models-data:
```

## 7.7 Kubernetes Deployment (Production)

```yaml
# kubernetes/platform-deployment.yaml

# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: ds-platform
---
# Auth Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
  namespace: ds-platform
spec:
  replicas: 3
  selector:
    matchLabels:
      app: auth-service
  template:
    metadata:
      labels:
        app: auth-service
    spec:
      containers:
      - name: auth
        image: dsplatform/auth-service:1.0
        ports:
        - containerPort: 5001
        env:
        - name: REDIS_URL
          value: "redis://redis:6379"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
# ML Service with GPU
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-service
  namespace: ds-platform
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-service
  template:
    metadata:
      labels:
        app: ml-service
    spec:
      containers:
      - name: ml
        image: dsplatform/ml-service:1.0
        ports:
        - containerPort: 5003
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: 1
---
# Code Agent
apiVersion: apps/v1
kind: Deployment
metadata:
  name: code-agent
  namespace: ds-platform
spec:
  replicas: 3
  selector:
    matchLabels:
      app: code-agent
  template:
    metadata:
      labels:
        app: code-agent
    spec:
      containers:
      - name: agent
        image: dsplatform/code-agent:1.0
        ports:
        - containerPort: 6002
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
            nvidia.com/gpu: 1
          limits:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
---
# Horizontal Pod Autoscaler Ğ´Ğ»Ñ ML Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-service-hpa
  namespace: ds-platform
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## 7.8 Ğ˜Ñ‚Ğ¾Ğ³Ğ¸: ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°

### ğŸ’¡ Ğ§Ñ‚Ğ¾ Ğ¼Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¸:

1. **Flutter Frontend** â†’ Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹ Ğ¾Ñ‚ Ğ½Ğ°Ğ½Ğ¾ Ğ´Ğ¾ Ğ³Ğ¸Ğ³Ğ°
2. **API Gateway** â†’ ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
3. **ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹** â†’ ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ (Auth, ML, Chat)
4. **ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹** â†’ Docker Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
5. **ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ** â†’ Kubernetes Ğ´Ğ»Ñ production
6. **AI ĞĞ³ĞµĞ½Ñ‚Ñ‹** â†’ Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸
7. **Distributed RAG** â†’ Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹
8. **ĞĞ²Ñ‚Ğ¾ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³** â†’ Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

### ğŸ¯ Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…:

```
ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ + ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ + Ğ˜Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ + ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ
```

---


# 8. Ğ‘Ğ£Ğ”Ğ£Ğ©Ğ•Ğ•: ĞšĞĞĞ’Ğ•Ğ™Ğ•Ğ Ğ« AI ĞœĞ˜ĞšĞ ĞĞ¡Ğ•Ğ Ğ’Ğ˜Ğ¡ĞĞ’

## 8.1 Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ AI ÑĞ¸ÑÑ‚ĞµĞ¼

### ğŸ“ˆ ĞŸÑƒÑ‚ÑŒ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ° Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°Ğ¼

```
2020: ĞœĞ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ñ‹Ğµ LLM (GPT-3)
  â†“
2022: Fine-tuned Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ)
  â†“
2023: RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ (Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ)
  â†“
2024: Multi-agent ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ (ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹)
  â†“
2025: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰Ğ¸ĞµÑÑ AI ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ â† ĞœĞ« Ğ—Ğ”Ğ•Ğ¡Ğ¬
  â†“
2026+: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ AI ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
```

## 8.2 Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰Ğ¸ĞµÑÑ AI ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹

### ğŸ’¡ ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ: AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ AI

**Ğ˜Ğ´ĞµÑ:** AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¸ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Meta-Agent          â”‚
                â”‚  (ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€)       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Agent Factory       â”‚
                â”‚  (Ğ¤Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
            Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
                           â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                   â”‚                   â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Agent  â”‚          â”‚Agent    â”‚        â”‚Agent    â”‚
   â”‚  A    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   B     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚   C     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”˜  Pipeline â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Pipelineâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                      Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
```

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ĞºĞ¾Ğ´Ğ°: Meta-Agent

```python
# meta_agent/orchestrator.py

from typing import List, Dict
import docker
import kubernetes

class MetaAgent:
    """ĞĞ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²"""

    def __init__(self):
        self.docker_client = docker.from_env()
        self.k8s_client = kubernetes.client.AppsV1Api()

        # Ğ ĞµĞµÑÑ‚Ñ€ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
        self.agent_templates = {
            'code_generator': 'dsplatform/code-agent:latest',
            'data_analyzer': 'dsplatform/data-agent:latest',
            'text_summarizer': 'dsplatform/summary-agent:latest',
            'image_processor': 'dsplatform/image-agent:latest',
            'researcher': 'dsplatform/research-agent:latest',
        }

    def solve_task(self, task_description: str) -> Dict:
        """Ğ ĞµÑˆĞ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ pipeline Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²"""

        # Ğ¨Ğ°Ğ³ 1: ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM
        task_analysis = self.analyze_task(task_description)

        # Ğ¨Ğ°Ğ³ 2: ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
        required_agents = task_analysis['required_agents']

        # Ğ¨Ğ°Ğ³ 3: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ pipeline
        pipeline = self.create_pipeline(required_agents)

        # Ğ¨Ğ°Ğ³ 4: Ğ Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
        deployed_agents = self.deploy_agents(pipeline)

        # Ğ¨Ğ°Ğ³ 5: Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡ĞµÑ€ĞµĞ· pipeline
        result = self.execute_pipeline(deployed_agents, task_description)

        # Ğ¨Ğ°Ğ³ 6: ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹
        self.cleanup_agents(deployed_agents)

        return result

    def analyze_task(self, task: str) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM"""

        prompt = f"""
        Analyze this task and determine what types of AI agents are needed:

        Task: {task}

        Respond with a JSON object:
        {{
            "required_agents": ["agent_type_1", "agent_type_2", ...],
            "pipeline_structure": "sequential" or "parallel",
            "estimated_complexity": "low" | "medium" | "high"
        }}
        """

        # Ğ’Ñ‹Ğ·Ğ¾Ğ² LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        response = self.llm.generate(prompt)

        return json.loads(response)

    def create_pipeline(self, agent_types: List[str]) -> Dict:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ pipeline"""

        pipeline = {
            'stages': [],
            'connections': []
        }

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ stage Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
        for i, agent_type in enumerate(agent_types):
            stage = {
                'id': f'agent_{i}',
                'type': agent_type,
                'image': self.agent_templates[agent_type],
                'replicas': self.determine_replicas(agent_type)
            }
            pipeline['stages'].append(stage)

            # Ğ¡Ğ²ÑĞ·ÑŒ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ stage
            if i > 0:
                pipeline['connections'].append({
                    'from': f'agent_{i-1}',
                    'to': f'agent_{i}'
                })

        return pipeline

    def deploy_agents(self, pipeline: Dict) -> List[Dict]:
        """Ğ Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Kubernetes"""

        deployed = []

        for stage in pipeline['stages']:
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Deployment Ğ² Kubernetes
            deployment = self.create_k8s_deployment(
                name=stage['id'],
                image=stage['image'],
                replicas=stage['replicas']
            )

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Service
            service = self.create_k8s_service(
                name=stage['id'],
                port=8000
            )

            deployed.append({
                'id': stage['id'],
                'type': stage['type'],
                'url': f"http://{stage['id']}:8000"
            })

        return deployed

    def execute_pipeline(self, agents: List[Dict], task: str) -> Dict:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡ĞµÑ€ĞµĞ· pipeline Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²"""

        result = {'input': task}

        # ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
        for agent in agents:
            response = requests.post(
                f"{agent['url']}/process",
                json=result
            )
            result = response.json()

        return result

    def determine_replicas(self, agent_type: str) -> int:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        complexity_map = {
            'code_generator': 3,
            'data_analyzer': 2,
            'text_summarizer': 1,
            'image_processor': 4,
            'researcher': 2,
        }
        return complexity_map.get(agent_type, 1)

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
meta_agent = MetaAgent()

task = """
Analyze the dataset 'sales_2024.csv',
generate Python code for visualization,
summarize insights,
and create a report with charts.
"""

result = meta_agent.solve_task(task)
print(result)
```

## 8.3 Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹

### ğŸ§  ĞĞ³ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğµ

**ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ:** ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑĞ²Ğ¾ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ

```python
# self_learning_agent/agent.py

class SelfLearningAgent:
    """ĞĞ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ ÑĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼"""

    def __init__(self, agent_id: str):
        self.agent_id = agent_id

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        self.metrics = {
            'total_tasks': 0,
            'successful_tasks': 0,
            'failed_tasks': 0,
            'average_response_time': 0,
            'user_satisfaction_score': 0
        }

        # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
        self.execution_history = []

        # RAG Ğ±Ğ°Ğ·Ğ° Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸
        self.best_practices_rag = FAISS.load_local(
            f"./best_practices_{agent_id}",
            embeddings
        )

    def process(self, task: Dict) -> Dict:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼"""

        start_time = time.time()

        # Ğ¨Ğ°Ğ³ 1: ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        similar_tasks = self._find_similar_successful_tasks(task)

        # Ğ¨Ğ°Ğ³ 2: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ best practices
        best_practices = self._extract_best_practices(similar_tasks)

        # Ğ¨Ğ°Ğ³ 3: Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°
        try:
            result = self._execute_with_practices(task, best_practices)

            # Ğ£ÑĞ¿ĞµÑ…
            self._record_success(task, result, time.time() - start_time)

            return result

        except Exception as e:
            # ĞĞµÑƒĞ´Ğ°Ñ‡Ğ° - Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ°Ñ‚ÑŒÑÑ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´
            self._record_failure(task, str(e))

            # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°
            alternative_result = self._execute_alternative(task)

            return alternative_result

    def _find_similar_successful_tasks(self, task: Dict) -> List[Dict]:
        """ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸"""

        # Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        task_embedding = self.embeddings.embed_query(str(task))

        similar = [
            exec_record for exec_record in self.execution_history
            if exec_record['status'] == 'success' and
            self._similarity(task_embedding, exec_record['embedding']) > 0.8
        ]

        return similar

    def _execute_with_practices(self, task: Dict, practices: List[str]) -> Dict:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ best practices"""

        # ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ñ best practices
        prompt = f"""
        Task: {task['description']}

        Best practices from previous similar tasks:
        {chr(10).join(practices)}

        Execute the task following these best practices:
        """

        result = self.llm.generate(prompt)

        return {'result': result, 'practices_used': practices}

    def _record_success(self, task: Dict, result: Dict, duration: float):
        """Ğ—Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ"""

        self.metrics['total_tasks'] += 1
        self.metrics['successful_tasks'] += 1

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
        self.execution_history.append({
            'task': task,
            'result': result,
            'duration': duration,
            'status': 'success',
            'timestamp': datetime.now(),
            'embedding': self.embeddings.embed_query(str(task))
        })

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² RAG best practices
        self.best_practices_rag.add_texts([
            f"Task: {task['description']}\nSolution: {result['result']}"
        ])

        # ĞŸĞµÑ€ĞµÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        self._update_metrics()

    def _record_failure(self, task: Dict, error: str):
        """Ğ—Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""

        self.metrics['total_tasks'] += 1
        self.metrics['failed_tasks'] += 1

        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
        error_analysis = self._analyze_error(task, error)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        self.execution_history.append({
            'task': task,
            'error': error,
            'error_analysis': error_analysis,
            'status': 'failed',
            'timestamp': datetime.now()
        })

    def get_performance_report(self) -> Dict:
        """ĞÑ‚Ñ‡ĞµÑ‚ Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""

        success_rate = (
            self.metrics['successful_tasks'] / self.metrics['total_tasks']
            if self.metrics['total_tasks'] > 0 else 0
        )

        return {
            'agent_id': self.agent_id,
            'total_tasks': self.metrics['total_tasks'],
            'success_rate': success_rate,
            'average_response_time': self.metrics['average_response_time'],
            'recommendations': self._generate_self_improvement_recommendations()
        }

    def _generate_self_improvement_recommendations(self) -> List[str]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ"""

        recommendations = []

        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
        if self.metrics['failed_tasks'] > self.metrics['successful_tasks'] * 0.2:
            recommendations.append(
                "High failure rate detected. Recommend retraining with recent successful examples."
            )

        if self.metrics['average_response_time'] > 10:
            recommendations.append(
                "High response time. Consider model optimization or adding cache layer."
            )

        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
        error_patterns = self._analyze_error_patterns()
        if error_patterns:
            recommendations.append(
                f"Common error patterns found: {error_patterns}. Recommend specialized handling."
            )

        return recommendations
```

## 8.4 ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ²

### ğŸ“Š AI ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ°Ğ¼Ğ¸

**ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ:** Meta-agent Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚/ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

```python
# auto_scaler/ai_autoscaler.py

class AIAutoscaler:
    """ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI pipeline"""

    def __init__(self):
        self.k8s_client = kubernetes.client.AppsV1Api()
        self.metrics_client = prometheus_client.PrometheusConnect()

    def monitor_and_scale(self):
        """ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ"""

        while True:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²ÑĞµÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
            agents_metrics = self.get_all_agents_metrics()

            for agent_id, metrics in agents_metrics.items():
                # ĞŸÑ€Ğ¸Ğ½ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸
                scaling_decision = self.decide_scaling(agent_id, metrics)

                if scaling_decision['action'] == 'scale_up':
                    self.scale_up(agent_id, scaling_decision['target_replicas'])

                elif scaling_decision['action'] == 'scale_down':
                    self.scale_down(agent_id, scaling_decision['target_replicas'])

            time.sleep(30)  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 30 ÑĞµĞºÑƒĞ½Ğ´

    def decide_scaling(self, agent_id: str, metrics: Dict) -> Dict:
        """Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ AI"""

        # Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸
        current_load = metrics['request_rate']
        current_latency = metrics['avg_latency']
        current_replicas = metrics['replicas']
        error_rate = metrics['error_rate']

        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ
        prompt = f"""
        Agent: {agent_id}
        Current metrics:
        - Request rate: {current_load} req/s
        - Average latency: {current_latency} ms
        - Current replicas: {current_replicas}
        - Error rate: {error_rate}%

        Decide scaling action:
        - If load is high (>80% capacity) or latency is high (>500ms): scale_up
        - If load is low (<20% capacity) for prolonged time: scale_down
        - Otherwise: no_action

        Respond with JSON:
        {{
            "action": "scale_up" | "scale_down" | "no_action",
            "target_replicas": number,
            "reason": "explanation"
        }}
        """

        response = self.llm.generate(prompt)
        decision = json.loads(response)

        return decision

    def scale_up(self, agent_id: str, target_replicas: int):
        """Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""

        print(f"Scaling UP {agent_id} to {target_replicas} replicas")

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Deployment Ğ² Kubernetes
        deployment = self.k8s_client.read_namespaced_deployment(
            name=agent_id,
            namespace='ds-platform'
        )

        deployment.spec.replicas = target_replicas

        self.k8s_client.patch_namespaced_deployment(
            name=agent_id,
            namespace='ds-platform',
            body=deployment
        )

    def scale_down(self, agent_id: str, target_replicas: int):
        """Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ¿Ğ»Ğ¸Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""

        print(f"Scaling DOWN {agent_id} to {target_replicas} replicas")

        # ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ scale_up
        deployment = self.k8s_client.read_namespaced_deployment(
            name=agent_id,
            namespace='ds-platform'
        )

        deployment.spec.replicas = max(1, target_replicas)  # ĞœĞ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 1 Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°

        self.k8s_client.patch_namespaced_deployment(
            name=agent_id,
            namespace='ds-platform',
            body=deployment
        )
```

## 8.5 Ğ‘ÑƒĞ´ÑƒÑ‰ĞµĞµ: ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ AI ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

### ğŸš€ Ğ’Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: AI ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°

**2026-2030: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ AI ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Ğ¡ĞĞœĞĞĞ Ğ“ĞĞĞ˜Ğ—Ğ£Ğ®Ğ©ĞĞ¯Ğ¡Ğ¯ AI Ğ­ĞšĞĞ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Meta-Meta-Agent (Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹)           â”‚  â”‚
â”‚  â”‚  - Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸     â”‚  â”‚
â”‚  â”‚  - ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²ÑĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹           â”‚  â”‚
â”‚  â”‚  - ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚Ğ¸               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Agent Factory (Ğ¤Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²)                    â”‚  â”‚
â”‚  â”‚  - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ´ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²      â”‚  â”‚
â”‚  â”‚  - ĞĞ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸                            â”‚  â”‚
â”‚  â”‚  - Ğ Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ² production                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Pool Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (ÑĞ¾Ñ‚Ğ½Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²)           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚Agent â”‚ â”‚Agent â”‚ â”‚Agent â”‚ ...  â”‚Agent â”‚         â”‚  â”‚
â”‚  â”‚  â”‚  1   â”‚ â”‚  2   â”‚ â”‚  3   â”‚      â”‚  N   â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â”‚  Ğ’ÑĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                            â”‚
â”‚  Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:                                          â”‚
â”‚  âœ… ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚ÑŒ (Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°)                   â”‚
â”‚  âœ… Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ (ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²)                â”‚
â”‚  âœ… Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹)              â”‚
â”‚  âœ… Ğ¡Ğ°Ğ¼Ğ¾Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ±Ğ¾ĞµĞ²)                 â”‚
â”‚  âœ… Ğ¡Ğ°Ğ¼Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹)           â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°

```python
# future/autonomous_ecosystem.py

class AutonomousAIEcosystem:
    """ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰Ğ°ÑÑÑ AI ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°"""

    def __init__(self):
        # Meta-Meta-Agent
        self.meta_agent = MetaMetaAgent()

        # Ğ¤Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
        self.agent_factory = AutonomousAgentFactory()

        # Pool Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
        self.active_agents = {}

        # Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
        self.monitoring = EcosystemMonitoring()

        # Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼
        self.evolution_engine = EvolutionEngine()

    def run_autonomous(self):
        """ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""

        while True:
            # 1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ
            state = self.monitoring.get_ecosystem_state()

            # 2. Meta-agent Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ
            decisions = self.meta_agent.make_strategic_decisions(state)

            # 3. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
            if decisions['create_new_agents']:
                new_agents = self.agent_factory.create_agents(
                    decisions['agent_specs']
                )
                self.deploy_new_agents(new_agents)

            # 4. Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
            if decisions['remove_agents']:
                self.remove_inefficient_agents(decisions['agents_to_remove'])

            # 5. Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²
            self.evolution_engine.evolve_agents(self.active_agents)

            # 6. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
            self.optimize_architecture(state)

            time.sleep(300)  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚

class MetaMetaAgent:
    """ĞĞ³ĞµĞ½Ñ‚ Ğ²Ñ‹ÑÑˆĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ - ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹"""

    def make_strategic_decisions(self, state: Dict) -> Dict:
        """ĞŸÑ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹"""

        prompt = f"""
        Current ecosystem state:
        - Total agents: {state['total_agents']}
        - System load: {state['system_load']}%
        - Success rate: {state['success_rate']}%
        - New task types detected: {state['new_task_types']}

        Make strategic decisions:
        1. Should we create new agent types?
        2. Should we remove underperforming agents?
        3. Should we modify the architecture?
        4. Should we allocate more resources?

        Respond with detailed plan and reasoning.
        """

        response = self.llm.generate(prompt)

        # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹
        decisions = self.parse_decisions(response)

        return decisions
```

## 8.6 Ğ˜Ñ‚Ğ¾Ğ³Ğ¸: ĞÑ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼

### ğŸ’¡ ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹:

**ĞĞ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ (2024-2025):**
- âœ… ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°
- âœ… ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Docker, Kubernetes)
- âœ… Multi-agent ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
- âœ… Distributed RAG
- âœ… Federated Learning

**Ğ‘Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ (2025-2027):**
- ğŸ”® Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑÑ‰Ğ¸ĞµÑÑ AI ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹
- ğŸ”® Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹
- ğŸ”® ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI
- ğŸ”® Meta-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

**Ğ”Ğ°Ğ»ÑŒĞ½ĞµĞµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ (2027+):**
- ğŸš€ ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ AI ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
- ğŸš€ Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
- ğŸš€ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
- ğŸš€ Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹

### ğŸ¯ Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞµ ÑĞ¿Ğ¾Ñ…Ğ¸:

```
ĞœĞĞ”Ğ£Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ¬ â†’ ĞšĞĞœĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ¯ â†’ ĞĞ’Ğ¢ĞĞĞĞœĞĞĞ¡Ğ¢Ğ¬ â†’ Ğ­Ğ’ĞĞ›Ğ®Ğ¦Ğ˜Ğ¯
```

**ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ:**

1. **Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚** â†’ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ UI ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚
2. **ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ** â†’ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ backend
3. **ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€** â†’ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ
4. **AI ĞĞ³ĞµĞ½Ñ‚** â†’ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚
5. **Meta-Agent** â†’ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
6. **Ğ­ĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°** â†’ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ

### ğŸŒŸ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ñ‹ÑĞ»ÑŒ:

**Ğ‘ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ - ÑÑ‚Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ.**

**Ğ‘ÑƒĞ´ÑƒÑ‰ĞµĞµ - ÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ:**
- Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ‡Ğ°Ñ‚ÑÑ
- Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚
- Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸
- Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ

**Ğ˜ Ğ²ÑĞµ ÑÑ‚Ğ¾ - Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ĞœĞĞ”Ğ£Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ˜.**

---

## ğŸ“š Ğ—ĞĞšĞ›Ğ®Ğ§Ğ•ĞĞ˜Ğ•

ĞœÑ‹ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹:

1. **ĞœĞ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹** - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ backend
2. **ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹** - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ
3. **Flutter Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹** - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ UI
4. **AI Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹** - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°
5. **RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹** - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹
6. **Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ** - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
7. **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°** - Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ
8. **Ğ‘ÑƒĞ´ÑƒÑ‰ĞµĞµ** - ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼

### Ğ’ÑĞµ ÑÑ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸ĞµĞ¹:

**Ğ‘ĞĞ›Ğ¬Ğ¨ĞĞ¯ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ = ĞšĞĞœĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ¯ ĞœĞĞ›Ğ«Ğ¥ ĞĞ•Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞ«Ğ¥ ĞœĞĞ”Ğ£Ğ›Ğ•Ğ™**

Ğ­Ñ‚Ğ° Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ° Ğ²ĞµĞ·Ğ´Ğµ:
- ĞÑ‚ ĞºÑ€Ğ¾ÑˆĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ¾ Ñ†ĞµĞ»Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
- ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Flask API Ğ´Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· ÑĞ¾Ñ‚ĞµĞ½ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
- ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ AI Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

**ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ - ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.**

**ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ - ÑÑ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.**

---

**ĞšĞ¾Ğ½ĞµÑ† Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°**

*Ğ’ĞµÑ€ÑĞ¸Ñ: 1.0*  
*Ğ”Ğ°Ñ‚Ğ°: 2026-01-08*  
*ĞĞ²Ñ‚Ğ¾Ñ€: AI Data Science Platform Team*

