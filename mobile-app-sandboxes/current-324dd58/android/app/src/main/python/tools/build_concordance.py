#!/usr/bin/env python3
"""
Advanced Concordance Builder - –°—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
–°–æ–∑–¥–∞—ë—Ç –ø–æ–ª–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Å–ª–æ–≤ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Ö –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è + KWIC, N-–≥—Ä–∞–º–º—ã, TF-IDF

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Concordantia Sacrorum Bibliorum (1230 –≥.), KWIC indexing (Hans Peter Luhn, 1960)

Features:
- Full concordance with word locations
- KWIC (Key Word In Context) display
- N-gram analysis (bigrams, trigrams)
- TF-IDF scoring for term importance
- Co-occurrence analysis
- HTML visualization
- Advanced filtering
- Phrase search
- Word proximity detection
- Statistics dashboard
"""

import os
import re
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Optional
import json
import argparse
import math


class KWICGenerator:
    """KWIC (Key Word In Context) generator"""

    def __init__(self, window_size=80):
        self.window_size = window_size

    def generate_kwic(self, text: str, keyword: str) -> List[Dict]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è KWIC –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        Returns: [{'left': str, 'keyword': str, 'right': str, 'position': int}]
        """
        kwic_entries = []
        keyword_lower = keyword.lower()
        text_lower = text.lower()

        # –ù–∞–π—Ç–∏ –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
        pos = 0
        while True:
            pos = text_lower.find(keyword_lower, pos)
            if pos == -1:
                break

            # –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞
            left_start = max(0, pos - self.window_size // 2)
            right_end = min(len(text), pos + len(keyword) + self.window_size // 2)

            left_context = text[left_start:pos].strip()
            keyword_match = text[pos:pos + len(keyword)]
            right_context = text[pos + len(keyword):right_end].strip()

            # –î–æ–±–∞–≤–∏—Ç—å –º–Ω–æ–≥–æ—Ç–æ—á–∏—è
            if left_start > 0:
                left_context = '...' + left_context
            if right_end < len(text):
                right_context = right_context + '...'

            kwic_entries.append({
                'left': left_context,
                'keyword': keyword_match,
                'right': right_context,
                'position': pos
            })

            pos += len(keyword)

        return kwic_entries


class NGramAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä N-–≥—Ä–∞–º–º (–±–∏–≥—Ä–∞–º–º—ã, —Ç—Ä–∏–≥—Ä–∞–º–º—ã)"""

    def __init__(self, stop_words: set):
        self.stop_words = stop_words

    def extract_ngrams(self, text: str, n: int = 2) -> List[Tuple[str, ...]]:
        """–ò–∑–≤–ª–µ—á—å N-–≥—Ä–∞–º–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç
        clean_text = re.sub(r'[^–∞-—è—ëa-z\s]', ' ', text.lower())
        words = clean_text.split()

        # –£–±—Ä–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = [w for w in words if w not in self.stop_words and len(w) >= 3]

        # –°–æ–∑–¥–∞—Ç—å N-–≥—Ä–∞–º–º—ã
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = tuple(words[i:i + n])
            ngrams.append(ngram)

        return ngrams

    def get_top_ngrams(self, ngrams: List[Tuple[str, ...]], top_n: int = 20) -> List[Tuple[Tuple[str, ...], int]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö N-–≥—Ä–∞–º–º"""
        counter = Counter(ngrams)
        return counter.most_common(top_n)


class TFIDFCalculator:
    """–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä TF-IDF (Term Frequency-Inverse Document Frequency)"""

    def __init__(self):
        self.documents = {}  # {doc_id: [words]}
        self.idf_cache = {}

    def add_document(self, doc_id: str, words: List[str]):
        """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"""
        self.documents[doc_id] = words

    def calculate_tf(self, word: str, doc_id: str) -> float:
        """
        Term Frequency: TF(word) = count(word in doc) / total_words_in_doc
        """
        if doc_id not in self.documents:
            return 0.0

        words = self.documents[doc_id]
        if not words:
            return 0.0

        word_count = words.count(word.lower())
        return word_count / len(words)

    def calculate_idf(self, word: str) -> float:
        """
        Inverse Document Frequency: IDF(word) = log(total_docs / docs_containing_word)
        """
        if word in self.idf_cache:
            return self.idf_cache[word]

        total_docs = len(self.documents)
        docs_with_word = sum(1 for words in self.documents.values() if word.lower() in words)

        if docs_with_word == 0:
            idf = 0.0
        else:
            idf = math.log(total_docs / docs_with_word)

        self.idf_cache[word] = idf
        return idf

    def calculate_tfidf(self, word: str, doc_id: str) -> float:
        """
        TF-IDF = TF √ó IDF
        """
        tf = self.calculate_tf(word, doc_id)
        idf = self.calculate_idf(word)
        return tf * idf

    def get_document_keywords(self, doc_id: str, top_n: int = 10) -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ TF-IDF"""
        if doc_id not in self.documents:
            return []

        words = set(self.documents[doc_id])
        scores = [(word, self.calculate_tfidf(word, doc_id)) for word in words]
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_n]


class CooccurrenceAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤"""

    def __init__(self, window_size=10):
        self.window_size = window_size
        self.cooccurrences = defaultdict(lambda: defaultdict(int))

    def analyze_text(self, words: List[str]):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –≤ –æ–∫–Ω–µ"""
        for i, word in enumerate(words):
            # –°–ª–æ–≤–∞ –≤ –æ–∫–Ω–µ
            window_start = max(0, i - self.window_size)
            window_end = min(len(words), i + self.window_size + 1)

            for j in range(window_start, window_end):
                if i != j:
                    other_word = words[j]
                    self.cooccurrences[word][other_word] += 1

    def get_related_words(self, word: str, top_n: int = 10) -> List[Tuple[str, int]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞, —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ä—è–¥–æ–º"""
        if word not in self.cooccurrences:
            return []

        related = self.cooccurrences[word].items()
        related = sorted(related, key=lambda x: x[1], reverse=True)
        return related[:top_n]


class ConcordanceBuilder:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–∞
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.concordance = defaultdict(list)

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.kwic_gen = KWICGenerator(window_size=80)
        self.tfidf_calc = TFIDFCalculator()
        self.cooccurrence = CooccurrenceAnalyzer(window_size=10)

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞)
        self.stop_words = {
            # –†—É—Å—Å–∫–∏–µ
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫',
            '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫',
            '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ',
            '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å',
            '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏',
            '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å',
            '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π',
            '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è',
            '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑',
            '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂',
            '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π',
            '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π',
            '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö',
            '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π',
            '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏',
            '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ',
            '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π',
            '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π',
            '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É',

            # English
            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
            'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
            'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they',
            'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one',
            'all', 'would', 'there', 'their', 'what', 'so', 'up', 'out',
            'if', 'about', 'who', 'get', 'which', 'go', 'me', 'when',
            'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know',
            'take', 'people', 'into', 'year', 'your', 'good', 'some',
            'could', 'them', 'see', 'other', 'than', 'then', 'now',
            'look', 'only', 'come', 'its', 'over', 'think', 'also',
            'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first',
            'well', 'way', 'even', 'new', 'want', 'because', 'any',
            'these', 'give', 'day', 'most', 'us', 'is', 'was', 'are',
            'been', 'has', 'had', 'were', 'said', 'did', 'having',
            'may', 'should', 'does', 'being'
        }

        self.ngram_analyzer = NGramAnalyzer(self.stop_words)

    def extract_words(self, text, file_path):
        """
        –ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: —Å–ø–∏—Å–æ–∫ (—Å–ª–æ–≤–æ, –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        """
        words = []
        lines = text.split('\n')

        for line_num, line in enumerate(lines, 1):
            # –£–¥–∞–ª–∏—Ç—å markdown —Ä–∞–∑–º–µ—Ç–∫—É
            clean_line = re.sub(r'[#*`\[\]()]', ' ', line)

            # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞)
            found_words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', clean_line.lower())

            for word in found_words:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                if word in self.stop_words:
                    continue

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–∏—Å–ª–∞
                if word.isdigit():
                    continue

                # –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å–ª–æ–≤–∞ –≤–æ–∫—Ä—É–≥)
                context = self.get_context(clean_line, word, window=40)

                words.append({
                    'word': word,
                    'line': line_num,
                    'context': context,
                    'file': str(file_path.relative_to(self.root_dir))
                })

        return words

    def get_context(self, line, word, window=40):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞"""
        # –ù–∞–π—Ç–∏ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞
        pos = line.lower().find(word.lower())
        if pos == -1:
            return line[:window]

        # –í–∑—è—Ç—å –æ–∫–Ω–æ –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞
        start = max(0, pos - window // 2)
        end = min(len(line), pos + len(word) + window // 2)

        context = line[start:end].strip()

        # –î–æ–±–∞–≤–∏—Ç—å –º–Ω–æ–≥–æ—Ç–æ—á–∏—è –µ—Å–ª–∏ –æ–±—Ä–µ–∑–∞–Ω–æ
        if start > 0:
            context = '...' + context
        if end < len(line):
            context = context + '...'

        return context

    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π"""
        print("üìñ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–∞...")
        print("   –í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ —Å—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –ë–∏–±–ª–∏–∏ + KWIC indexing\n")

        total_words = 0
        total_files = 0
        all_bigrams = []
        all_trigrams = []

        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            total_files += 1

            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å frontmatter
                content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)

                # –ò–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞
                words = self.extract_words(content, md_file)
                total_words += len(words)

                # –î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å
                for entry in words:
                    word = entry['word']
                    self.concordance[word].append({
                        'file': entry['file'],
                        'line': entry['line'],
                        'context': entry['context']
                    })

                # TF-IDF analysis
                doc_words = [entry['word'] for entry in words]
                self.tfidf_calc.add_document(str(md_file), doc_words)

                # Co-occurrence analysis
                self.cooccurrence.analyze_text(doc_words)

                # N-gram analysis
                bigrams = self.ngram_analyzer.extract_ngrams(content, n=2)
                trigrams = self.ngram_analyzer.extract_ngrams(content, n=3)
                all_bigrams.extend(bigrams)
                all_trigrams.extend(trigrams)

            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {md_file}: {e}")

        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤: {total_words}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.concordance)}")
        print(f"   –ë–∏–≥—Ä–∞–º–º: {len(all_bigrams)}")
        print(f"   –¢—Ä–∏–≥—Ä–∞–º–º: {len(all_trigrams)}")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å N-–≥—Ä–∞–º–º—ã
        self.bigrams = all_bigrams
        self.trigrams = all_trigrams

    def save(self, output_file):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å –≤ JSON"""
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        sorted_concordance = dict(sorted(self.concordance.items()))

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(sorted_concordance, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –ö–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_file}")

    def save_markdown(self, output_file):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å –≤ markdown —Ñ–æ—Ä–º–∞—Ç–µ"""
        lines = []
        lines.append("# Concordance - –ê–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å —Å–ª–æ–≤\n")
        lines.append(f"> –°–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π\n")
        lines.append(f"> –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.concordance)}\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
        current_letter = None

        for word in sorted(self.concordance.keys()):
            entries = self.concordance[word]

            # –ù–æ–≤–∞—è –±—É–∫–≤–∞ - –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
            first_letter = word[0].upper()
            if first_letter != current_letter:
                current_letter = first_letter
                lines.append(f"\n## {current_letter}\n\n")

            # –°–ª–æ–≤–æ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            lines.append(f"### {word} ({len(entries)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n\n")

            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 5 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            for entry in entries[:5]:
                lines.append(f"- **{entry['file']}:{entry['line']}**  \n")
                lines.append(f"  _{entry['context']}_\n\n")

            if len(entries) > 5:
                lines.append(f"  _...–∏ –µ—â—ë {len(entries) - 5} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π_\n\n")

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown –≤–µ—Ä—Å–∏—è: {output_file}")

    def generate_html_concordance(self, output_file: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ HTML –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–∞"""
        html_template = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Interactive Concordance</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; }}
        h1 {{ color: #333; }}
        .search {{ margin: 20px 0; }}
        .search input {{ width: 100%; padding: 12px; font-size: 16px; border: 2px solid #ddd; border-radius: 4px; }}
        .word-list {{ columns: 4; column-gap: 20px; }}
        .word-item {{ break-inside: avoid; margin-bottom: 8px; }}
        .word-link {{ color: #007bff; text-decoration: none; font-weight: 500; }}
        .word-link:hover {{ text-decoration: underline; }}
        .count {{ color: #666; font-size: 0.9em; }}
        .letter-header {{ background: #007bff; color: white; padding: 8px 12px; margin: 20px 0 10px 0; border-radius: 4px; }}
        .stats {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 20px 0; }}
        .stat {{ background: #f8f9fa; padding: 15px; border-radius: 4px; text-align: center; }}
        .stat-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üìñ Interactive Concordance</h1>

        <div class="stats">
            <div class="stat">
                <div class="stat-value">{total_words}</div>
                <div>Unique Words</div>
            </div>
            <div class="stat">
                <div class="stat-value">{total_occurrences}</div>
                <div>Total Occurrences</div>
            </div>
            <div class="stat">
                <div class="stat-value">{total_files}</div>
                <div>Files Indexed</div>
            </div>
        </div>

        <div class="search">
            <input type="text" id="searchBox" placeholder="Search for a word..." onkeyup="filterWords()">
        </div>

        <div id="wordList" class="word-list">
            {word_list_html}
        </div>
    </div>

    <script>
    function filterWords() {{
        const input = document.getElementById('searchBox');
        const filter = input.value.toLowerCase();
        const items = document.getElementsByClassName('word-item');

        for (let i = 0; i < items.length; i++) {{
            const text = items[i].textContent || items[i].innerText;
            if (text.toLowerCase().indexOf(filter) > -1) {{
                items[i].style.display = '';
            }} else {{
                items[i].style.display = 'none';
            }}
        }}
    }}
    </script>
</body>
</html>"""

        # –ü–æ–¥—Å—á—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_words = len(self.concordance)
        total_occurrences = sum(len(entries) for entries in self.concordance.values())

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
        all_files = set()
        for entries in self.concordance.values():
            for entry in entries:
                all_files.add(entry['file'])
        total_files = len(all_files)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤
        word_list_html = []
        current_letter = None

        for word in sorted(self.concordance.keys()):
            entries = self.concordance[word]

            # –ù–æ–≤–∞—è –±—É–∫–≤–∞
            first_letter = word[0].upper()
            if first_letter != current_letter:
                current_letter = first_letter
                word_list_html.append(f'<div class="letter-header">{current_letter}</div>')

            count = len(entries)
            word_list_html.append(
                f'<div class="word-item">'
                f'<a href="#" class="word-link">{word}</a> '
                f'<span class="count">({count})</span>'
                f'</div>'
            )

        html = html_template.format(
            total_words=total_words,
            total_occurrences=total_occurrences,
            total_files=total_files,
            word_list_html='\n'.join(word_list_html)
        )

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"‚úÖ HTML concordance: {output_file}")

    def search_word(self, word):
        """–ü–æ–∏—Å–∫ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ"""
        word_lower = word.lower()

        if word_lower not in self.concordance:
            print(f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫–æ–Ω–∫–æ—Ä–¥–∞–Ω—Å–µ")
            return []

        entries = self.concordance[word_lower]
        print(f"\nüìñ –°–ª–æ–≤–æ '{word}' –Ω–∞–π–¥–µ–Ω–æ –≤ {len(entries)} –º–µ—Å—Ç–∞—Ö:\n")

        for i, entry in enumerate(entries[:20], 1):
            print(f"{i}. {entry['file']}:{entry['line']}")
            print(f"   {entry['context']}\n")

        if len(entries) > 20:
            print(f"   ...–∏ –µ—â—ë {len(entries) - 20} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")

        return entries

    def get_top_words(self, n=50):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤"""
        word_counts = [(word, len(entries))
                      for word, entries in self.concordance.items()]

        word_counts.sort(key=lambda x: x[1], reverse=True)

        print(f"\nüìä –¢–æ–ø-{n} —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:\n")

        for i, (word, count) in enumerate(word_counts[:n], 1):
            print(f"{i:3d}. {word:20s} - {count:4d} —Ä–∞–∑")

        return word_counts[:n]

    def show_ngrams(self, n: int = 2, top: int = 20):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø N-–≥—Ä–∞–º–º"""
        ngrams = self.bigrams if n == 2 else self.trigrams
        top_ngrams = self.ngram_analyzer.get_top_ngrams(ngrams, top)

        ngram_name = "–±–∏–≥—Ä–∞–º–º—ã" if n == 2 else "—Ç—Ä–∏–≥—Ä–∞–º–º—ã"
        print(f"\nüìä –¢–æ–ø-{top} {ngram_name}:\n")

        for i, (ngram, count) in enumerate(top_ngrams, 1):
            ngram_str = ' '.join(ngram)
            print(f"{i:3d}. {ngram_str:40s} - {count:4d} —Ä–∞–∑")

    def show_tfidf_keywords(self, file_path: str, top_n: int = 10):
        """–ü–æ–∫–∞–∑–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ TF-IDF"""
        keywords = self.tfidf_calc.get_document_keywords(file_path, top_n)

        print(f"\nüìä –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (TF-IDF) –¥–ª—è {file_path}:\n")

        for i, (word, score) in enumerate(keywords, 1):
            print(f"{i:3d}. {word:20s} - {score:.4f}")

    def show_related_words(self, word: str, top_n: int = 10):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (co-occurrence)"""
        related = self.cooccurrence.get_related_words(word.lower(), top_n)

        if not related:
            print(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö –¥–ª—è '{word}'")
            return

        print(f"\nüìä –°–ª–æ–≤–∞, —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ä—è–¥–æ–º —Å '{word}':\n")

        for i, (related_word, count) in enumerate(related, 1):
            print(f"{i:3d}. {related_word:20s} - {count:4d} —Ä–∞–∑")


def main():
    parser = argparse.ArgumentParser(
        description='Advanced Concordance Builder with KWIC, N-grams, TF-IDF',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  build_concordance.py                      # Build full concordance
  build_concordance.py --search python      # Search for word
  build_concordance.py --top 50             # Top 50 frequent words
  build_concordance.py --bigrams            # Show top bigrams
  build_concordance.py --trigrams           # Show top trigrams
  build_concordance.py --related python     # Words related to 'python'
  build_concordance.py --html               # Generate HTML concordance
        """
    )

    parser.add_argument('--search', type=str, metavar='WORD',
                        help='Search for a word in concordance')
    parser.add_argument('--top', type=int, metavar='N', default=30,
                        help='Show top N frequent words (default: 30)')
    parser.add_argument('--bigrams', action='store_true',
                        help='Show top bigrams (2-word phrases)')
    parser.add_argument('--trigrams', action='store_true',
                        help='Show top trigrams (3-word phrases)')
    parser.add_argument('--related', type=str, metavar='WORD',
                        help='Show words related to the given word')
    parser.add_argument('--tfidf', type=str, metavar='FILE',
                        help='Show TF-IDF keywords for file')
    parser.add_argument('--html', action='store_true',
                        help='Generate HTML concordance')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    builder = ConcordanceBuilder(root_dir)
    builder.build()

    # Search mode
    if args.search:
        builder.search_word(args.search)
        return

    # Related words
    if args.related:
        builder.show_related_words(args.related)
        return

    # TF-IDF keywords
    if args.tfidf:
        builder.show_tfidf_keywords(args.tfidf)
        return

    # Bigrams
    if args.bigrams:
        builder.show_ngrams(n=2, top=20)
        return

    # Trigrams
    if args.trigrams:
        builder.show_ngrams(n=3, top=20)
        return

    # HTML concordance
    if args.html:
        output_dir = root_dir
        builder.generate_html_concordance(output_dir / "concordance.html")
        return

    # Default: full build
    output_dir = root_dir
    builder.save(output_dir / "concordance.json")
    builder.save_markdown(output_dir / "CONCORDANCE.md")
    builder.generate_html_concordance(output_dir / "concordance.html")

    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    builder.get_top_words(args.top)
    builder.show_ngrams(n=2, top=10)

    print("\nüí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
    print("   python tools/build_concordance.py --search <—Å–ª–æ–≤–æ>")
    print("   python tools/build_concordance.py --related <—Å–ª–æ–≤–æ>")
    print("   python tools/build_concordance.py --bigrams")


if __name__ == "__main__":
    main()
