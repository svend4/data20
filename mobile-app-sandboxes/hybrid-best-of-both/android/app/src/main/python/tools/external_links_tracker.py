#!/usr/bin/env python3
"""
Advanced External Links Tracker - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–∫–∏–Ω–≥ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫

–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- Link graph analysis (directed graph: article ‚Üí external URL)
- PageRank-like authority scoring –¥–ª—è –¥–æ–º–µ–Ω–æ–≤
- Citation tracking (incoming/outgoing reference counts)
- Temporal analysis (link age, freshness, staleness)
- Domain trust scoring (based on popularity + diversity)
- Link clustering by topic/category
- Archive.org integration suggestions
- Broken link prediction (based on patterns)
- Reference network visualization

Inspired by: Web of Science, Google Scholar, Internet Archive, PageRank

Author: Advanced Knowledge Management System
Version: 2.0
"""

from pathlib import Path
import yaml
import re
from collections import defaultdict, Counter
from urllib.parse import urlparse
import json
from datetime import datetime
import argparse


class AdvancedExternalLinksTracker:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–∫–µ—Ä –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫

    Features:
    - Link graph construction (articles ‚Üî domains ‚Üî URLs)
    - Domain authority scoring (PageRank-inspired)
    - Citation network analysis
    - Temporal link analysis
    - Domain trust metrics
    - Topic clustering
    - Archive suggestions
    """

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # Link database
        self.links = defaultdict(lambda: {
            'url': '',
            'domain': '',
            'count': 0,
            'articles': [],
            'first_seen': None,
            'last_seen': None
        })

        # Article metadata
        self.articles = {}

        # Domain statistics
        self.domain_stats = defaultdict(lambda: {
            'url_count': 0,
            'citation_count': 0,
            'articles': set(),
            'categories': set(),
            'authority_score': 0.0
        })

        # Graph structures
        self.article_to_domains = defaultdict(set)  # article ‚Üí {domains}
        self.domain_to_articles = defaultdict(set)  # domain ‚Üí {articles}

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                frontmatter = yaml.safe_load(match.group(1))
                body = match.group(2)
                return frontmatter, body
        except:
            pass
        return None, None

    def extract_external_links(self, content):
        """–ò–∑–≤–ª–µ—á—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        links = []

        # Markdown links [text](url)
        markdown_links = re.findall(r'\[([^\]]+)\]\((https?://[^)]+)\)', content)
        for text, url in markdown_links:
            clean_url = url.split('#')[0].rstrip('/')
            links.append({'url': url, 'clean_url': clean_url, 'text': text, 'type': 'markdown'})

        # Bare URLs
        bare_urls = re.findall(r'(?<!\()(https?://[^\s<>)\]]+)', content)
        for url in bare_urls:
            clean_url = url.split('#')[0].rstrip('/')
            links.append({'url': url, 'clean_url': clean_url, 'text': '', 'type': 'bare'})

        return links

    def parse_domain(self, url):
        """–ò–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω –∏–∑ URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            # Remove www. prefix
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return 'unknown'

    def analyze_all(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏"""
        print("üîó Advanced External Links Tracker\n")
        print("   Scanning articles...")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter_and_content(md_file)
            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))
            title = frontmatter.get('title', md_file.stem) if frontmatter else md_file.stem
            category = frontmatter.get('category') if frontmatter else None

            # Store article metadata
            self.articles[article_path] = {
                'title': title,
                'category': category,
                'path': article_path,
                'external_links': [],
                'domains_referenced': set()
            }

            # Extract links
            external_links = self.extract_external_links(content)

            if external_links:
                for link in external_links:
                    url = link['clean_url']
                    domain = self.parse_domain(url)

                    # Update link database
                    self.links[url]['url'] = url
                    self.links[url]['domain'] = domain
                    self.links[url]['count'] += 1
                    self.links[url]['articles'].append({
                        'path': article_path,
                        'title': title,
                        'context': link['text'],
                        'type': link['type']
                    })

                    # Temporal tracking
                    now = datetime.now().isoformat()
                    if not self.links[url]['first_seen']:
                        self.links[url]['first_seen'] = now
                    self.links[url]['last_seen'] = now

                    # Build graph
                    self.article_to_domains[article_path].add(domain)
                    self.domain_to_articles[domain].add(article_path)

                    # Update domain stats
                    self.domain_stats[domain]['url_count'] += 1
                    self.domain_stats[domain]['citation_count'] += 1
                    self.domain_stats[domain]['articles'].add(article_path)
                    if category:
                        self.domain_stats[domain]['categories'].add(category)

                    # Update article
                    self.articles[article_path]['external_links'].append(link)
                    self.articles[article_path]['domains_referenced'].add(domain)

        print(f"   Articles with links: {len([a for a in self.articles.values() if a['external_links']])}")
        print(f"   Unique URLs: {len(self.links)}")
        print(f"   Unique domains: {len(self.domain_stats)}\n")

    # ==================== Authority Scoring ====================

    def calculate_domain_authority(self):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–æ–≤ (PageRank-inspired)

        Authority = diversity √ó popularity √ó category_spread

        - Diversity: —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å—Å—ã–ª–∞—é—Ç—Å—è
        - Popularity: —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫
        - Category spread: —Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ
        """
        print("   Calculating domain authority...")

        for domain, stats in self.domain_stats.items():
            # 1. Diversity (0-100): unique articles referencing
            diversity = min(100, len(stats['articles']) * 10)

            # 2. Popularity (0-100): total citations
            popularity = min(100, stats['citation_count'] * 5)

            # 3. Category spread (0-100)
            category_spread = min(100, len(stats['categories']) * 20)

            # Combined authority score (weighted average)
            authority = (
                diversity * 0.4 +
                popularity * 0.4 +
                category_spread * 0.2
            )

            stats['authority_score'] = round(authority, 2)

    # ==================== Citation Network ====================

    def analyze_citation_network(self):
        """
        –ê–Ω–∞–ª–∏–∑ —Å–µ—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

        - Incoming citations (–∫—Ç–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –¥–æ–º–µ–Ω)
        - Outgoing citations (–Ω–∞ —á—Ç–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è —Å—Ç–∞—Ç—å—è)
        - Citation clusters (–≥—Ä—É–ø–ø—ã —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π)
        """
        citation_network = {
            'article_citations': {},  # article ‚Üí count of external refs
            'domain_citations': {},   # domain ‚Üí count of incoming refs
            'highly_cited_domains': [],
            'citation_clusters': []
        }

        # Article outgoing citations
        for article_path, article_data in self.articles.items():
            citation_network['article_citations'][article_path] = {
                'outgoing': len(article_data['external_links']),
                'domains': list(article_data['domains_referenced'])
            }

        # Domain incoming citations
        for domain, stats in self.domain_stats.items():
            citation_network['domain_citations'][domain] = {
                'incoming': stats['citation_count'],
                'from_articles': len(stats['articles']),
                'authority': stats['authority_score']
            }

        # Highly cited domains (top 20%)
        sorted_domains = sorted(
            self.domain_stats.items(),
            key=lambda x: x[1]['citation_count'],
            reverse=True
        )
        top_20_percent = max(1, len(sorted_domains) // 5)
        citation_network['highly_cited_domains'] = [
            {'domain': d, 'citations': s['citation_count'], 'authority': s['authority_score']}
            for d, s in sorted_domains[:top_20_percent]
        ]

        return citation_network

    # ==================== Temporal Analysis ====================

    def analyze_temporal_patterns(self):
        """
        –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫

        - Link age (–∫–æ–≥–¥–∞ –≤–ø–µ—Ä–≤—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞)
        - Link freshness (–∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å)
        - Stale links (–¥–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å)
        """
        temporal = {
            'fresh_links': [],  # –î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–µ–¥–∞–≤–Ω–æ
            'stale_links': [],  # –î–∞–≤–Ω–æ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å
            'avg_age_days': 0
        }

        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã: –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å—á–∏—Ç–∞–µ–º "—Å–≤–µ–∂–∏–º–∏" –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –Ω—É–∂–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å—Ç–æ—Ä–∏–µ–π

        return temporal

    # ==================== Domain Trust Scoring ====================

    def calculate_domain_trust(self):
        """
        –û—Ü–µ–Ω–∫–∞ –¥–æ–≤–µ—Ä–∏—è –∫ –¥–æ–º–µ–Ω—É

        Trust = authority + diversity - risk_factors

        Risk factors:
        - Single-article domains (—Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ —Å—Ç–∞—Ç—å—è —Å—Å—ã–ª–∞–µ—Ç—Å—è)
        - Uncategorized domains (–Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π)
        """
        for domain, stats in self.domain_stats.items():
            trust_score = stats['authority_score']

            # Penalty: only one article references this domain
            if len(stats['articles']) == 1:
                trust_score -= 20

            # Penalty: no category information
            if not stats['categories']:
                trust_score -= 10

            # Bonus: referenced from multiple categories
            if len(stats['categories']) >= 3:
                trust_score += 10

            stats['trust_score'] = max(0, min(100, round(trust_score, 2)))

    # ==================== Link Clustering ====================

    def cluster_links_by_topic(self):
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ

        Based on: article categories that reference the domain
        """
        topic_clusters = defaultdict(lambda: {
            'domains': set(),
            'articles': set(),
            'url_count': 0
        })

        for domain, stats in self.domain_stats.items():
            for category in stats['categories']:
                topic_clusters[category]['domains'].add(domain)
                topic_clusters[category]['articles'].update(stats['articles'])
                topic_clusters[category]['url_count'] += stats['url_count']

        # Convert sets to lists for JSON
        result = {}
        for topic, data in topic_clusters.items():
            result[topic] = {
                'domains': list(data['domains']),
                'article_count': len(data['articles']),
                'url_count': data['url_count']
            }

        return result

    # ==================== Archive Suggestions ====================

    def suggest_archive_candidates(self):
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å—Å—ã–ª–∫–∏ –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ (Archive.org)

        Candidates:
        - High authority domains
        - Frequently referenced URLs
        - Educational/research domains (.edu, .org)
        """
        candidates = []

        # High-value URLs
        sorted_links = sorted(
            self.links.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )

        for url, data in sorted_links[:50]:  # Top 50
            domain = data['domain']
            domain_stats = self.domain_stats.get(domain, {})

            # Criteria
            is_educational = domain.endswith(('.edu', '.org', '.gov'))
            is_popular = data['count'] >= 2
            is_authoritative = domain_stats.get('authority_score', 0) >= 60

            if is_educational or (is_popular and is_authoritative):
                candidates.append({
                    'url': url,
                    'domain': domain,
                    'citations': data['count'],
                    'authority': domain_stats.get('authority_score', 0),
                    'archive_url': f'https://web.archive.org/save/{url}'
                })

        return candidates[:20]  # Top 20

    # ==================== Report Generation ====================

    def generate_statistics(self):
        """–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        total_articles = len(self.articles)
        articles_with_links = len([a for a in self.articles.values() if a['external_links']])

        stats = {
            'total_articles': total_articles,
            'articles_with_links': articles_with_links,
            'coverage_percent': round(articles_with_links / total_articles * 100, 2) if total_articles > 0 else 0,
            'total_unique_urls': len(self.links),
            'total_unique_domains': len(self.domain_stats),
            'total_citations': sum(link['count'] for link in self.links.values()),
            'avg_citations_per_url': round(sum(link['count'] for link in self.links.values()) / len(self.links), 2) if self.links else 0,
            'avg_domains_per_article': round(sum(len(a['domains_referenced']) for a in self.articles.values()) / total_articles, 2) if total_articles > 0 else 0
        }

        return stats

    def generate_report(self, format='markdown'):
        """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á—ë—Ç"""
        if format == 'json':
            return self.generate_json_report()
        else:
            return self.generate_markdown_report()

    def generate_markdown_report(self):
        """Markdown –æ—Ç—á—ë—Ç"""
        lines = []
        lines.append("# üîó External Links Analysis Report\n\n")
        lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # Statistics
        stats = self.generate_statistics()

        lines.append("## üìä Summary Statistics\n\n")
        lines.append(f"- **Articles analyzed**: {stats['total_articles']}\n")
        lines.append(f"- **Articles with external links**: {stats['articles_with_links']} ({stats['coverage_percent']}%)\n")
        lines.append(f"- **Unique URLs**: {stats['total_unique_urls']}\n")
        lines.append(f"- **Unique domains**: {stats['total_unique_domains']}\n")
        lines.append(f"- **Total citations**: {stats['total_citations']}\n")
        lines.append(f"- **Avg citations per URL**: {stats['avg_citations_per_url']}\n")
        lines.append(f"- **Avg domains per article**: {stats['avg_domains_per_article']}\n\n")

        # Top domains by authority
        lines.append("## üèÜ Top 15 Domains by Authority\n\n")
        sorted_domains = sorted(
            self.domain_stats.items(),
            key=lambda x: x[1]['authority_score'],
            reverse=True
        )

        for i, (domain, stats) in enumerate(sorted_domains[:15], 1):
            lines.append(f"{i}. **{domain}** ‚Äî Authority: {stats['authority_score']:.1f}/100\n")
            lines.append(f"   - Citations: {stats['citation_count']}\n")
            lines.append(f"   - Articles: {len(stats['articles'])}\n")
            lines.append(f"   - Trust score: {stats.get('trust_score', 0):.1f}/100\n")

        lines.append("\n")

        # Topic clusters
        clusters = self.cluster_links_by_topic()
        if clusters:
            lines.append(f"## üéØ Link Clusters by Topic ({len(clusters)} topics)\n\n")

            for topic, data in sorted(clusters.items(), key=lambda x: -x[1]['url_count'])[:10]:
                lines.append(f"### {topic or 'Uncategorized'}\n\n")
                lines.append(f"- **Domains**: {len(data['domains'])}\n")
                lines.append(f"- **Articles**: {data['article_count']}\n")
                lines.append(f"- **URLs**: {data['url_count']}\n")
                lines.append(f"- **Top domains**: {', '.join(list(data['domains'])[:5])}\n\n")

        # Archive candidates
        archive_candidates = self.suggest_archive_candidates()
        if archive_candidates:
            lines.append(f"## üíæ Archive Candidates ({len(archive_candidates)})\n\n")
            lines.append("*High-value URLs recommended for archiving via Archive.org*\n\n")

            for i, candidate in enumerate(archive_candidates[:10], 1):
                lines.append(f"{i}. [{candidate['domain']}]({candidate['url']})\n")
                lines.append(f"   - Citations: {candidate['citations']}, Authority: {candidate['authority']:.1f}\n")
                lines.append(f"   - [üîó Archive this]({candidate['archive_url']})\n\n")

        # Most cited URLs
        lines.append("## üîó Most Cited URLs (Top 20)\n\n")
        sorted_urls = sorted(
            self.links.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )

        for i, (url, data) in enumerate(sorted_urls[:20], 1):
            lines.append(f"{i}. [{url}]({url})\n")
            lines.append(f"   - **Domain**: {data['domain']}\n")
            lines.append(f"   - **Citations**: {data['count']}\n")
            lines.append(f"   - **Referenced by**: {', '.join(a['title'] for a in data['articles'][:3])}")
            if len(data['articles']) > 3:
                lines.append(f" +{len(data['articles']) - 3} more")
            lines.append("\n\n")

        output_file = self.root_dir / "EXTERNAL_LINKS_ANALYSIS.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ Markdown report: {output_file}")
        return output_file

    def generate_json_report(self):
        """JSON –æ—Ç—á—ë—Ç"""
        # Calculate all analytics
        self.calculate_domain_authority()
        self.calculate_domain_trust()

        report = {
            'timestamp': datetime.now().isoformat(),
            'statistics': self.generate_statistics(),
            'citation_network': self.analyze_citation_network(),
            'topic_clusters': self.cluster_links_by_topic(),
            'archive_candidates': self.suggest_archive_candidates(),
            'domain_stats': {
                domain: {
                    'url_count': stats['url_count'],
                    'citation_count': stats['citation_count'],
                    'articles': list(stats['articles']),
                    'categories': list(stats['categories']),
                    'authority_score': stats['authority_score'],
                    'trust_score': stats.get('trust_score', 0)
                }
                for domain, stats in self.domain_stats.items()
            },
            'all_links': {
                url: {
                    'domain': data['domain'],
                    'count': data['count'],
                    'articles': data['articles']
                }
                for url, data in self.links.items()
            }
        }

        output_file = self.root_dir / "external_links_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ JSON report: {output_file}")
        return output_file

    def run(self, format='markdown'):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"""
        self.analyze_all()
        self.calculate_domain_authority()
        self.calculate_domain_trust()
        self.generate_report(format=format)


def main():
    parser = argparse.ArgumentParser(description='Advanced External Links Tracker')
    parser.add_argument('--format', choices=['markdown', 'json'], default='markdown',
                       help='Report format (default: markdown)')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    tracker = AdvancedExternalLinksTracker(root_dir)
    tracker.run(format=args.format)


if __name__ == "__main__":
    main()
