#!/usr/bin/env python3
"""
Table of Contents Generator - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—ë—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è markdown —Ñ–∞–π–ª–æ–≤

–§—É–Ω–∫—Ü–∏–∏:
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ markdown
- –°–æ–∑–¥–∞–Ω–∏–µ —è–∫–æ—Ä–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
- –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
- Auto-numbering (1.1, 1.2, etc.)
- Multi-format export (Markdown, HTML, JSON)
- TOC validation (–ø—Ä–æ–≤–µ—Ä–∫–∞ —è–∫–æ—Ä–µ–π)
- Cross-reference detection
- Interactive HTML TOC
"""

from pathlib import Path
import re
import json
import argparse
from typing import List, Tuple, Dict, Optional
from datetime import datetime
from collections import defaultdict


class AutoNumbering:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω—É–º–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""

    def __init__(self, style='decimal'):
        """
        style: 'decimal' (1.1.1), 'roman' (I.A.1), 'legal' (1.1.1.1)
        """
        self.style = style
        self.counters = defaultdict(int)

    def generate_number(self, level: int) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–º–µ—Ä –¥–ª—è —É—Ä–æ–≤–Ω—è"""
        # Reset deeper levels when going back up
        for l in range(level + 1, 7):
            self.counters[l] = 0

        self.counters[level] += 1

        if self.style == 'decimal':
            # 1.1.1
            numbers = [str(self.counters[l]) for l in range(1, level + 1)]
            return '.'.join(numbers)
        elif self.style == 'legal':
            # 1.1.1.1
            numbers = [str(self.counters[l]) for l in range(1, level + 1)]
            return '.'.join(numbers)
        elif self.style == 'roman':
            # I.A.1
            roman_map = [
                lambda n: self._to_roman(n).upper(),  # I, II, III
                lambda n: chr(64 + n),                # A, B, C
                lambda n: str(n),                     # 1, 2, 3
                lambda n: chr(96 + n),                # a, b, c
            ]
            if level <= len(roman_map):
                return roman_map[level - 1](self.counters[level])
            return str(self.counters[level])

        return ''

    @staticmethod
    def _to_roman(num: int) -> str:
        """Convert to Roman numerals"""
        val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]
        syms = ['M', 'CM', 'D', 'CD', 'C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I']
        roman = ''
        i = 0
        while num > 0:
            for _ in range(num // val[i]):
                roman += syms[i]
                num -= val[i]
            i += 1
        return roman


class TOCValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è"""

    def __init__(self, content: str, headings: List[Tuple[int, str, str]]):
        self.content = content
        self.headings = headings

    def validate(self) -> Dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è

        Returns: {
            'valid': bool,
            'issues': [{'type': str, 'message': str, 'anchor': str}, ...],
            'stats': {...}
        }
        """
        issues = []

        # Check for duplicate anchors
        anchors = [h[2] for h in self.headings]
        anchor_counts = defaultdict(int)
        for anchor in anchors:
            anchor_counts[anchor] += 1

        for anchor, count in anchor_counts.items():
            if count > 1:
                issues.append({
                    'type': 'duplicate_anchor',
                    'message': f'–Ø–∫–æ—Ä—å "{anchor}" –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {count} —Ä–∞–∑',
                    'anchor': anchor,
                    'severity': 'high'
                })

        # Check for empty headings
        for level, text, anchor in self.headings:
            if not text.strip():
                issues.append({
                    'type': 'empty_heading',
                    'message': f'–ü—É—Å—Ç–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É—Ä–æ–≤–Ω—è {level}',
                    'anchor': anchor,
                    'severity': 'medium'
                })

        # Check heading hierarchy (no skipped levels)
        prev_level = 0
        for level, text, anchor in self.headings:
            if level > prev_level + 1 and prev_level > 0:
                issues.append({
                    'type': 'skipped_level',
                    'message': f'–ü—Ä–æ–ø—É—â–µ–Ω —É—Ä–æ–≤–µ–Ω—å: {prev_level} ‚Üí {level} –≤ "{text}"',
                    'anchor': anchor,
                    'severity': 'low'
                })
            prev_level = level

        # Stats
        stats = {
            'total_headings': len(self.headings),
            'levels': defaultdict(int),
            'max_depth': max((h[0] for h in self.headings), default=0),
            'unique_anchors': len(set(anchors))
        }

        for level, _, _ in self.headings:
            stats['levels'][f'h{level}'] = stats['levels'].get(f'h{level}', 0) + 1

        return {
            'valid': len([i for i in issues if i['severity'] in ['high', 'critical']]) == 0,
            'issues': issues,
            'stats': dict(stats['levels']) | {
                'total': stats['total_headings'],
                'max_depth': stats['max_depth'],
                'unique_anchors': stats['unique_anchors']
            }
        }


class CrossReferenceDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""

    def __init__(self, content: str, file_path: Path, root_dir: Path):
        self.content = content
        self.file_path = file_path
        self.root_dir = root_dir

    def find_internal_links(self) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ [text](url)"""
        # Pattern: [text](url) or [text](url#anchor)
        pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        matches = re.findall(pattern, self.content)

        links = []
        for text, url in matches:
            # Skip external URLs
            if url.startswith('http'):
                continue

            # Parse URL and anchor
            if '#' in url:
                path, anchor = url.split('#', 1)
            else:
                path = url
                anchor = ''

            links.append({
                'text': text,
                'url': url,
                'path': path,
                'anchor': anchor,
                'type': 'internal' if path else 'anchor_only'
            })

        return links

    def validate_links(self, links: List[Dict]) -> List[Dict]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç"""
        broken = []

        for link in links:
            if link['type'] == 'anchor_only':
                # Anchor within same file - would need TOC to validate
                continue

            # Check if file exists
            if link['path']:
                # Resolve relative to current file
                target = (self.file_path.parent / link['path']).resolve()

                if not target.exists():
                    broken.append({
                        'link': link,
                        'reason': f'–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {link["path"]}',
                        'severity': 'high'
                    })
                elif target.is_dir():
                    broken.append({
                        'link': link,
                        'reason': f'–°—Å—ã–ª–∫–∞ –Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {link["path"]}',
                        'severity': 'medium'
                    })

        return broken


class TOCGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è"""

    def __init__(self, root_dir=".", numbered=False, numbering_style='decimal'):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"
        self.numbered = numbered
        self.numbering_style = numbering_style

    def extract_frontmatter_and_content(self, file_path):
        """–ò–∑–≤–ª–µ—á—å frontmatter –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
            if match:
                return match.group(1), match.group(2)
        except:
            pass
        return None, None

    def extract_headings(self, content):
        """
        –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ markdown

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫: [(level, text, anchor), ...]
        """
        headings = []

        for line in content.split('\n'):
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if match:
                hashes = match.group(1)
                text = match.group(2).strip()

                level = len(hashes)

                # –°–æ–∑–¥–∞—Ç—å —è–∫–æ—Ä—å (–∫–∞–∫ GitHub)
                anchor = text.lower()
                # –£–¥–∞–ª–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                anchor = re.sub(r'[^\w\s-]', '', anchor)
                # –ó–∞–º–µ–Ω–∏—Ç—å –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –¥–µ—Ñ–∏—Å—ã
                anchor = re.sub(r'\s+', '-', anchor)
                # –£–¥–∞–ª–∏—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã
                anchor = re.sub(r'-+', '-', anchor)
                # –£–¥–∞–ª–∏—Ç—å –¥–µ—Ñ–∏—Å—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
                anchor = anchor.strip('-')

                headings.append((level, text, anchor))

        return headings

    def generate_toc(self, headings, min_level=2, max_level=4, numbered=None):
        """
        –°–æ–∑–¥–∞—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

        min_level: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–æ–±—ã—á–Ω–æ 2, —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å h1)
        max_level: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≥–ª—É–±–∏–Ω—ã)
        numbered: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –Ω—É–º–µ—Ä–∞—Ü–∏—é (None = use self.numbered)
        """
        if not headings:
            return ""

        if numbered is None:
            numbered = self.numbered

        lines = []
        lines.append("## üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n")

        # Auto numbering
        numbering = AutoNumbering(self.numbering_style) if numbered else None

        for level, text, anchor in headings:
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–µ –∏–ª–∏ –Ω–∏–∑–∫–∏–µ —É—Ä–æ–≤–Ω–∏
            if level < min_level or level > max_level:
                continue

            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–∞–º–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
            if '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ' in text.lower() or 'table of contents' in text.lower():
                continue

            # –û—Ç—Å—Ç—É–ø –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —É—Ä–æ–≤–Ω—é
            indent = "  " * (level - min_level)

            # –ù—É–º–µ—Ä–∞—Ü–∏—è
            if numbering:
                number = numbering.generate_number(level)
                lines.append(f"{indent}- {number} [{text}](#{anchor})\n")
            else:
                lines.append(f"{indent}- [{text}](#{anchor})\n")

        lines.append("\n")

        return ''.join(lines)

    def generate_toc_html(self, headings, min_level=2, max_level=4) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ HTML –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ"""
        if not headings:
            return ""

        html_lines = []
        html_lines.append('<nav class="toc">\n')
        html_lines.append('  <h2>üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ</h2>\n')
        html_lines.append('  <ul class="toc-list">\n')

        numbering = AutoNumbering(self.numbering_style) if self.numbered else None
        current_level = min_level

        for level, text, anchor in headings:
            if level < min_level or level > max_level:
                continue
            if '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ' in text.lower():
                continue

            # Handle nesting
            while current_level < level:
                html_lines.append('    ' * current_level + '  <ul>\n')
                current_level += 1

            while current_level > level:
                html_lines.append('    ' * current_level + '  </ul>\n')
                current_level -= 1

            number = numbering.generate_number(level) + ' ' if numbering else ''
            indent = '    ' * (level - min_level + 1)
            html_lines.append(f'{indent}<li><a href="#{anchor}">{number}{text}</a></li>\n')

        # Close remaining lists
        while current_level >= min_level:
            html_lines.append('    ' * current_level + '  </ul>\n')
            current_level -= 1

        html_lines.append('  </ul>\n')
        html_lines.append('</nav>\n')

        return ''.join(html_lines)

    def generate_toc_json(self, headings, min_level=2, max_level=4) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è –≤ JSON"""
        toc_data = []

        numbering = AutoNumbering(self.numbering_style) if self.numbered else None

        for level, text, anchor in headings:
            if level < min_level or level > max_level:
                continue
            if '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ' in text.lower():
                continue

            item = {
                'level': level,
                'text': text,
                'anchor': anchor
            }

            if numbering:
                item['number'] = numbering.generate_number(level)

            toc_data.append(item)

        return json.dumps(toc_data, ensure_ascii=False, indent=2)

    def generate_toc_plaintext(self, headings, min_level=2, max_level=4) -> str:
        """–°–æ–∑–¥–∞—Ç—å plaintext –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ (–¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–∞)"""
        if not headings:
            return ""

        lines = []
        lines.append("üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n")
        lines.append("=" * 50 + "\n\n")

        numbering = AutoNumbering(self.numbering_style) if self.numbered else None

        for level, text, anchor in headings:
            if level < min_level or level > max_level:
                continue
            if '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ' in text.lower():
                continue

            indent = "  " * (level - min_level)

            if numbering:
                number = numbering.generate_number(level)
                lines.append(f"{indent}{number}. {text}\n")
            else:
                lines.append(f"{indent}‚Ä¢ {text}\n")

        return ''.join(lines)

    def calculate_toc_stats(self, headings) -> Dict:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è"""
        if not headings:
            return {}

        levels = defaultdict(int)
        total_chars = 0

        for level, text, anchor in headings:
            levels[f'h{level}'] += 1
            total_chars += len(text)

        return {
            'total_headings': len(headings),
            'levels': dict(levels),
            'max_depth': max(h[0] for h in headings),
            'min_depth': min(h[0] for h in headings),
            'avg_heading_length': round(total_chars / len(headings), 1) if headings else 0,
            'total_chars': total_chars
        }

    def add_toc_to_file(self, file_path):
        """–î–æ–±–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª"""
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        if not content:
            return False

        # –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headings = self.extract_headings(content)

        if not headings:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
        if '## üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ' in content or '## Table of Contents' in content:
            # –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
            content = re.sub(
                r'## üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n.*?\n\n',
                '',
                content,
                flags=re.DOTALL
            )
            content = re.sub(
                r'## Table of Contents\n\n.*?\n\n',
                '',
                content,
                flags=re.DOTALL
            )

        # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
        toc = self.generate_toc(headings)

        # –í—Å—Ç–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–æ–±—ã—á–Ω–æ h1)
        lines = content.split('\n')
        insert_index = 0

        for i, line in enumerate(lines):
            if re.match(r'^#\s+', line):
                # –ù–∞–π—Ç–∏ –∫–æ–Ω–µ—Ü –ø–µ—Ä–≤–æ–≥–æ –±–ª–æ–∫–∞ (–ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞)
                for j in range(i + 1, len(lines)):
                    if lines[j].strip() == '':
                        insert_index = j + 1
                        break
                break

        # –í—Å—Ç–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
        lines.insert(insert_index, toc.rstrip('\n'))

        # –°–æ–±—Ä–∞—Ç—å —Ñ–∞–π–ª –æ–±—Ä–∞—Ç–Ω–æ
        new_content = "---\n" + frontmatter + "\n---\n\n" + '\n'.join(lines)

        # –ó–∞–ø–∏—Å–∞—Ç—å
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)

        return True

    def process_all_articles(self):
        """–î–æ–±–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∫–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º"""
        print("üìë –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π...\n")

        count = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            try:
                if self.add_toc_to_file(md_file):
                    count += 1
                    print(f"‚úÖ {md_file.relative_to(self.root_dir)}")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ {md_file}: {e}")

        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {count}")

    def validate_file_toc(self, file_path) -> Dict:
        """–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        frontmatter, content = self.extract_frontmatter_and_content(file_path)

        if not content:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª'}

        headings = self.extract_headings(content)

        if not headings:
            return {'error': '–ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}

        # Validate TOC
        validator = TOCValidator(content, headings)
        validation = validator.validate()

        # Find and validate cross-references
        detector = CrossReferenceDetector(content, file_path, self.root_dir)
        links = detector.find_internal_links()
        broken_links = detector.validate_links(links)

        # Calculate stats
        stats = self.calculate_toc_stats(headings)

        return {
            'file': str(file_path.relative_to(self.root_dir)),
            'validation': validation,
            'cross_references': {
                'total_links': len(links),
                'broken_links': broken_links
            },
            'stats': stats
        }

    def validate_all_tocs(self):
        """–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        print("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π...\n")

        results = []
        issues_count = 0
        broken_links_count = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            result = self.validate_file_toc(md_file)

            if 'error' not in result:
                validation = result['validation']
                issues = [i for i in validation['issues'] if i['severity'] in ['high', 'critical']]
                broken = result['cross_references']['broken_links']

                if issues or broken:
                    print(f"\n‚ö†Ô∏è  {result['file']}")

                    if issues:
                        print(f"   Issues: {len(issues)}")
                        for issue in issues:
                            print(f"     - {issue['message']}")
                        issues_count += len(issues)

                    if broken:
                        print(f"   Broken links: {len(broken)}")
                        for link_issue in broken[:3]:  # Show first 3
                            print(f"     - {link_issue['reason']}")
                        broken_links_count += len(broken)

                results.append(result)

        print(f"\n‚úÖ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(results)}")
        print(f"   –í—Å–µ–≥–æ issues: {issues_count}")
        print(f"   Broken links: {broken_links_count}")

        # Save detailed report
        output_file = self.root_dir / "toc_validation_report.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'generated': datetime.now().isoformat(),
                'summary': {
                    'files_checked': len(results),
                    'total_issues': issues_count,
                    'broken_links': broken_links_count
                },
                'results': results
            }, f, ensure_ascii=False, indent=2)

        print(f"üìÑ Detailed report: {output_file}")

    def generate_master_toc(self):
        """–°–æ–∑–¥–∞—Ç—å –≥–ª–∞–≤–Ω–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("\nüìö –°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–≥–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è...\n")

        lines = []
        lines.append("# üìö –ì–ª–∞–≤–Ω–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π\n\n")
        lines.append("> –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å –∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º\n\n")
        lines.append(f"_–°–æ–∑–¥–∞–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\n\n")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        by_category = {}
        total_headings = 0

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –ø—É—Ç–∏
            parts = md_file.relative_to(self.knowledge_dir).parts
            if len(parts) > 0:
                category = parts[0]

                if category not in by_category:
                    by_category[category] = []

                frontmatter, content = self.extract_frontmatter_and_content(md_file)

                if content:
                    headings = self.extract_headings(content)
                    total_headings += len(headings)

                    by_category[category].append({
                        'file': str(md_file.relative_to(self.root_dir)),
                        'headings': headings
                    })

        # Summary stats
        lines.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
        lines.append(f"- **–ö–∞—Ç–µ–≥–æ—Ä–∏–π**: {len(by_category)}\n")
        total_articles = sum(len(articles) for articles in by_category.values())
        lines.append(f"- **–°—Ç–∞—Ç–µ–π**: {total_articles}\n")
        lines.append(f"- **–ó–∞–≥–æ–ª–æ–≤–∫–æ–≤**: {total_headings}\n\n")

        # –í—ã–≤–µ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category in sorted(by_category.keys()):
            lines.append(f"## {category.title()}\n\n")

            for article in sorted(by_category[category], key=lambda x: x['file']):
                # –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ (–ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞)
                title = Path(article['file']).stem
                if article['headings']:
                    title = article['headings'][0][1]

                lines.append(f"### [{title}]({article['file']})\n\n")

                # –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
                if len(article['headings']) > 1:
                    for level, text, anchor in article['headings'][1:]:
                        if level <= 3:  # –¢–æ–ª—å–∫–æ h2 –∏ h3
                            indent = "  " * (level - 2)
                            lines.append(f"{indent}- {text}\n")
                    lines.append("\n")

        output_file = self.root_dir / "MASTER_TOC.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –ì–ª–∞–≤–Ω–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ: {output_file}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(by_category)}")
        print(f"   –°—Ç–∞—Ç–µ–π: {total_articles}")
        print(f"   –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {total_headings}")


def main():
    parser = argparse.ArgumentParser(
        description='Table of Contents Generator - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                                    # –°–æ–∑–¥–∞—Ç—å TOC –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
  %(prog)s --file article.md                  # TOC –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
  %(prog)s --master                           # –ì–ª–∞–≤–Ω–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã
  %(prog)s --validate                         # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö TOC
  %(prog)s --numbered --style decimal         # –° –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π 1.1.1
  %(prog)s --export html --file article.md    # –≠–∫—Å–ø–æ—Ä—Ç –≤ HTML
  %(prog)s --stats                            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ TOC
        """
    )

    parser.add_argument(
        '-f', '--file',
        help='–î–æ–±–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–∞–π–ª—É'
    )

    parser.add_argument(
        '-a', '--all',
        action='store_true',
        help='–î–æ–±–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∫–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º'
    )

    parser.add_argument(
        '-m', '--master',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å –≥–ª–∞–≤–Ω–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–π –±–∞–∑—ã'
    )

    parser.add_argument(
        '--validate',
        action='store_true',
        help='–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è (–ø—Ä–æ–≤–µ—Ä–∫–∞ —è–∫–æ—Ä–µ–π, broken links)'
    )

    parser.add_argument(
        '--numbered',
        action='store_true',
        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω—É–º–µ—Ä–∞—Ü–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤'
    )

    parser.add_argument(
        '--style',
        choices=['decimal', 'roman', 'legal'],
        default='decimal',
        help='–°—Ç–∏–ª—å –Ω—É–º–µ—Ä–∞—Ü–∏–∏: decimal (1.1.1), roman (I.A.1), legal (1.1.1.1)'
    )

    parser.add_argument(
        '--export',
        choices=['markdown', 'html', 'json', 'plaintext'],
        help='–≠–∫—Å–ø–æ—Ä—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ'
    )

    parser.add_argument(
        '--stats',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é'
    )

    parser.add_argument(
        '--min-level',
        type=int,
        default=2,
        help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)'
    )

    parser.add_argument(
        '--max-level',
        type=int,
        default=4,
        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4)'
    )

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    generator = TOCGenerator(root_dir, numbered=args.numbered, numbering_style=args.style)

    # Validation mode
    if args.validate:
        generator.validate_all_tocs()
        return

    # Export mode
    if args.export and args.file:
        file_path = root_dir / args.file
        frontmatter, content = generator.extract_frontmatter_and_content(file_path)

        if not content:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {args.file}")
            return

        headings = generator.extract_headings(content)

        if not headings:
            print(f"‚ùå –ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {args.file}")
            return

        print(f"\nüì§ –≠–∫—Å–ø–æ—Ä—Ç TOC –≤ —Ñ–æ—Ä–º–∞—Ç: {args.export}\n")

        if args.export == 'markdown':
            toc = generator.generate_toc(headings, args.min_level, args.max_level)
            print(toc)
        elif args.export == 'html':
            toc = generator.generate_toc_html(headings, args.min_level, args.max_level)
            print(toc)
        elif args.export == 'json':
            toc = generator.generate_toc_json(headings, args.min_level, args.max_level)
            print(toc)
        elif args.export == 'plaintext':
            toc = generator.generate_toc_plaintext(headings, args.min_level, args.max_level)
            print(toc)

        return

    # Stats mode
    if args.stats:
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π –ø–æ –≤—Å–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π\n")

        total_files = 0
        total_headings_count = 0
        levels_overall = defaultdict(int)

        for md_file in generator.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = generator.extract_frontmatter_and_content(md_file)

            if content:
                headings = generator.extract_headings(content)
                stats = generator.calculate_toc_stats(headings)

                if stats:
                    total_files += 1
                    total_headings_count += stats['total_headings']

                    for level, count in stats['levels'].items():
                        levels_overall[level] += count

        print(f"–§–∞–π–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {total_files}")
        print(f"–í—Å–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {total_headings_count}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ —Ñ–∞–π–ª: {total_headings_count / total_files:.1f}" if total_files > 0 else "N/A")
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º:")

        for level in sorted(levels_overall.keys()):
            count = levels_overall[level]
            percent = (count / total_headings_count * 100) if total_headings_count > 0 else 0
            print(f"  {level}: {count} ({percent:.1f}%)")

        return

    # File mode
    if args.file:
        file_path = root_dir / args.file
        if generator.add_toc_to_file(file_path):
            print(f"‚úÖ –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –∫ {args.file}")

            # Show validation
            result = generator.validate_file_toc(file_path)

            if 'error' not in result:
                print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                for key, value in result['stats'].items():
                    print(f"   {key}: {value}")

                if result['validation']['issues']:
                    print(f"\n‚ö†Ô∏è  Issues: {len(result['validation']['issues'])}")
                    for issue in result['validation']['issues'][:3]:
                        print(f"     - {issue['message']}")
        else:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ")

    # All mode
    elif args.all:
        generator.process_all_articles()

    # Master mode
    elif args.master:
        generator.generate_master_toc()

    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –æ–±–∞ –¥–µ–π—Å—Ç–≤–∏—è
        generator.process_all_articles()
        generator.generate_master_toc()


if __name__ == "__main__":
    main()
