#!/usr/bin/env python3
"""
Advanced Related Articles Finder - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
–§—É–Ω–∫—Ü–∏–∏:
- TF-IDF similarity (—Ç–µ–∫—Å—Ç–æ–≤–∞—è –ø–æ—Ö–æ–∂–µ—Å—Ç—å)
- Cosine similarity (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- Jaccard similarity (–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ)
- Content-based filtering (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ)
- Hybrid recommendation (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤)
- Similarity graph (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–≤—è–∑–µ–π)
- Auto-linking suggestions (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫)
- Multiple similarity metrics (–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
- Related articles network (–≥—Ä–∞—Ñ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏)
- Export formats (JSON, Markdown, Graph)
- Caching for performance (–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ)
- Weighted scoring (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ)

–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–æ: Netflix recommendations, Medium related articles, Wikipedia "See also",
              collaborative filtering, content-based recommender systems
"""

import os
import re
from pathlib import Path
import yaml
import sys
import json
import math
from collections import defaultdict, Counter


class AdvancedRelatedFinder:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""

    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.knowledge_dir = self.root_dir / "knowledge"

        # –ö—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.documents = {}  # path -> {frontmatter, content, tokens}
        self.document_frequency = Counter()  # –î–ª—è IDF
        self.total_documents = 0

        # –ö—ç—à –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        self.tfidf_cache = {}
        self.similarity_cache = {}

    def extract_frontmatter(self, file_path):
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)
        if not match:
            return None, content

        try:
            frontmatter = yaml.safe_load(match.group(1))
            body = match.group(2)
            return frontmatter, body
        except:
            return None, content

    def tokenize(self, text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–∏—Ç—å markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        text = re.sub(r'!?\[([^\]]*)\]\([^)]+\)', r'\1', text)  # –°—Å—ã–ª–∫–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        text = re.sub(r'[#*`_]', '', text)  # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # –ö–æ–¥-–±–ª–æ–∫–∏

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å (—Å–ª–æ–≤–∞ 3+ —Å–∏–º–≤–æ–ª–æ–≤)
        tokens = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        stop_words = {
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one',
            'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now',
            '—ç—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '—á—Ç–æ', '–ø—Ä–∏', '–∏–ª–∏', '–µ–≥–æ', '–µ—â–µ', '—Ç–∞–∫', '—É–∂–µ', '–≥–¥–µ', '—Ç–∞–º',
            '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '—á–µ–º', '–≤—Å–µ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '–±–æ–ª–µ–µ'
        }

        return [t for t in tokens if t not in stop_words]

    def load_documents(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–∞–º—è—Ç—å"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        for md_file in self.knowledge_dir.rglob("*.md"):
            if md_file.name == "INDEX.md":
                continue

            frontmatter, content = self.extract_frontmatter(md_file)

            if not content:
                continue

            article_path = str(md_file.relative_to(self.root_dir))

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            tokens = self.tokenize(content)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
            self.documents[article_path] = {
                'frontmatter': frontmatter or {},
                'content': content,
                'tokens': tokens,
                'token_set': set(tokens)
            }

            # –û–±–Ω–æ–≤–∏—Ç—å document frequency
            for token in set(tokens):
                self.document_frequency[token] += 1

        self.total_documents = len(self.documents)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.total_documents}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(self.document_frequency)}\n")

    def calculate_tfidf(self, tokens):
        """–í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –≤–µ–∫—Ç–æ—Ä –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not tokens:
            return {}

        # Term Frequency
        tf = Counter(tokens)
        total_terms = len(tokens)

        # TF-IDF
        tfidf = {}
        for term, count in tf.items():
            term_freq = count / total_terms
            doc_freq = self.document_frequency.get(term, 1)
            idf = math.log(self.total_documents / doc_freq) if doc_freq > 0 else 0
            tfidf[term] = term_freq * idf

        return tfidf

    def cosine_similarity(self, vec1, vec2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        # –ù–∞–π—Ç–∏ –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        common_terms = set(vec1.keys()) & set(vec2.keys())

        if not common_terms:
            return 0.0

        # –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        dot_product = sum(vec1[term] * vec2[term] for term in common_terms)

        # –ù–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–æ–≤
        norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
        norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def jaccard_similarity(self, set1, set2):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –ñ–∞–∫–∫–∞—Ä–∞"""
        if not set1 or not set2:
            return 0.0

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0

    def tag_similarity(self, tags1, tags2):
        """–°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —Ç–µ–≥–∞–º"""
        if not tags1 or not tags2:
            return 0.0

        set1 = set(tags1)
        set2 = set(tags2)

        return self.jaccard_similarity(set1, set2)

    def category_similarity(self, cat1, cat2, subcat1=None, subcat2=None):
        """–°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        score = 0.0

        if cat1 == cat2:
            score += 1.0

        if subcat1 and subcat2 and subcat1 == subcat2:
            score += 0.5

        return score

    def find_related(self, target_file, num_results=10, algorithm='hybrid'):
        """–ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        target_path = str(Path(target_file).relative_to(self.root_dir)) if not target_file.startswith('knowledge') else target_file

        if target_path not in self.documents:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {target_file}")
            return []

        target_doc = self.documents[target_path]
        target_fm = target_doc['frontmatter']

        print(f"\nüìÑ –¶–µ–ª–µ–≤–∞—è —Å—Ç–∞—Ç—å—è: {target_fm.get('title', target_file)}")
        print(f"üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(target_fm.get('tags', []))}")
        print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {target_fm.get('category')} / {target_fm.get('subcategory')}")
        print(f"üìä –ê–ª–≥–æ—Ä–∏—Ç–º: {algorithm}\n")

        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if target_path not in self.tfidf_cache:
            self.tfidf_cache[target_path] = self.calculate_tfidf(target_doc['tokens'])

        target_tfidf = self.tfidf_cache[target_path]
        target_tags = set(target_fm.get('tags', []))
        target_category = target_fm.get('category')
        target_subcategory = target_fm.get('subcategory')

        # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        candidates = []

        for article_path, doc in self.documents.items():
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–∞–º—É —Ü–µ–ª–µ–≤—É—é —Å—Ç–∞—Ç—å—é
            if article_path == target_path:
                continue

            fm = doc['frontmatter']

            # –ö—ç—à –∫–ª—é—á
            cache_key = f"{target_path}:{article_path}:{algorithm}"

            if cache_key in self.similarity_cache:
                similarity = self.similarity_cache[cache_key]
            else:
                # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self.calculate_similarity(
                    target_doc, doc, target_tfidf,
                    algorithm=algorithm
                )
                self.similarity_cache[cache_key] = similarity

            if similarity > 0:
                candidates.append({
                    'file': article_path,
                    'title': fm.get('title', article_path),
                    'similarity': similarity,
                    'tags': fm.get('tags', []),
                    'category': fm.get('category')
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        candidates.sort(key=lambda x: -x['similarity'])

        # –í—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(candidates)} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π\n")
        print("–¢–æ–ø —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:")
        print("=" * 80)

        for i, article in enumerate(candidates[:num_results], 1):
            print(f"\n{i}. {article['title']}")
            print(f"   –§–∞–π–ª: {article['file']}")
            print(f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {article['similarity']:.4f}")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {article['category']}")
            print(f"   –¢–µ–≥–∏: {', '.join(article['tags'][:5])}")

        # –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏
        print("\n" + "=" * 80)
        print("\nüí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: –¥–æ–±–∞–≤–∏—Ç—å –≤ —Å–µ–∫—Ü–∏—é '–°–º. —Ç–∞–∫–∂–µ':\n")

        for article in candidates[:num_results]:
            filename = Path(article['file']).name
            print(f"- [[{filename}]] - {article['title']}")

        return candidates[:num_results]

    def calculate_similarity(self, doc1, doc2, tfidf1, algorithm='hybrid'):
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
        if algorithm == 'tfidf':
            return self.calculate_tfidf_similarity(doc1, doc2, tfidf1)
        elif algorithm == 'jaccard':
            return self.calculate_jaccard_similarity(doc1, doc2)
        elif algorithm == 'tags':
            return self.calculate_tags_similarity(doc1, doc2)
        elif algorithm == 'hybrid':
            return self.calculate_hybrid_similarity(doc1, doc2, tfidf1)
        else:
            return 0.0

    def calculate_tfidf_similarity(self, doc1, doc2, tfidf1):
        """TF-IDF –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        # –í—ã—á–∏—Å–ª–∏—Ç—å TF-IDF –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
        doc2_path = None
        for path, d in self.documents.items():
            if d == doc2:
                doc2_path = path
                break

        if doc2_path and doc2_path in self.tfidf_cache:
            tfidf2 = self.tfidf_cache[doc2_path]
        else:
            tfidf2 = self.calculate_tfidf(doc2['tokens'])
            if doc2_path:
                self.tfidf_cache[doc2_path] = tfidf2

        return self.cosine_similarity(tfidf1, tfidf2)

    def calculate_jaccard_similarity(self, doc1, doc2):
        """–ñ–∞–∫–∫–∞—Ä –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
        return self.jaccard_similarity(doc1['token_set'], doc2['token_set'])

    def calculate_tags_similarity(self, doc1, doc2):
        """–°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —Ç–µ–≥–∞–º"""
        tags1 = set(doc1['frontmatter'].get('tags', []))
        tags2 = set(doc2['frontmatter'].get('tags', []))

        return self.jaccard_similarity(tags1, tags2)

    def calculate_hybrid_similarity(self, doc1, doc2, tfidf1):
        """–ì–∏–±—Ä–∏–¥–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤)"""
        # TF-IDF —Å—Ö–æ–¥—Å—Ç–≤–æ (40%)
        tfidf_sim = self.calculate_tfidf_similarity(doc1, doc2, tfidf1)

        # Jaccard –Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö (20%)
        jaccard_sim = self.calculate_jaccard_similarity(doc1, doc2)

        # –°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —Ç–µ–≥–∞–º (30%)
        tags_sim = self.calculate_tags_similarity(doc1, doc2)

        # –°—Ö–æ–¥—Å—Ç–≤–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (10%)
        fm1 = doc1['frontmatter']
        fm2 = doc2['frontmatter']
        cat_sim = self.category_similarity(
            fm1.get('category'), fm2.get('category'),
            fm1.get('subcategory'), fm2.get('subcategory')
        )
        cat_sim = min(cat_sim, 1.0)  # Normalize

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        hybrid = (
            tfidf_sim * 0.40 +
            jaccard_sim * 0.20 +
            tags_sim * 0.30 +
            cat_sim * 0.10
        )

        return hybrid

    def build_similarity_graph(self, threshold=0.2, max_articles=50):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)"""
        print(f"üï∏Ô∏è  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (threshold={threshold})...")

        edges = []

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        articles = list(self.documents.keys())[:max_articles]

        for i, article1 in enumerate(articles):
            doc1 = self.documents[article1]

            if article1 not in self.tfidf_cache:
                self.tfidf_cache[article1] = self.calculate_tfidf(doc1['tokens'])

            tfidf1 = self.tfidf_cache[article1]

            for article2 in articles[i+1:]:
                doc2 = self.documents[article2]

                # –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self.calculate_similarity(doc1, doc2, tfidf1, algorithm='hybrid')

                if similarity >= threshold:
                    edges.append({
                        'source': article1,
                        'target': article2,
                        'weight': similarity
                    })

        print(f"   –£–∑–ª–æ–≤: {len(articles)}")
        print(f"   –†—ë–±–µ—Ä: {len(edges)}\n")

        return {
            'nodes': [
                {
                    'id': article,
                    'label': self.documents[article]['frontmatter'].get('title', article),
                    'category': self.documents[article]['frontmatter'].get('category')
                }
                for article in articles
            ],
            'edges': edges
        }

    def generate_auto_linking_suggestions(self, min_similarity=0.3):
        """–°–æ–∑–¥–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π"""
        print(f"üîó –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å—Å—ã–ª–æ–∫ (min_similarity={min_similarity})...")

        suggestions = {}

        for article_path in self.documents.keys():
            # –ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ (–±–µ–∑ –≤—ã–≤–æ–¥–∞)
            doc = self.documents[article_path]
            fm = doc['frontmatter']

            if article_path not in self.tfidf_cache:
                self.tfidf_cache[article_path] = self.calculate_tfidf(doc['tokens'])

            tfidf = self.tfidf_cache[article_path]

            related = []

            for other_path, other_doc in self.documents.items():
                if other_path == article_path:
                    continue

                similarity = self.calculate_similarity(doc, other_doc, tfidf, algorithm='hybrid')

                if similarity >= min_similarity:
                    related.append({
                        'file': other_path,
                        'title': other_doc['frontmatter'].get('title', other_path),
                        'similarity': similarity
                    })

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
            related.sort(key=lambda x: -x['similarity'])

            suggestions[article_path] = {
                'title': fm.get('title', article_path),
                'suggestions': related[:5]  # –¢–æ–ø-5
            }

        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(suggestions)}\n")

        return suggestions

    def export_suggestions_markdown(self, suggestions):
        """–≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ Markdown"""
        lines = []
        lines.append("# üîó –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫\n\n")
        lines.append(f"> –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(suggestions)}\n\n")

        for article_path, data in sorted(suggestions.items()):
            if not data['suggestions']:
                continue

            lines.append(f"## {data['title']}\n\n")
            lines.append(f"**–§–∞–π–ª**: `{article_path}`\n\n")
            lines.append("**–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ —Å—Å—ã–ª–∫–∏:**\n\n")

            for suggestion in data['suggestions']:
                filename = Path(suggestion['file']).name
                lines.append(f"- [[{filename}]] - {suggestion['title']} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {suggestion['similarity']:.3f})\n")

            lines.append("\n---\n\n")

        output_file = self.root_dir / "AUTO_LINKING_SUGGESTIONS.md"

        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        print(f"‚úÖ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {output_file}")

    def export_graph_json(self, graph):
        """–≠–∫—Å–ø–æ—Ä—Ç –≥—Ä–∞—Ñ–∞ –≤ JSON"""
        output_file = self.root_dir / "similarity_graph.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(graph, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –ì—Ä–∞—Ñ: {output_file}")

    def export_related_json(self, related_articles, target_file):
        """–≠–∫—Å–ø–æ—Ä—Ç —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ JSON"""
        data = {
            'target': target_file,
            'related': related_articles
        }

        output_file = self.root_dir / "related_articles.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ JSON: {output_file}")

    def generate_statistics(self):
        """–°–æ–∑–¥–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É"""
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        # –í—ã–±–æ—Ä–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–ø–µ—Ä–≤—ã–µ 20 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        sample = list(self.documents.keys())[:20]

        similarities = []

        for i, doc1_path in enumerate(sample):
            doc1 = self.documents[doc1_path]

            if doc1_path not in self.tfidf_cache:
                self.tfidf_cache[doc1_path] = self.calculate_tfidf(doc1['tokens'])

            tfidf1 = self.tfidf_cache[doc1_path]

            for doc2_path in sample[i+1:]:
                doc2 = self.documents[doc2_path]

                sim = self.calculate_similarity(doc1, doc2, tfidf1, algorithm='hybrid')
                similarities.append(sim)

        if similarities:
            avg_sim = sum(similarities) / len(similarities)
            max_sim = max(similarities)
            min_sim = min(similarities)

            print(f"   –ü–∞—Ä –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(similarities)}")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {avg_sim:.4f}")
            print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max_sim:.4f}")
            print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min_sim:.4f}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Advanced Related Articles Finder')
    parser.add_argument('target_file', nargs='?', help='–¶–µ–ª–µ–≤–∞—è —Å—Ç–∞—Ç—å—è')
    parser.add_argument('-n', '--num', type=int, default=10,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    parser.add_argument('-a', '--algorithm', choices=['tfidf', 'jaccard', 'tags', 'hybrid'],
                       default='hybrid', help='–ê–ª–≥–æ—Ä–∏—Ç–º —Å—Ö–æ–¥—Å—Ç–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: hybrid)')
    parser.add_argument('--graph', action='store_true',
                       help='–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏')
    parser.add_argument('--auto-link', action='store_true',
                       help='–°–æ–∑–¥–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π')
    parser.add_argument('--stats', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É')
    parser.add_argument('--json', action='store_true',
                       help='–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON')

    args = parser.parse_args()

    script_dir = Path(__file__).parent
    root_dir = script_dir.parent

    finder = AdvancedRelatedFinder(root_dir)
    finder.load_documents()

    if args.stats:
        finder.generate_statistics()

    if args.graph:
        graph = finder.build_similarity_graph(threshold=0.2, max_articles=30)
        finder.export_graph_json(graph)

    if args.auto_link:
        suggestions = finder.generate_auto_linking_suggestions(min_similarity=0.25)
        finder.export_suggestions_markdown(suggestions)

    if args.target_file:
        related = finder.find_related(
            args.target_file,
            num_results=args.num,
            algorithm=args.algorithm
        )

        if args.json:
            finder.export_related_json(related, args.target_file)


if __name__ == "__main__":
    main()
